WEBVTT

00:00.090 --> 00:06.000
The cost of connections, understanding the cost of connections.

00:06.210 --> 00:09.890
When you establish a DC action, there is a cost of the cost for everything.

00:10.440 --> 00:13.590
And this is my 17 years of experience.

00:13.600 --> 00:17.400
This is if I learned anything, anything I am going to do.

00:17.790 --> 00:22.020
There is a cost to it and only if I understand the cost.

00:22.260 --> 00:24.920
I feel comfortable using that think right.

00:25.020 --> 00:29.520
If I don't understand the cost, I feel anxiety is like, okay, what's going on here?

00:30.060 --> 00:33.120
Am I really am I good using this thing or not?

00:33.660 --> 00:35.130
And the cost of connection is one of them.

00:35.760 --> 00:36.510
Let's jump into it.

00:38.530 --> 00:41.020
Connection establishment is closely.

00:42.530 --> 00:44.450
So the TCP three way handshake.

00:47.770 --> 00:49.360
What do we know about the TCP freeway?

00:49.630 --> 00:52.210
Can't talk to you unless you handshake me.

00:52.510 --> 00:53.530
I need to talk.

00:54.010 --> 00:54.930
Handshake to you?

00:55.000 --> 00:55.210
No.

00:55.480 --> 00:56.350
I need to.

00:57.350 --> 00:59.030
Reach out and you need to reach out back.

00:59.030 --> 01:02.810
Sinek, send Sinek back and then I can send data.

01:02.810 --> 01:05.120
So that's cost, that's latency.

01:05.820 --> 01:08.950
The whole cost really is latency here if you think about it.

01:08.990 --> 01:13.030
Yeah, if I'm close to each other, I don't care about the TCB handshake.

01:13.070 --> 01:20.000
Like a virus is like one less than a millisecond if the devices are next to each other.

01:20.000 --> 01:26.750
But if it's across the globe, if I'm talking to someone in China and I am in California, there is

01:26.750 --> 01:32.600
physical distance that the network has to go through and that is sold rather than all the titles that

01:32.600 --> 01:35.120
need to happen and all the inspection.

01:35.480 --> 01:40.100
My packet will be abused by the time it reaches China.

01:40.190 --> 01:44.150
By the time my packet reaches the destination, it will be it will be dead.

01:44.150 --> 01:45.650
The packet won't be done.

01:46.220 --> 01:46.580
Right.

01:47.030 --> 01:49.520
So there is latency is what kills us here.

01:49.610 --> 01:49.820
No.

01:50.060 --> 01:54.460
The further apart the peers, the slower it's it is to send segments out.

01:54.470 --> 01:59.540
Of course slow start keeps the connection from reaching its potential right away.

01:59.540 --> 02:06.260
I talked about the slow start algorithm them and edited it in a minute and then and then so it goes

02:06.260 --> 02:13.520
slowly until it funny is slow start actually fast because the exponential you know because it adds one

02:13.520 --> 02:15.260
segment after each an acknowledgement.

02:15.260 --> 02:23.450
So if you don't have delayed acknowledgment enabled right then your slow start is actually faster because

02:23.450 --> 02:27.290
you're going to get a lot of access, you know, which is pretty cool.

02:27.500 --> 02:31.340
So slow start will kick up a little quickly, but it's a cost regardless.

02:31.340 --> 02:33.050
We have to do a slow start, right?

02:33.500 --> 02:34.820
We cannot send a lot of data.

02:34.820 --> 02:36.440
We cannot send our data right away.

02:37.610 --> 02:40.940
And then congestion control and flow control limits that further.

02:40.940 --> 02:47.390
There's so much stuff delayed and delayed in acknowledgment and Nigella got them can further slow down

02:47.390 --> 02:47.750
things.

02:48.200 --> 02:50.090
Destroying the connection is also expensive.

02:50.090 --> 02:51.110
This is costed.

02:51.110 --> 02:52.820
Everything we do.

02:53.090 --> 02:55.790
What do we do with our lives, you guys?

02:57.650 --> 02:58.970
Meat Connection Polling.

02:59.990 --> 03:07.130
So Connection polling is something I talk about all the time in my courses and my YouTube gal most implementation

03:07.160 --> 03:08.240
database back in now.

03:08.480 --> 03:11.870
Now we're linking vendors to actual databases and reverse proxies.

03:11.870 --> 03:12.140
Right.

03:13.340 --> 03:16.220
This is this is now the link that I was talking about.

03:16.250 --> 03:20.870
You know, just now we're talking about TCP IP and UDP and IP packets and headers.

03:21.170 --> 03:22.490
Now we're what?

03:22.640 --> 03:26.940
Now we are in the trench, you know, connection pooling.

03:27.860 --> 03:31.220
Most implementation of the database beckons and reverse proxy use pooling, actually.

03:31.430 --> 03:32.780
So what is a reverse proxy?

03:32.780 --> 03:37.180
Reverse proxy is a is a server that a.

03:38.800 --> 03:44.560
That talks to a bunch of back and fleet of machines without you knowing.

03:44.890 --> 03:46.810
You talk to one reverse proxy.

03:47.960 --> 03:55.310
As your final destination, and that talks to a back end or one or more back end.

03:55.700 --> 04:01.220
So by design, the reverse proxy have to talk to many backends.

04:01.730 --> 04:07.580
So it's in its interest to establish this connection and warm them.

04:08.770 --> 04:16.480
In advance because of the cost that we talk about, because if you send a request synchronously establish

04:16.750 --> 04:23.890
the reverse blocks, you will synchronously establish connections upon the reception of the request.

04:24.490 --> 04:29.380
Then there is an additional cost that you need to manage, plus the the slow start and all this.

04:29.380 --> 04:31.930
A Why would you make the client suffer?

04:32.800 --> 04:34.240
Start the reverse proxy.

04:36.570 --> 04:40.140
Hit the fleet in the back and warm all the connections.

04:40.320 --> 04:44.970
Start 30 connections and prepare, you know, for this stuff.

04:45.900 --> 04:47.910
So that's one idea of the connection pulling.

04:48.480 --> 04:50.400
Same thing with the databases or databases.

04:50.700 --> 04:56.910
You would have a database server and you have the back end and you will create a pool of database connections.

04:58.200 --> 05:00.900
With that user effectively connected to the database.

05:01.110 --> 05:08.410
And then clients that hit you as a back end web app will pick one connection from the pool, reserve

05:08.490 --> 05:14.610
it or nobody touch it because of the problem we talked about earlier with the sequel statements, you

05:14.610 --> 05:19.710
don't know which responses for ones request, you know, reserve it or nobody else can use it.

05:19.860 --> 05:23.370
Send your sequel query, get the response back.

05:23.370 --> 05:28.410
Now you know that only you can respond back in this machine, this connection, and now get back the

05:28.410 --> 05:30.120
response and then releases back to the pool.

05:30.450 --> 05:32.580
So now you don't have any leaking on anything like that.

05:32.580 --> 05:40.560
So you can effectively have these connections warm and already reaching its potential slow, start wise,

05:40.560 --> 05:45.870
congestion control wise and all this stuff, you know, how beautiful is this?

05:45.870 --> 05:47.790
If you think about it, I'm so excited.

05:47.790 --> 05:48.300
I'm sorry.

05:48.300 --> 05:49.590
I just love this stuff.

05:50.940 --> 05:53.400
So establish a bunch of these pictures in the back end to keep them running.

05:53.580 --> 05:54.180
Good idea.

05:54.180 --> 05:54.540
Good idea.

05:54.540 --> 05:55.080
Good idea.

05:55.370 --> 05:58.350
You know, Donna Summer connection on request, right?

05:59.260 --> 06:00.870
Because that will just slow things down.

06:00.870 --> 06:01.140
Right?

06:02.340 --> 06:03.390
Don't do asynchronously.

06:03.390 --> 06:05.130
Do asynchronously, if you will.

06:05.940 --> 06:08.010
Any request that that's not really asynchronous.

06:08.010 --> 06:08.610
I take that back.

06:09.000 --> 06:11.370
Any request that comes to the back end.

06:11.370 --> 06:13.590
Use an already opened connection.

06:13.590 --> 06:14.070
Beautiful.

06:14.070 --> 06:15.810
An open connection that is hot.

06:16.860 --> 06:22.770
By hot, I mean it's already warm enough such that data has been sent there.

06:22.770 --> 06:25.920
Poor schlub first client will suffer a slow snooze.

06:27.510 --> 06:36.780
Let me let me say this without the tongue twisting the poor slob client that first makes a request of

06:36.780 --> 06:37.980
the reverse proxy.

06:39.190 --> 06:47.140
And their first party will hit that freshly open connection to the bag and will suffer through a slow

06:47.140 --> 06:49.990
start and will suffer through congestion controller.

06:50.620 --> 06:52.750
Congestion avoidance algorithm.

06:53.140 --> 06:53.510
Right.

06:54.580 --> 06:59.900
But as you send more data, more data, more data, the connection will be warmed and the congestion

06:59.900 --> 07:02.410
window will be really fat and big.

07:02.920 --> 07:04.010
And that's what we want.

07:04.030 --> 07:05.500
We want big windows.

07:06.220 --> 07:08.230
We want big windows.

07:08.950 --> 07:09.850
That's why we want.

07:12.320 --> 07:15.080
Any recourse that comes with the back end or any connection to talking about that.

07:15.080 --> 07:17.570
This way your connection will be warm and slow.

07:17.570 --> 07:19.160
Start would have been already kick.

07:19.160 --> 07:23.540
Then don't close that connection unless you absolutely don't need to.

07:24.380 --> 07:30.080
Absolutely don't need it now don't lose the connection unless you absolutely do not need it.

07:30.800 --> 07:31.700
Keep that action open.

07:32.030 --> 07:33.140
What do you care?

07:33.740 --> 07:34.370
Memory.

07:35.340 --> 07:36.210
Its abundance.

07:36.270 --> 07:38.130
Keep it running in the back end.

07:38.460 --> 07:38.880
Right.

07:39.180 --> 07:42.240
Don't close that connection because if you close it, then you have to open it.

07:42.540 --> 07:49.950
If you anticipate some, you know, uh, traffic, keep it running.

07:49.950 --> 07:53.700
If you really think that you absolutely don't need it, be a way to close it.

07:54.330 --> 07:54.600
Right.

07:58.990 --> 08:04.990
Another concept that we front engineer and back engineer uses eager versus lazy loading.

08:05.860 --> 08:13.360
So how does that affect us as a networking when it comes to networking, eager versus lazy loading?

08:13.360 --> 08:18.640
These are two paradigms that we use all the time, which comes to back end and front end engineering.

08:18.650 --> 08:21.730
These are techniques that there is no wrong, there is no right.

08:22.210 --> 08:26.050
Each technique is used based on certain use cases.

08:26.440 --> 08:33.460
And I, I used both approaches and I cannot tell you when to use one because it all depends on you use

08:33.460 --> 08:33.970
case, right?

08:34.540 --> 08:39.880
So depending on what paradigm you use, you can actually save on resources if you care about saving

08:39.880 --> 08:41.590
resources, if you want to scale.

08:41.890 --> 08:42.190
Right.

08:42.520 --> 08:45.850
When it comes to scaling, so you would use eager loading.

08:45.850 --> 08:47.260
What is ego loading means?

08:47.260 --> 08:51.070
Hey, I started my applications.

08:51.670 --> 08:53.920
Let's load everything up.

08:54.490 --> 08:56.100
Start everything up.

08:57.280 --> 08:58.060
Start up.

08:59.600 --> 09:00.490
Start of everything.

09:01.000 --> 09:01.540
Run.

09:02.110 --> 09:02.530
Open.

09:02.530 --> 09:03.730
700 connections.

09:04.060 --> 09:05.050
Leave it running.

09:05.140 --> 09:08.020
I want everything to be eager, you know.

09:09.760 --> 09:12.340
So in this case, the start of a slow ride.

09:12.340 --> 09:15.850
Because now you're doing all this work right on starhub.

09:16.210 --> 09:20.890
But requests that comes in will immediately get served.

09:21.040 --> 09:21.430
Right?

09:21.820 --> 09:24.100
Because they have this stuff hot.

09:25.240 --> 09:26.290
So some apps.

09:27.910 --> 09:30.610
Send actually warm up data, which is numb.

09:30.850 --> 09:36.970
I don't really like that, but I've seen some of does that some absence warm up data to kick in the

09:36.970 --> 09:42.400
slow start algorithm but really careful about the bandwidth when it comes to scalability here.

09:42.430 --> 09:42.730
Right.

09:43.150 --> 09:48.130
So what these apps do in the back and specifically, I was like, okay, it will start the back end

09:48.520 --> 09:50.680
and then we'll start that ECB connection.

09:50.680 --> 09:55.990
But it will send dummy data to the back end.

09:56.320 --> 09:56.590
Right.

09:56.590 --> 09:59.650
And we configured the back end to discard this data effectively.

09:59.890 --> 10:06.070
And since it's the back end and the reverse proxy is so close to each other such that it doesn't really

10:06.070 --> 10:08.580
matter because it's a local network.

10:08.590 --> 10:08.920
Right.

10:09.190 --> 10:16.300
I can do that, but I'll send warm dummy data just to kick in the slow start all the way up to the congestion

10:16.300 --> 10:19.180
avoidance so that I have a large congestion window.

10:19.390 --> 10:26.560
Some people do that I've seen, although I are like it, I feel I feel less let let things happen naturally.

10:26.560 --> 10:32.950
You know, you just loo why waste the resources when you don't really need just to get that extra performance.

10:32.950 --> 10:33.540
I don't know.

10:34.210 --> 10:34.600
I don't know.

10:34.600 --> 10:35.320
It already depends.

10:35.320 --> 10:36.820
So this is the ego loading effectively.

10:36.820 --> 10:37.690
Let's load everything.

10:37.870 --> 10:41.830
Let's warm everything up and make it hot and ready.

10:43.180 --> 10:47.200
Lazy loading is only load things on on demand in this case.

10:47.200 --> 10:49.840
So startup is immediate.

10:51.250 --> 10:52.570
Startup is not doing anything.

10:53.170 --> 10:54.760
And that's something you might want.

10:55.150 --> 10:56.950
You don't want your startup.

10:57.220 --> 10:57.520
Yeah.

10:57.520 --> 11:03.160
If you're doing eager loading and you want to test AB Oh my god, that will be really slow because the

11:03.160 --> 11:04.270
startup will be really slow.

11:04.270 --> 11:09.160
You're doing all these resources, you're consuming all this work every time you run the AB is doing

11:09.160 --> 11:11.170
all this warming up on the backend.

11:11.170 --> 11:12.760
Oh don't do that.

11:13.150 --> 11:18.040
Lazy loading immediate, but the request and the first request will suffer.

11:18.460 --> 11:22.030
The first pull shrub that makes the request will suffer.

11:22.030 --> 11:25.960
Unfortunately, startup is fast, but request will support as initially.

11:25.960 --> 11:27.750
All right, that's it then.

11:27.760 --> 11:30.670
So connection establishment is expensive.

11:30.670 --> 11:34.270
So that is basically what you need to understand.

11:34.270 --> 11:36.070
Connection establishes is costly.

11:36.640 --> 11:38.560
We can do certain things about it.

11:38.800 --> 11:41.260
We can do more things in the next lecture, we're going to talk about it.

11:41.650 --> 11:44.800
But this is some of the things that we can do to.

11:46.000 --> 11:50.410
Eventually amortized the cost of the connection establishment.

11:51.100 --> 11:52.090
On to the next lecture.
