# networking consolidated


## Introduction


### Welcome vtt

Welcome and thank you so much for checking out this cause. My name is Hussein and I have built this course specifically for people like myself who are back in engineers, you know, software engineer, frontend engineers who started up directly programming and building network applications without really fully grasping and understanding the fundamentals of networking. You know, I spent over 17 years, you know, just using and building software, designing and architecting back ends. But only recently in the past five years, I actually decided to completely grasp what it what I, why I'm what am I really interacting with, you know, what are these network stack? What are the limitations of the network that we have today? So like what's going on? What is a TCP handshake? What is an acknowledgement why sometimes that connection is slower, sometimes it's fast. All these questions, you know, it became it became really frustrating at a certain point that I don't really understand that. And the courses that we took in the university really kind of it was so dry and they fed you this networking concept, but you really don't have anything to hook on to kind of relate to, you know? Yeah, this is a TCP window, but okay, okay, why? Why does it have to do anything with me as a computer science student? You know, there is a bridge that is missing. And and I spent some time trying to craft a course specifically to bridge that gap between back in engineers and network engineering. It's a self because I do believe network engineering is one of the most critical concept to understand. If you're a back an engineer and if you're a front end engineer, you know, and definitely if you are a full stack, you know, just understand what is underneath the first principle, first principle building blocks. And this is what this course is for. So in summary, I kind of built this course for my old self, you know, when I was a little bit frustrated about things that I don't really understand, you know, it comes to the TCP stack and the IP stack and the UDP and TCP protocols and and all these small concepts that really critical to understand. Layer seven, layer four, layer three, layer six, you know, all these concepts that really shows up every day when you are a back end engineer, specifically if you're in a DevOps, you know. But so I kind of built this course for that with that thing in mind. Thank you so much for checking it out again and I hope you enjoy it.


### Who is this course for vtt

So who is this cause for? I specifically built this course for back end engineer like myself who are really building back in applications that clients consume, but they don't really necessarily understand what is going on behind the scenes. When a segment arrives at their application, what the operating system is doing, what is the application is listening on, and why the segment goes to a certain process but not the other. It's effectively tries to unveil and remove the blinds from this, you know, foggy, understanding and trying to build a bridge. That gap, as I talked about previously and also remember that the fact that we're using the word back and kind of indicate that there is a frontend that consumes that back in and mostly the medium that connects the back end to the front end is networking. So really good idea to understand what is happening behind the scenes when you're building these basic fundamentals. This course is also very good for frontend engineer who are trying to build back in application or built back in application before. Because guess what? Frontend engineers are those who make the calls to the back end to an API to consume it. So it's really interesting to send a call and really follow the call all the way to the back end and make sure that it actually reaches. So if there is a slowdown in a certain request, it doesn't mean that a back in is slow. Could be the networking between this point, to this point has certain limitation and it's good to understand that there could be a configuration in your client side when it comes to the network and is way lower level down beneath you. You know it doesn't it's not something you as on the application necessarily without Node.js or Python as low level is so understanding that and once you really understand is what really matters here when you understand you know what to do. When you don't understand, it becomes really a black box and you're just left out with more confusion. Because also for network engineers who are interested to build back end or frontend applications and not rendering already understand these basic fundamentals. You know, some, some of them you don't understand everything. Nobody does, obviously. But that gap that we talked about between the application and the networking is really huge. And you'll be surprised that most of the networking that I met, we don't see the application. They treat everything above Layer seven as an application, although there are so many other layers above that, you know, it's not just, oh, why is the application doing sending a reset to the socket? Well, sometimes the even the back end engineer doesn't understand why is happening, not necessarily. Right. So there is a huge gap between the network engineer and the backend engineer and I try to bridge that gap as much as possible. Sometimes I fail, sometimes I succeed. But this is the goal of this course, at least to bridge this gap. So if you are networking Jews who want to build application and you want to even to refresh your memory on this kind of thing, I think this course, this course might be for you.


### Course Outline vtt

So we talked about who this course is for. We talked about an overview of the course, but now what do we need to talk about the outline of the course? What are you going to get when you finish by the end of the course? What are you going to grasp? All right. How about we jump into it? So I broke up this course into six sections, but not necessarily they will remain six section when you watch this content, because I, as you might know from my other courses, I always add and make the courses and add more content to the course. It'll be more than six sections of activity, but the major six sections are. The first thing I'm going to talk about is the fundamentals of networking. And these are two sets of lectures that will help you kind of build the foundation, if you will, of of the networking aspects, like starting from scratch as if you don't know anything. What why do we need networking? Why networking was invented as a model client server, architecture, hostels, communication, things like that. To talk about just a fundamental on networking that we really need to understand before jumping into the other protocols, which the first one very critical is the Internet protocol, the IP. So we're going to demystify everything there is about the Internet protocol, the IP. And I'm going to go into details here. I'm going to talk about the routing protocol. I'm going to talk about how the IP address looks like. I'm going to talk about how the IP packets are routed. Go talk about certain protocols that are complementing the IP protocol, ARB, ICMP, stuff like that, and not just mentioning them, actually linking them with reality with what we do on a daily basis. And that's what I believe missing. And most of the courses that I interacted with at least. The link between what we do and what we study. Even mathematics, if you know it is like I used to like mathematics, but a lot of people were frustrating with mathematics because they memorize a formula and I have no idea when to use it because they are memorizing things. A lot of people memorize networking concept because they have no idea when this thing shows up. So I tried to bridge that gap by giving you examples as much as possible. So I try to give you as much as examples as possible when it comes to linking the networking aspect with real life examples, you know. I have 17 years of experience, so I try to pull in examples from personal problems that I've personally faced and from my colleagues that I ran into or from blogs article I read and I tried to reference all of these. So you'll see all the tons of references in this course as well. And it would be a good idea to download the master slide deck to It's Over, I think 170 slides, you know, there's a lot of stuff there. But yeah, it's going to be very, very interesting. The IP protocol, very, very critical concept. So another thing in the IP section, I'm going to talk about the IP packet itself. We're going to turn to the cartoon and then I'll try to understand all these headers that we have. And then what are the use cases for this head on when they will show up? And I'm not going to explain all the headers. I'm going to explain the headers that are personally I interacted with and I they kind of pinched me in a way or another, you know, some of them obviously I don't know everything, but I will talk only about the things that I believe is relatable. You know, some of that not all of the headers are I have personal experience with, so I'll explain whatever I can. So as a back an engineer, you really, to be honest, really care about what the headers are and try to understand or memorize them. Almost never. If you want to know what a header looks like, you can just google it. You know, never memorize anything. This is not what the course is for, you know? It's just understanding sometimes what each header do does. And if you want a refresher, you can always Google how it looks like, you know that's that's what it really critical here. And the next one is the UDP. I started with UDP because kind of TCP is, I would say like a superset of UDP. UDP is a simpler protocol, you know, so I'll talk about a UDP datagram that's why we call it the pros and cons of UDP, when to use it, when not use it, you know, as again personal experiences. And we'll talk about the TCP protocol and talk about the segment concept. I talk about the handshake, talk about why does TCP exist? I tried to I had a lecture which explains why do we need blah? You know, why do we need IP? Why do I need you to be? But it kind of it became a little bit a little bit cumbersome to manage. But that's what that's really what the question you you should ask all the time. It's like, why do we need IP protocol? Once you really ask the question of why it really unlocks something in your brain, you know, because you will understand why it exist. You know, because always anything exists. Because most of the time it exists because of a problem that we try to solve and we invent something to solve. This problem for nothing exists just for the sake of fashion, you know, always. And, and I find it personally when I, when I, when I understand why something exists, I, I find it I find it easier to remember. I find it, you know, more. I don't I don't I don't have, you know, fog and misconception anymore. I just find it very clear. So. TCB, We're going to ask you like why this EP exists. So that's a whole new section is it's filled with content. You talk about flow control. Why does full control exist? We're talk about congestion control. Why does it exist? When I was slow, start the super fast open handshake, closing connections connection states. I'm going to talk about so much stuff, but don't be overwhelmed because most of this is going to be fun. Because I'm going to explain because I really enjoy talking about these concepts and relate them to the to the present, to, to what we do on a daily basis that back intention into two applications that we develop. Right instead of just memorizing stuff. You know, then my favorite section, which is this is the section that kind of bridges the gap. This is the section, right? That will. Cement the first principle of networking and kind of link them to the back end performance, you know, and this section. One of my favorite networking concept that effect back in applications design. You know this this this this section specifically is the link that links that back in engineering to the network engineering. You know, you will find most of these lectures that I, I authored as a link to directly to the performance of the back end application or the front end application. Because some of this stuff is really a configuration of the client side because the client needs to make a connection at the end of the day, you know, and the concept of a connection, the concept of a performance, the concept of why when I make this handshake after a while, immediately I my data is not getting through like or why is when I started the application is so slow, but after a while it picks up why I cannot listen on on the same port twice. Why? Why cannot why cannot to do that? Why when I send I tried to send a request. It's very, very slow and I have no idea why. There is really something that this large request that I tried to send, it's really slow for some reason why it doesn't make any sense. And why does it? Sometimes when I connect to this server as fast when but when I connect to the server a slow, you know, the response comes slower. All these things, you know, while they are a little bit low level understanding them kind of bridges the gap between the back end application that we write when you actually write a line of code says, hey, I want to listen to this port on this IP address, what does what is it really that is happening when I receive data? No. And we're going to explain the concept of a request, you know, and all these stuff. It's fascinating to me. I absolutely love this thing. I talk about the maximum segments, ISE and maximum transmission unit and how do this to leg to each other and why what what determines the size of this packet or the segment specifically? And is larger packets or segments better you know all these kind of things. I how fast can I send data all the you know and how does this translate to manage GDP request get request for example all these things my thought to explain this as much as possible then as I believe it's going to be fun and unfortunately is not it won't be a networking course without actually firing up Wireshark, which is if you don't know Wireshark is this utility that allows you to sniff any packet that is going out of your machine that Wireshark is in a stolen. And we'll look through that. So we're going to warthog TCP protocol. We're going to want to CHAKA UDP how looks like we're going to wireshark ebs we go through it. We will do a trick to actually describe the Dallas handshake and look at the content. So there's going to be so much fun. Obviously, if there was an additional section, I'll add a new kind of a lecture, talk about the course update and then just like I do with my database and other courses as well. So I believe I really hope you enjoy and enjoy this course. And let's get started.


## Fundamentals of Networking


### Client - Server Architecture vtt

So I thought we start with the client server architecture. This is the revolution that started it all. You know, how can I put my server and my client in different location where different core pieces of code can live somewhere else? And I need to call certain piece to execute somewhere else. A revolution, indeed. Now, we don't have this big mainframe. We don't need them anymore. You know where it runs? Everything. We can just have cheap commodity hardware sitting on the client and move them. Move their workload that are really heavy on the server. That's basically it, you know. So if you think about it, machines are expensive, applications are complex. How can I separate the application into multiple component? Doesn't necessarily just do you know, just let's break down the application that is running on a single machine into two components. If you think about it, microservices kind of inherited or borrowed from this model kind of, you know, because we have we used to have this big monoliths service and we broke it into multiple smaller microservices and let them call each other effectively. That's the same, you know, original classic concept. Let's, let's break our application into multiple components. Let them call each other. And that's the beauty here. Expensive workload can be done on the server where you can anything that is expensive. What do you say when we say expensive here.We really mean Ram takes a lot of Ram or takes a lot of CPU or takes a lot of latency, whatever that means, right? Reading to desk, you know, takes time to do all this computation. Let's move them to a a server, a machine that is a really beefy that has good resources. And then let's keep the caller in a smaller, you know, tablet or, you know, I thin out a client, if you will. Now, that's that's the beauty here. The clients call servers to perform expensive tasks. So simple, so elegant. A remote procedural call was born. This is what what what what we refer to our RBC. Right. RBC calls has been there since was sixties seventies. You know the idea of let's make a call by to a remote call. You know, previously there was no standard. You know, let's just let's whatever. Let's just send it across there, the wire. And there's absolutely no no standard. You as long as you can make it to the server, you've done it, you know, but those standards started to build up. And this is there's another important technical concept to, you know, J RBC. G RBC actually borrowed also from this concept, you know, where the Google, a remote called procedure, you know, kind of used to be to to build upon this concept but make it the universal I believe now it's a job is is there is a universal effectively communication between any two components if you are. So what are the benefits of clients over architecture? Servers have DVR, where the clients have community hardware. So you can have a lot of clients call a single server, if you will. So in this case, you kind of centralize the work and you can you kind of scale better, you know? Clients that I built, I swear, this is the same concept as microservices because this is the same, you know, advantages to microservices, like let's scale them better. Don't get me wrong, I don't necessarily in favor of microservices. I've been very critical of this technology. You know, I talked about it in my YouTube channel, but there are benefits definitely, you know. But I think we're we're a little bit overdo it with microservices. But that's another topic. So yeah, so the idea here is just really scale better clients that calls. Yeah, you can do as many clients because now they are lighter, they start faster because they're they don't they don't have all this application logic that I used to have. You know, the binaries are smaller. My God, this is so much better. You know, just move it around so clients go. But here's the thing. Clients can still perform lightweight tasks. This is the this is the trend with edge computing. If you heard about it, you know, even clients that Iot devices that are literally just censoring data and then sending requests somewhere else, they can't perform, perform compute logic. You know, unfortunately, all of these Iot devices are just mining Bitcoin at this moment. But regardless, you get the idea, right? You can you can do work in the client site as well. Now people are moving here. This is like, okay, let's do more client side logic. But that's the beauty here. Client no longer required dependencies. What does that mean? The application when you built it, when it was a monolith in one machine, it records all these dependencies, all his libraries, all these, you know, calling to desks, calling to whatever, you know, printer, all this stuff, all these dependencies or no, the server responsibility. I mean, if you want to talk to a database, you need a database driver. Now, at least Oracle, a SQL Server does, you know, for example, that means you have to install this runtime in your application. I'm just an example here. So if you move the server, you know, somewhere else. Right. The server only needs to talk to the De Beers, so it needs that dependency while the client is lightweight, it just makes the call. Yeah. And we're going to talk about what does that mean, making a call in a minute. So that's that's really powerful, if you think about it, you know, and that's where the three tier architecture, when it comes to the database and moving the server, it kind of it's got I consider the three tier architecture is going as a special case over there clients over if you will. However. However. We need a beautiful communication model. We cannot we cannot let this be the wild, wild west, my friends. We cannot. We need. And we need a standard. Okay. Got it. Client server is awesome. I need a network. You know, I need to send a call, send some data from one machine to another. But how should I connect them with a telephone wire and then send data? How. How do I do that? Well. You can do it any way you want. You know, if you figured out how to transpose, if you will, the bits into radio or electric signal, you can do it anything you want, but there is no standard. So we need a standard so we all can understand each other, you know, very critical. How about we jump into the next lecture?


### OSI Model vtt

In this lecture, I'll talk about the Orci model, the open systems inter connection model. And I still remember 20 years ago when I was in university and. My instructor tried to explain the LCI model and I really, truly didn't understand anything. And I really didn't think it was important because because I was excited about, you know, writing C code and C++ code. I wanted to build an interface. I want to build a visual basic application. And, you know, I was I was interested in that stuff and I really didn't understand the value of what they were explaining, right? AD And so I end up memorizing everything and I was just like passing the exam, but I didn't get anything. Unfortunately, although the doctor the doctor was really good, you know, unfortunately. So I really regret that. But so what I wanted to do is just learn from that experience. Always a model is really critical to understand any engineer or any software engineer. If you ever want to interact with networking, you really need to understand those AI model, you know, and don't. Don't think like you have to understand everything in it, but just understand this seven layers effectively. And we're going to talk about also about the simplified Orci model because it does have some criticism, you know, when it comes to that. But just the Orci mind these seven layers and where does your application live that is what really matter. Where does your application live? You might say, I don't care. Why would I care where my app live? No, you should. Because if your app is a bridge between two other apps, we really need to understand. What are you looking at? What do you see? Are you looking at Mac addresses? Are you looking at IP packets already? Looking at segments, are looking at ports, are looking at the DCP options? Or are you looking at the consider of a connection, the TCB connection itself or audio decoding or encoding the audio serializing this allows in Jason or you decrypting my stuff to look at it and every single layer has a meaning and every application out there, a CD and a reverse proxy, a load balancer, a reverse proxy, an API gateway has to live in one or more of these layers. And this is what I wanted to talk about. Let's jump into the slides and nail this. I promise this is going to be fun. All right. So these slides a little bit later, so I'm going to disappear in a minute. But I used the awful day. I'll be here. I'll be here. I'm just going to explain that concepts behind the scenes and I'll show up at the end to summarize this stuff. All right. Those I model the open systems interconnection model, you know, open systems open has to be open because we really need an open communication. We really need to understand and make a standard out of this thing. So Orci model, let's get started. Why do we need a communication model? I always, as you know, I always start with a Y because to me, I don't like to understand something that I don't know why it exists personally, at least you know. So the goal here is to build agnostic application. So, you know, imagine this if you really don't have a standard, then we want to build a networking applications. Then we cannot do this because my server have no idea how to talk to your client. Like, how are you? You know, how do you how are you transposing the beds into digital to analog and analog back to digital? How, how, how am I supposed to look at this bits? You know, there must be a standard. It doesn't have to be a stunner, have to be a protocol to understand how to chop up these beds, to make sense of it as the application level. Without a standard, your application must have knowledge of the underlining network medium. That's even worse. Imagine this. Just imagine if you have to author a different version of your apps so that it works on Wi-Fi. You need a different version to work on within it. You need everybody to work on LTE and you need a different order version to work in fiber. That will be a disaster. Yes, we take it for granted today. We take it definitely for granted. Let me show up for a second. We really take this thing for granted. You are building a Node.js application today and you're sending a request or just listening. And it doesn't matter where this application runs, it runs on any CPU, you know, because someone smart built it. So it compiles on all CPUs. And when you send a request, it doesn't matter if before sending it through the satellite or sending it through radio Wi-Fi or sending it through Ethernet electric signals or sending it through the LTE radio waves, or they get it through fiber light. It doesn't matter why? Because we built a standard and that standard is used globally. Effectively, yeah. And the whole ward was the difference in my ward and global. Global is this means that the earth or just goes to space. I believe it goes the space. It's a global open system. Communication is everywhere. Just understanding of that is really powerful because imagine you have to build a different application to work on the light because it's all different mediums, right? So like, how would you expect the application to be the same? You have to have a standard. Otherwise the application was, Oh, I'm talking to this fiber, this is how I need to convert my bits to light signal. Or This is radio wave. I have to do this to do radio. But I just wanted to explain this because this is really taken for granted. But people, smart people, build this so that we don't have to worry about it. Back to this slides. Network equipment management. Without a standard model, you know? Without a standard model, upgrading network equipment becomes very difficult. You know, now. If every equipment has a different standard, then you cannot move on. You know, you you you will have different models and different things that communicate with each other. First of all, they won't be able to communicate with each other. This router won't be able to talk to this author because there is no standard. Right. And the beauty of this here is. Regardless of the underlining medium, you can upgrade the actual network because it is completely decoupled from the actual medium itself. So there is there is nothing that we can really what would you have to worry about? The underlining medium, you can just upgrade the equipment normally and whatever you bring in, we'll just support it, you know, as long as we know how to talk to it, you know, it's decoupled for innovation. You know, that's what we talked about in a minute. Innovation can be done in each layer separately without affecting the rest of the module. Yeah. And that's that's very, very critical as well. And we're going to talk about the layers in a minute. But each layer is built this way because each layer you can innovate and improve each layer alone. You know, this is a little bit vague now, but it's going to be clear in a minute. You know, you can improve the physical layer because that's the medium. You know, that's the radio. You can if someone invented something faster than fiber, I don't think you can. There's nothing faster than light. But you get my point. Yeah, it's like more efficient. Then we'll just build the interface for, then we'll go support it, you know? Right. And layer two has nothing to do with. It's just a layer two will pass into bits and the convergence will happen in the physical layer, you know, layer three, which is the I believe it cares about this stuff. If you want to add more content or more headers, it can be added of a layer layer too, you know, although this will break other stuff at the end of the day. But that's another topic. You know, just you cannot just increase the header because there is something called the protocol ossification, which is all these protocols, all these routers in the middle actually understand how to read things in a way. And if the sink change, they freak out. We'll talk about this in a bit. Actually, I'll have to remember to add a lecture. Just talk about a protocol. Ossification, a very critical concept. What is those? I model seven layers each. Describe a specific network component right layer seven the application that's what you even that can induce even they don't and they don't really interact with they are seven directly. It's usually above the application. When you look at layer seven two, a network engineer, that's just data coming in, you know, but to a back, an engineer there are they are using different libraries are listening. They're sending packages in the packets, they're using maybe a jar PC, you know, protocol or certain protocol that sits on top of FDB to which, which, which, which creates more and more packets, you know. But that's the application and that's really what, what really critical here the application is, is really huge here and everyone is looking just at that layer differently. But to us, anything above, you know, effectively the the SDP is the application, you know, or a FTP or GRB. See anything, you add this layer, this is the actual application layer six, the presentation layer encode serialization. Ever send a Jason right through fetch or Axios that's the JSON need to be serialized from this JSON object. If your JavaScript or Python, if you're if you're like a bunch of arrays or that data structure down to a string and this is happening on layer six. While you shouldn't care about this, this is already happening for you. So it's little bit a step down from will application. Your application sends a whole object, but the conversion of the encoding happens there to serialization. Oh, this is a UTF eight whatever lets encoded so that under leave layer does the job. That's why talking about this stuff is like you might say, wait a minute, say who cares, right? Right. You might say, who cares? Like about encoding and serialization. This is happening in my application and that's what people are frustrated about. The was model people is like really you really need to break this that fine of a level of a detail I don't care you know sometimes this is one of the and that presentation layer and also the session layer at some cases you know, that's why it this TCP IP model kind of simplifies this. But I still prefer the all time model just in if. Why? Because we're used to it, you know, back to the slides. So. Yeah. So that's the presentation layer. Then we have the session layer. You know, the session layer is where TLC happen, you know, where connection establishing a moment where state effectively sets place, you know, where you store a state in the client, you know, where you state, state. It's not a state in the server. That's why some protocols called stateful, you know, there's state less. DTP doesn't have a session layer because it's a it's a stateless protocol. You know, while TCP, it is a state for protocol. You store a state on the server and a state on the client and you manage a session effectively. And if this session is destroyed effectively, either you can restart that connection or invalidate. So the session layer effectively checks that. Again, a lot of people says, hey, really, really you want. It's like really session layer, really. You need a whole layer just to talk about session. Believe me, this session is actually this. This layer is actually important. A lot of proxies like Linker D, I believe, actually built logic only at the session layer, only at the connection establishment. They would do certain logic. So their application is a layer five app, you know, because they capture a connection and save it or do pooling about it, you know. So that is a layer five app. That's what it means. Like your application is layer seven. Well, this is a layer seven proxy. This is a layer for a proxy. They talk about this in a minute. All right. Transport one of the most important one. Let's be honest, layer four and layer seven has a back engineer. That is the only two things that you're going to worry about most of the time. Hey, maybe if you go and a little bit of DevOps, see, you're going to care about layer three and layer two. When you build like BRP sessions and work with Keep Alive and stuff like that, we're going to worry about this, but we live here, baby. We live in layer four and layer seven. Most of the time we build layer four, we configure doesn't necessarily be building, but we configure our application to be a layer four app. Why? What does that mean? It means we are aware of the transport. It means we're aware of the packets. Right. They're not go because they they're called segments when it's called DCP and Data Control Corner. DB All right. Let's be very specific about these naming in a minute. You're going to across the course, you're going to you're going to worry about this, understanding it very clearly. But this is what it is are effectively TCP UDP. Right. Very, very important concept. The protocols live here. These are the only two protocols, let's be honest, that are and there aren't any other protocols when it comes to transport. Well, you can call count quick as part of it. There's a very new quick protocol. The roughly new. But it's it's a transfer protocol. But that's pretty much it. You know, these are the three protocol and everything is built on top of them. To be built on top of TCP is to be to DCP is to be three quick quick is built on UDP, you know, so everything is built on either imagery on DCP or to be there. Anything else you can be fancy and build application directly on the IP, which is layer three, the IP protocol, the internet protocol here, we don't have a transport concept. You know, I don't care about the if the packet reaches or not, I'll try my best. I'll tell you if it's bad, but hey, it's up to you. It's a network protocol, you know. It's a packet that destined to an IP. Here we have visibility to ports, you know, port 80, port 80, 80. Here we don't know what ports are. We only know addresses, IP addresses, you know. So every layer has kind of a beautiful concept introduced with it, you know. So the IP have the concept of the routing and the IP addresses, which we're going to explain in the whole section. So stay tuned. Beautiful stuff coming. Beautiful stuff coming. So layer two, another important concept here. Layer two is the data link layer. And we're dealing here with the physical medium. At the end of the day, physical network addresses, you know, hey, this Wi-Fi has this Mac address. I want to send a frame to it. So we're sending a frame in layer two. And we have we know about we don't know about IP addresses at this layer, nothing. We only know about MAC addresses. We only know about. That's my. And the protocol that we use like Ethernet, Wi-Fi 802 or whatever it's called, you know, frames we send frames a layer two. I'm going to repeat this million times during the course, we send frames in layer two, we send packets in layer three and we send segment and TCP layer four and we set daylight grams when it comes to UDP, you know, also called segments. And for both, you can see with me if this is not correct. All right. But yeah, don't don't worry about it. The terminology of the physical layer, this is the bare metal. What is what is once you take the frame, how does it transport to the other medium? Electric signal when it comes to Ethernet is if fiber and light light waves, you know. Is it radiowaves, you know, wi fi or LTE? All this stuff, the physical and it is. When it comes to this is just radio waves at the end of the day or light or electricity. But eventually someone need to to convert that electric signal back to a layer two frame. Well, first, it's converted into a bunch of digital 101010 and then converts it back to frame, which then the frame convert to IP, which then the IP, we take the pieces of the data and we understand that DCP segment and then we understand that, Oh, do we have a session here? We need a state in this case. And do I need to decode, serialize and deliver to the application so you can serve your beautiful HDTV request, for example? Here's an example, actually. From from the point of view of a sender, we're sending a postal request and I specifically to the post because I need to send body and we cannot send body with get request right so we have to do a post. So sending a post request to an TPS web page here to be aware of this means that it's encrypted. So there's tools here, but we'll come to that the application post request with the JSON data to actually be a server. That's the that's what that application does. So it you use Axios or you use Fetch or you use your Python request library and we send our post request with this blob of Jason and said, Hey, send it. The presentation here takes your JSON object, which is just the object here and serialize it to flat bytes things. Right, because you're going to send an object doesn't mean anything, right? The object is meaningful in your language only. Yeah, it does. It doesn't mean anything when it comes to it's a string. You need to convert it to bytes. Right. And shove this bytes as a bunch of data here. So now we have this JSON and we have the protocol, we have other stuff as well. Information the session here. Do I need to establish a DC connection? Do I need to establish deals? Yes and yes. Yes and yes. We establish that ECB connection because we need to communicate. We're going to explain all this beautiful stuff. We're going to explain how this large DCP connection, what is DLLs really mean here in a minute. Right. So the encryption that happens, so we have this is what happened in session where you need to start a state transport, send the Syn request target port four, four, three. So that's the actual first thing that we send, right? This is a direct command from session layer because to establish a connection you need to send the send ciac ack. Right. We're going to talk about this. Don't worry if it's confusing, but but don't say that. It, it send just the send request. We're paused here. The JSON doesn't go away, doesn't go to the other party because we don't have a connection yet. So you establish a connection, you receive a connection request, then you authenticate and then you send the data here. But let's just say I'm just sending the send request for a minute here. This then is sent to port four, four, three. Why? Because the transport understand what ports are. Why? Four, four, three. Well, you said PDBs and that's the default port. Four, four, four, three, we understand four, four, three as the port for this and then network the network layer which is we we shove that send request which is the segment down to an IP packet and add the source and destination IP. So if you need the IP address, you need to do a DNS to get the IP address. And once you get the IP address, we shove it into this packet. Well, don't worry, we're going to have a beautiful diagram to explain all that stuff. But data link goes each packet goes into a single frame and as the source and destination mechanism. So we need to figure out the Mac as well. That's ARP going to go on it. Each frame becomes a string of bits and the physical layer just 10101010. These one zeros are converted into this radio signal or electricity now or light and then that's it. And take this with a grain of salt. Whatever we explain here doesn't have to be clear cut and dry, as they say. You know, some layers can inherit from each other. I'm doing stuff at the end of the human build. This, you know, nothing is just whole layer for layer five. You can argue about all any of that stuff. Receiver What does the receiver do? The receiver will receive the physical layer first, right? So we received a bunch of radio or electrical light, right? Because your server might be on fiber, but your network, you can have send that from Wi-Fi. See the beauty guys. Do you see the beauty of this? I can send your data from Wi-Fi. Right? And then you receive it through fiber. You might say, duh, but I absolutely appreciate this. So now the physical layer at the receiver side converters, the digital bits and then the data link. Okay, all the bits from layer one are assembled and the frames, the frames from layer two are simple to enable a packet. Oh now IP packet. I know that the destination IP is this for me. Yes, it is. Right. And every letter, by the way, asked the same question as well. I'm going to come to that like is this packet for me. So the transport, the IP packets from layer three on a symbol into TCP segments, you know. All right. And then this is what congestion controls, the flow control then through your transmission and it is this the the thick was this the rice packet? Is this the old packet? Is this a new and going to order all that stuff? If it's a segment that is a sin, we don't need to go any further into more layers as we are still processing the connection request. Here's the beauty, right? Since it's the connection request, we don't really go down. There's no point to go down to the session layer. Well, we have to go to the session layer here to establish that connection. And once we have the connection, we're going to have our Jason back in this next request and then we're going to go all the way to that connection here. And then we're going to go to the DC area. We're going to do the presentation layer DC rules that fly by string Jason back to the Jason object for the app to consume. And this serialization here could be completely different from the serialization that happened on the client because you can just, you can serialize from JavaScript but serialized into Python or you can serialize from C-sharp and disk realize into go, you know, it doesn't matter. And that's where your logic is. That's why a lot of people say really the presentation application layer is just the application, to be honest, like this is the application, my application is doing this logic at the end of the day, and you can argue with that. Of course, again, take it with a grain of salt. The application understand that JSON post requests and it says, hey, your express JSON or Apache request at the end of the received the event and it will trigger that event which says, Hey, someone just sent you a post request. What do you want to do with it? That function that will get called it will be triggered. Say, Hey, someone just sent your request. You can consume the request, go to the database, do something and then get back synchronously in the response back to the client. All right, finally some diagrams. So this diagram took me a while to build. Let's see if you guys like it. So the first thing client. So the client and server notice the arrow were going down or going up them is the client is the sender in this case and the server is the receiver. Does it mean that the server doesn't send data? It also can absolutely send data. So the application layer goes to the presentation. We talk about it serialization, decoding, encoding. All this shows such an layer establishing a connection, whether this deals or or the handshake itself, the TCP handshake establishes the concept of a connection, a stateful thing, then transport actually working with a TCP flow control in the ports and all that jazz, you know, the actual segments go down network, right? The IP packets itself go down data length. This is yellow. This orange. I don't know if it's clear. Hopefully it's clear. Hopefully. I don't regret the choice of these colors, you know, the physical layer, which is the actual medium. And then we send it and I chose the waves. In this case, the server receives that we received the first and from the physical medium because that's how you receive things, you know, the lowest layer your network card receives this signal and then convert it to the data link and then convert it back to the IP address. No IP packets all the way transport and then session, you know, go to the presentation and go all the replica so you just dirt. You see them all right. And then go all the way up. It's like this. It's you. Now. Let's go, dirt. The question is, do I have to always go through all these layers? Absolutely not. I'll give you an example. If you if you send a TCP. No, let's go. I'm sending initiative request. But I need to establish a new connection. Well, the application sends the request. You know, the presentation does the Jason thing, the session pauses the presentation. They say, wait a minute, session. I don't have a session here. What are you talking about? Let me create a session. Wait for me. Wait a minute. Just let me create a session real quick. So we go send the cynical two, two, two, two, two, two. To that we send that send request to create a connection. Goes all the way here. You seeing this point there? This is important to see the point. Otherwise the whole thing is pointless. You see, go out and reach the session. Okay. I agree. I'm going to create a connection. We don't go up. We don't. There's no point. There's no application yet. This is just a request to send a request to send a connection here to establish a connection. And then we establish this. Okay. Snack, bomb, bomb, bomb, bomb, bomb, bomb, bomb, bomb, bomb, bomb, bomb, bomb, bomb. Go back is sent. Okay. Okay. If you were a snack and let me finally do NEC two, two, two to go back and then boom, stop here session. Now we have a full connection. Now the session will say, okay, now that we sent the EC, you may unlock and go further. All right, you have it now. A session continue. Remember the pause. JSON request is now resumed. Go to the transport layer or element, put it in a segment. This is you put your bytes strong JSON into the transport layer, shove it into an IP packet, determine the IP, address the destination shovel and do the data link, determine the MAC address. Obviously we already determined all that stuff in the handshake, but I'm just explaining here again and then send that. Then send the physical layer, draw all the data, transport, transport all the way, go to transport layer. You have a session. Yes, you do. Presentation layer. What do I do? I have the beautiful JSON. I have all this data and this could be a single. Packet or a segment, if you will, or multiples. So this will do the assemblage will assemble all the packets that necessary and then deliver one huge string to the presentation layer which will decode it or this realize it, go back to application and all of a sudden you just triggered that lesson. Uh, you know what the reason was called the post writer, if you will, if you're using express j j as here, just that now it's trigger. So look at the work. That is happening, guys, and that is the purpose of this course. Understand what is happening. And there is beauty of of understanding. I don't know what that means, but you get the point. All right. So this is a segment. This is the package. And there's a phrase I'm going to repeat that million times on daily. This is a hammered frame packet segment that is critical to understand. This is another view that I like to do. I personally do. I never seen anyone do something like this before. I just this is how I understand it. It not necessarily this hour looks like in the wire. You know, if you well or this is how I will look, I look at this. This is the Russian matryoshka. If you are aware of this, this doll's inside a doll. This is exactly what I as I'm all you application. You go to the presentation layer, go to the session layer, and the content here goes into a TCP segment. You add a destination port and a source port left and right, but not necessarily left and right. It's just literally at the end. But I like it to do it this way because I understand it better. And then this this green segment shoves into an IP packet and that IP packet. Now we have to add the destination IP address and the source IP address and then the whole IP packet gets shoved in into a frame and boy, it benefits and off frame. And we're going to talk about the frame size, of course, called an empty you maximum transmission unit. You cannot put an IP ad IP packet into multiple frames unless you fragment, which is something we're going to talk about in the future. Destination Mac and the source Mac, you pour that and then the whole thing becomes registered with thread blob of radio signals and we're going to do the same thing. Hey. Get the bets right from the physical layer. Understand where the frames start, where the frame ends, because that's what the physical layer does. And actually, they are to death. Healing does that for you. And now, hey, we have the Mac address here. We had the source. Mac, take the data portion of this frame. That becomes the IP packet. Hey, there's the destination IP. This is also IP. Here, you don't see this. This is shove the heat down into that frame, right? You have to unpack it. That unpacking takes a finite amount of time. If you nanoseconds, if you will, you know, just understand this nanosecond time that is critical. So move up. Take that IP packet. What do we do? The IP packet. Hey, destination IP is a for me. Yes. Take that segment. What do we lose? A segment? And back at there is a segment destination port surfboard, which application this is destined to. There are thousands of port here. Oh, you want this port? Okay. Let me deliver that segment to that application, to that process. Because based on the port, you going to deliver this way, right. And this is something we're also going to explain. Now, each port is technically one process is one port. This can change, obviously, but this is just a general rule of thumb. And then we understand, do I have a connection? Yes, we do. Do I need to do anything? The presentation layer? Nope. Do I have to do anything? I application here and we deliver the app effectively. Here's another slide that is very important to understand. You want to say that the client is not directly connected to the server, right? This is the client. This is over. There are switches in the middle. Those are outers. They're not proxies. The A ends. That is reverse proxies. You know those load balancers? What are those suckers doing in the middle? They look they peek the content. And when they peek at the content, they take a finite amount of time because they make decisions based on this. Of the data. All right. So let's go through an example where a client want to talk to a server, but it goes to a square generator. Right. So the client will send information as usual. You know, the client was in information as usual to go through the application presentation session, transport network data and their physical. And then we'll send. Right. The first thing you might hit in your organization, maybe switch the switch connects different substance together. You know, hey, this is the network. This is a subnet where this is happening the way I want to connect them together. And I don't want data to be sent unnecessarily to a different network. You know, that's the power of the switch. You know, it understands where to send the data based on the frame itself, based on the Mac address. That's the trick. It does like, oh, this computer's connected to this port, you know. So the Mac address of that is this. So it goes to this board, you know, so it doesn't necessarily unlike a hub, for example, where just literally the hub broadcasts that the data to every single port, which is a lot that is a lot of bandwidth wastage there. But the switch to do its job. It needs to look at the Mac addresses because that's how it does its lookup. So it only be to take the physical layer. Convert that to data link. Look at the frames. Look at the destination Mac address. And then that's it. It doesn't need the IP addresses mod, so it's more to most. The switches don't. But some do. But most switches. Just look at layer two. That's it. And then once it does, it is like, Oh, I'm done. And then sends that data to their next sports. Right. So it does it does re transmit the data, if you think about it here. Well, now let's go through our router. The router will do the same thing. The router will need the physical layer. Obviously, we'll go to the data link. It will sometimes act like a switch if you're sending to the same subnet. But the most important thing, routers need to run out and routers need the IP addresses and order out. So it needs to go up to layer three. To root. That's why it's out. There is a layer three device while the switch is a layer two device. Very critical concept to understand. So you go up two and then go down and then rather goes all the way to layer three and then go down. And then until you reach the application, you might go to multiple routers and we go up to layer three and go down up to the area and go down underneath the application where we go all the way to application if necessary. Again, because sometimes you're establishing your connection or you want only to go to the layer session, the session layer, or sometimes the, the packet is invalid and we'll it will fail right here today. You don't have a connection. Why you me sending me data with a connection? You know, that's the TCP idea effectively. So that's why UDP doesn't really have a concept of a session layer if you think about it might be wrong there, but if you think about it, there's no state and UDP. UDP is stateless. You just send data and there's that. There is no state. But that's that's a very critical concept as well. My favorite let's add a firewall. Let's add a layer for proxy. Let's add a seed in content manager content delivery network. So a firewall, guys, what does a firewall do? FARA blocks certain applications from sending data or blocks and blocks 13 unwanted packets to come through your network. And in order to do that, it needs to look at the IP address. It needs to look at the ports. That's why the at least the ports, some firewalls are are they go all the way and look at the application. These are the firewalls that look up the here. They're effectively called transparent firewall. Transparent by all the transparent proxies really are transparent like your ISP. Everything up to here is available to everyone. Everyone can read your ports. Everyone can read your IP addresses. That is why. It's public. It's not. It's never encrypted. Never. The network openness. You need. You need the IP address in order to travel data and the ports. You need them public as well. All right. So these are public. That's why anyone looking at this, they are called transparent. Transparent firewall, transparent proxies. If you ever heard about this, these are proxies that doesn't really change anything. It's almost like a transplant. They go through it. And that's why your ISP can technically block you from going to any website they want. They don't want you to go there. A lot of our government uses transparent proxies. You know, they don't go all the way to the application because if you go to the application, you need to decrypt. You need to stop the session and decrypt it. Right. And to do that, you need to serve the certificate of the server, which you don't have. Right. Unless you're Kazakhstan, which literally forces a certificate into every computer and they're citizens, you know, to the certificate authority for trusting it. But most of the world, they're not like that, you know, so they are always in transparent. The only thing you can do is just block the IP address that you're going to. You can do that. All you have to do is just stop. Don't send the data right, because it goes through you. And if something goes through, you can stop it. So the firewall stops that layer for proxy if you want to do the load balancing based on a port. Hey, if you're going to a port at 80, I want you to rewrite the packet and send it to to this address instead or send it to this address instead. Right. So you can rewrite the packet right here, change it effectively, and then send it somewhere else. Actually, if you think about it, your ISP can do that too. Right. Can't get to can send you somewhere else if you want. If they want to know anyone, I say your ISP because your ISP is where your first packets go to. Right. That's why a lot of people use VPNs. VPNs effectively. I didn't add it here, but VPN is actually a layer three protocol if you think about it. It takes the IP packets and put them in another IP packets. That's that's a simpler, simpler solution for the IP for the VPNs. Most Vivian's not doesn't do this, but one solution is to put an IP in an IP. So I don't care what the content of this IP packet is, it could be an issue request could be a geographic database because I don't care. It's an IP. And that's why AWS, I, I'm one of the really critical, you know, take that and just shove it there and then send it, you know. So the VPN is actually a layer three protocol, if you think about it. Some people actually live in the layer two as well. No. So. Yeah. Go all the way here as a firewall or a proxy. But then if you have a load balancer, like a layer seven load balancer, you know. A few built one as a back injury. And, you know, like in generics where you want to cache data or you want to balance based on certain paths, like if I go to slash whatever, slash pictures, right? Go to this server if you go to slash images. Pictures are the same thing. Images, slash, whatever. Blob. Go to this set of servers if you want to do that. The slash, that path is actually an application concept which is encrypted most of the time. To look at it, you have to decrypt it. So the Layer seven proxies or content delivery networks or Layer seven, they play at this layer. That means they decrypt everything you send, look at it, cache it, and then send it. So they go all the way there. So they are way slower than the routing or file wise. Lower. You go and go all the way up and then re transmit everything back down. So you effectively know you need a completely different session between this, those two copies, right? Because this session, this is one session and this is one session. This is the final endpoint to you as a client. You're connecting to this. You know, that's why this is called also a reverse proxy. You know, where this is your final destination, but your true final destination is actually this back in server, which you don't know anything about. That's how Google work and any other application. When you go to Google, you really don't touch the front end server. You're just going there. But the Google server might turn around and send requests to another server on the back end that you have no idea of, as I call a reverse proxy. While a proxy goes through, you know, you're sending a request, but you know, the final destination, you know, you know this is your final destination, but the proxy makes that request on your behalf, right? Does that make sense? All right. So what are the shortcomings of this Orci model? What is and what has too many layers which can be hard to comprehend? And definitely that was my belief and opinions back maybe three years ago. And I changed my mind ever since then, you know, I really thought I really session and presentation. I think this this shouldn't really treat as one. But now the more I understand, the more I build back in application, the more I start to read and understand. We really need that low level, you know, breaking down sometimes, you know, not all the time, but is really hard to argue about which they are. That's what it's like. People still argues like all presentation or should only really describes or presentation layer should really decode. Okay. And then do they. It's all people, you know, speculating of anything. So it's it's really not funny. You know, just everybody has an opinion. It becomes an opinion at the end of the day. Right. Because there is no if you look at the computer, there is no they are fi. You know, it just is not something you will read as something we talk about or our. So us engineers can talk through things, you know, so similar. It is definitely simpler to deal with. They are five, six and seven as just one level, which is the application. And that's what the the Cppib model does exactly that. It's much simpler. It's just the application layer. Hey, five, six, seven is the application. I don't care don't, don't complicate things which I, I have mixed feeling about this. It's up to you. Really. What do you think? But. Applications on application, right? Some people say, hey, don't done don't do that now just hey, one layer application. I don't care. But yeah, avoid numbering the CPI B layer, it's not layer five you know, or layer four, layer three you can with therefore layer three, it's the same language, you know, because they are four nearly identical to the OSA model. But layer five is if you say they are five, you have to specify or this is the TCP IP, all that means this is the application while layer five and and also is actually the session layer you know so the can become real cumbersome to talk through layer two same thing data link and physical layer is really something never talked about as far as I know. Right. So I don't even see this layer in the TCP IP model. So it's literally the TCP IP, which is DCP and IP and the data link, right, which is the MAC addresses and all that jazz. All right. How about we summarize this beautiful all say model? I believe one of the most important thing specifically when you write an applications, you know, it's really good to understand where your application sits, if it is if it's a network based application, if it if you if if data passes through your application, it's good to know where your application is so that you can talk through a SU. You can know what you can optimize, what do you have access to and what can you improve? Right. It's very critical. Right. So we talked about the OCI model. We talked about the we we showed examples, many examples where firewalls, we added proxies. We had a reverse proxy read the content delivery network CDOs like fastly stuff like that. They all every ETN is a layer seven reverse proxy. That's what they are just you know, that's a glorified naming see content delivery network because it stores the content by definition it has to access it has it need access to your content, right. Right. So again, take it with a grain of salt. You know, each device doesn't have to map to all the seven layers. You know, it's really blurred. The line is blurred sometimes. You know, the TCP IP is definitely a simpler model. My personally, I prefer the the finer grain quality of things, the breaking things down. Now while we don't talk about session five as layer six is the really the layer that I almost never hear anything anyone talk about, right? But layer five I've seen use cases where someone, hey, I'm actually a layer five. We're playing a layer five here. Linker This is a clear example. One envoy, I believe another one, right. Hey, we managed sessions. We managed connections ourselves, you know. So you're playing it, you're playing a session layer, you're playing at the connection, you're playing at that just that you don't you don't really care about anything else. You're building a proxy at the end of the day, link up. There's a proxy, by the way, if you didn't know that. But yeah, guys. Hope you enjoy this lecture. How we jump into the next one.


### Host to Host communication vtt

Coast to coast communication. How messages are sent between hosts. I struggled with naming in this lecture, but I found it very critical to add just because we knew we need to talk about just how to host are talking to each other at the lowest level. You know, and this is what really you need to to understand that we're going to talk about not open windows than what else is. This is layer two concepts. Most of this stuff really and layer three, sometimes we need to send a message from host to host. B What do we really need? Usually this is a request to do something on Hornsby. Hey, I'm sending an RBC call. Hey, B, do my work for me, please. Right. Each host has a network card with a unique media access control or called a mac address. So if you ever heard about this, you can check your MAC address right now. Go to your network properties. You must have a network address. You know, if you have a VM, a virtual Mac address is created for you, you can create that. But those this is this is how it looks like effectively. This is a mac address. This is one Mac address. This is another Mac address. Right. And they are what, the lowest level of communication. Of discussion, you know, of these these are the lowest level of communication that it needs to happen regularly, you know, so if are able to send a message to be so B, C, D is the MAC addresses, I just simplify them here for simplicity. Just wanted to make sure that it clean and clean. You know, I don't want to add the whole, you know, whatever number of bytes here, but l want to send a message to B and this is the, the, this is the network we have. Literally everyone is connected to each other, the mesh network, as they call it. So everyone on the network will get the message, but only B will actually accept that message. Why? Because, you see, when I send, I have no idea where to send the data here. This is this is a analytical homework, almost like a broadcast. Hey, the send message. Whoever have this Mac address, please get my message. But is the thing we have to go up to layer two here to look at the Mac address and compare it to mine. So everyone, hey, b c DNA will get this frame of data name and then we'll look at it says okay is this is destined to be I am c drop this is destined to be I am d drop. No, this is destined to be I am b take it and send that frame to the higher levels the layer. So layer three, layer four eventually going to get it, all of it if necessary, you know. So that's very critical if you are in this configuration, which is a very wastage, if you think about it. And not only wasting is actually security wise is dangerous. Right. That's how what happened in Wi-Fi public Wi-Fi connection when you send packets, right. Everyone in the Wi-Fi because Wi-Fi is a mesh like everyone is connected device is not so like it's a wired right knows like I'm connected to this machine and this machine connected to this you know I only connected to this no Wi-Fi as everyone everyone will get the Mac address as that's how people and public Wi-Fi sniff passwords back in the day. It doesn't happen anymore. But if you have a sniffer that just gets the Mac address but doesn't really drop it because hey, I yeah, this is not for me, but I wanted I want this beautiful frame. Give it to me. Give me that frame. I need it. So that's how sniffing networks, nothing is happening. As long as the data passes through you. So imagine millions of machines. Can you get in with each other? How? How do we effectively scale? We cannot just send the data to everyone. No, we need we need to eliminate the need to send the data to everyone. The address need to get better. These mechanisms are just random. You know, they don't have information that tells me. Information that tells me where to send it and we're not to send it. We need relatability. I don't know if that a word I, I just used it. So I'm sorry if it's not a word, but we need to out meet the IP address. That's why we invent things, guys. We only invent things when we absolutely need it. Right? I even spoke in a British accent just to understand it. Right. We absolutely need it. We need this IP address because there is a problem with MAC addresses. You know, a lot of people say, hey, Mac, aliases are supposed to be global. Why do we need IP addresses? My Mac, my iPhone, my fridge has a unique global Mac address. They will no two machines will have the same device. Why do we need another address? Why complicate our life? We do. Because of this Barbie relatability. We cannot rout. Hey, let's say, okay, if you will give me your Mac address and I'm going to send you a message. You to close the globe there. Absolutely no way you're going to get it, because how do I know you get it? You would need to scan the entire network to find it. It's literally the same thing I talk about in my database cause, you know, it's just like a scanning it. Whole entire table is slower than using an index, you know, to narrow that path. And that's what we need to do. We need to slash the number of machines that we need to scan. And that's why we have an IP address. The host to host communication. I built this lecture just because I want to explain the concept of mach an IP address and why we need to. Because of IP. Because I'm not going to explain this again or this is very critical to understand and I want to take it for it. Just grab the concept and then run with it. The IP address has two parts. The first part is and divide the network and then the second one to define the host. So it's really while this it's this much bytes, I believe it's four bytes. It's four bytes. Yeah, four bytes. But it describes the entire, you know, sets of 4 billion devices. But how does it do it? You know, we don't really scatter 4 billion devices when we send an IP packet, we use the network portion to eliminate many, many network. So I only send this packet to the network that is destined. And then that network we we effectively only have this many hosts. I'm going to explain that the host path is used to find the host and still we still need the MAC addresses. There is no escape there because at that low level in that private network, we need MAC addresses to just deliver that frame it. Now that we're local, send it. So here's an example. We have ABC here and I use the same name here just to confuse you for more host on network n and then wants to talk to host B on network in two. So this is not work in two and this is in one. I used this abstraction. We're going to become a little bit clearer now. But this is a network, right? And there is a router that allows them to network together. Right. Raster is a layer three protocol. So it needs to see the IP packet to look at the addresses in order to allow. So A-one, two told me this is how it's done, right? Those two A's and the browser and then the router will send them back it to be. Right. So here is the IP address, the IP addresses for. Four bites? No, a part of it is the network, and part of it is actually the. The host. Right. I'm going to explain how is this actually effectively as is specified in a minute. But for example, 192168123. This is 192168812. And this is 19268121. And these are different network. 192.168.2. See that? 192.16.1 is the shared between this network. This is the network section. Why? Why did you look, Jose? And why did you look at the first three by? Why not the first two? Well, because we told you that this is the network. The first 24 bit is the network slash 24. You know, there are other stuff like class AA, class B. Personally, I don't. I never seen them used. Right. The class eight, class B, class C, you know, this is the future. No, the classless thing. That's why I don't want to explain things that. You're not going to mention in reality, it's as theoretical at the end of the day. Previously this was Class A, this was class B, class C. Then they noticed that they made a mistake because, oh, it's really a waste of, you know bytes IP to use class and Cosby class C so they move to this classless thing, which is basically let me know how many bits is your network because you can use one bit for your network or up to 31, you know, because we have a. For 8 minutes. Right. So now we have the network and this is the network. So that's 94, right? So this is the shared part. If this part is different. That means you are on a different network, Poppy. So now 192168123 wants to talk to 192.168.222. These are different networks. How do you know this from network? Simple. There's something called the subnet mask. You apply that, which is basically this slash 24. And if you get a different number, that means you are not in the same network. If you get the same number, if you get the same network, you're on the same network. If you are not. Then you say all bets are off. Send it to the net to the router, which is your gateway, and that gateway will take care of routing it to somewhere else. So it changes things in the process. There's a reason I do think about. So, yeah, so that's how it's done. You're out from one network to another network. This is a pause. If you don't have this logic, then you have to send it here as it is here. And then to here and then to here as it is here. So you have to scan six machines just to find that. No. But here you immediately know, because this writer knows exactly where this machine is. It will send it immediately to it. But my host have many apps. It's not enough just to address the host. Right. Because now. Yeah, you send me a a packet. What do I do with that? Don't I have many applications? My. My machine's not really running one thing. No, it has all sorts of things. So we need another fine grained level control beside the IP address. You can send a request on Port 82 right on the same machine and the DNS request on Port 53 and says this request on Port 22, all running on the same server. How does this happen? Ports is what? What? What do we use here? We use ports in addition to the IP address to know. Okay. Yeah. The Rather did a job deliver the packet to me as a host. But I. Need to know where do you want to go, which process? I have thousands of processes and that the port, the destination port and the source port is also important to understand where exactly you need to go. So host the host communication summary. I really out of this lecture to talk about the IP address, why do we need it? Talk about Mac addresses and then the problem with Mac addresses and why would we introduced IP addresses and then talk about the needing of the ports. And I talk about really we added a flavor of adding that, talking about the concept of networks which we will explain in detail in the future, but talking about different networking, you know, how what does it mean to send one packet to another network? What does it mean to send a packet to the same network? All of that that will be will be demystified. But I just wanted to kind of tease up our guys to introduce this concept, just to have an overview, because this is kind of a fundamental really and understanding as in my opinion. And finally, we really need the idea of ports. I am you noticed that everything in this guide, I say, why do we need something? And I just why, you know, and this is the only reason I'm doing this and it maybe a little bit annoying. I'm sorry, but the reason I'm doing is because I personally ran into these questions before, you know, I was frustrated. Okay, why do any ports why do you need IP addresses? Why do we need a network? All of these questions. I try to frame them in a way so that I have an answer that will reveal and demystify things like let me know if this is this helps or not. But yeah, four ports are also another concept. So host to host communication really critical to understand these basic. Building blocks, you know. IP address, MAC address, network host and port. You know, simple things. You deal with them on a daily basis. Viewer back in engineering network engine you know even the front end user you send port request or request to port all the time but and you specify the IP address as well. Right? I wish I added another slide to talk about localhost, but it's kind of clear I localizes the IP address that is loopback. You know, never leaves the machine. You're talking to the same IP address. 127001. No, that's the IPV four and call in one k so IPV six. But we're talking about the same machine that we, when we listen on a server DB server locally and send a request, nobody else can talk to this machine because we listened on local horse ram. That's another constant ad and in addition, in the summary. All right. Thank you so much. Let's jump to the next lecture.


## Internet Protocol (IP)


### The IP Building Blocks vtt

All right. How about we start this section, the IP protocol with the first lecture, the IP building blocks. And I named it this way because I think these are the basic, you know, building blocks that we need to understand in order to understand the IP protocol. You know, we're going to talk about things that I mentioned in the previous lectures like Subnet Mask and Network and the whole host and, you know, class lists and stuff like that. We're going to talk about all that stuff. You know, I'm just trying to demystify this as much as possible. What happened when you actually wrote the packet? What happened when I sending a packet? No, I'm mentioning the word packet to guide. And I want you in your brains to think about what a packet is, is the layer three. Now, when I say packet as layer three, that means it's a bunch of data with destination IP address, with source IP address that's set with other headers. But we don't care now. Right. But that's it. Ports. I don't care for the. Yeah, that is ports inside of there is the trace and the data, all of this stuff inside this IP back but to the rafters it's just an IP packet and that's the beauty of this whole all is I think, you know, if you think about it, let's jump into it. IP address is a layer three property. We talked about this. It can be set automatically or stateless, statically, statically. Right. And this is the DHCP thing. I'm going to talk about it, but the idea is the IP address can be assigned. You know, this is a way or a network engineering course, actual course detail ID will be valuable. You know, talking about okay, how is the CPU oracle is assigning this is a network level thing, you know, networking, network engineering kind of a thing. You know, it's a little bit more detail. So I won't gone to that level of detail because hey, I as long as I have an IP address, I'm good. I'm not troubleshooting my network. So there are the network engineers who are, you know, well versed and adept in that particular thing, you know, troubleshooting and oh, IP, the IP. And how does it work to protocol? It's all right. Maybe I will change my mind and add a lecture for it in the future. Who knows? But yeah, the IP address can be assigned automatically. Or you can statically fix it on your machine. No, and it has. I will talk about this. It has a network port and a host portion. The IP address is a four by two and it's going to be V four and it's more than that. IPV six. I'm going to talk about IPV six in this course. Maybe in the future I'm going to add more. Yeah, let's keep it law. But there's so much content as it will add more stuff in the future. But yeah, there is do a bit right. And there's two. Portion the A, B, C, D or the a, b CD. Is this is the first by a second by third by four by and the slash x. And I want you to understand this slash x, right? The X here is the network bits and what remains are the host. And here's an example. Yeah. If you say 12192168.254.0 slash 24. That is the first 24 bit which is 888. That's 24. These puppies are the network portion and this left by alone. The fourth byte in this case is the host. That means I can have up to 25d5 horse, right while I can have up to whatever two to the ball 24 networks. Now the first 24 bit three byte are network host the rest are eight for the host just talked about this this means we can have up to two the power 24. Look at how many networks do you on? Do we really need this much networks? Ask yourself again if you're a network administrator, that's that's one part of your job. And this is not something we usually do as a back in and do we build applications. We need to understand how that packets are delivered. But configuring the network is something really is better left to the network engineer who this is their bread and butter, as they say. Right? Like how many network do I need? Oh, do I need slash 24. A little bit too much. Let me let me reduce that. I am more host than networks, actually, and you can play with that. This is a ward by itself. That's why I really mentioned that the discourse is not really for a depth network engineer want want, teach you all these low level stuff, you know, CCNA and passing all these exams. No, this is actually decided, designed for our software engineers who want to understand the network. It's a bridge now. After that, if you're really interested in networking, you might take an actual networking course and you might have a actually a better, you know, better luck with it if you want if you think about it, because now you get to understand the high level stuff. And each of these network will have 2 to 55 hosts. And this is also called a subnet. If you ever heard about this subnet because of a subnet subnet mask, very critical concept. So if I do 19216822 54.0 slash 24. This is also called a subnet and the subnet must have a subnets mask. So if the subject mass is used, if you're sending an IP packet, you know, and you have an IP address and you want to know that is this IP address that I'm about to send to belong to my subnet or not to answer that particular question because that question will. Will be forced into to, you know, logics, like if it's in my subnet to do this, if it's not of my summit, do that. Right. If it's in my subnet, you can use the Mac address to send it directly back to the host to host communication. Just use the mac address to send it directly because it's literally in the same network as you. Yeah. While if it's not in your subnet, all bets are off. You need to talk to someone who knows how to rout this and use it. This is usually called a router or a gateway. That's why you have a gateway IP address. In every network you assign, there's a gateway IP address. That's when things are unclear. Hey, what do I know that I don't know. Send it to the gateway. Subnet mask is used to determine whether an IP is in the same subnet or not. Default gateway. We just talk about this, right? Default Gateway. Most networks consist of host and a default gateway. So you have a network and you have a bunch of hosts right in that network up to whatever the host bets you are allowed to have that network and there must be one of those host must be a gateway. Why? Because those host they want to talk to somewhere outside of their subnet. They need to talk to the gateway. The gateway is just another device that happened to have two network interfaces. One network that is assigned to belong to this network, and another network that belongs to another network, or maybe three or four or five or six. These border routers have hundreds of networks. You know, they can talk to many other networks. So host a can actually talk to host B directly if they are both in the same network, you know, same subnet effectively. And they use the Mac address for that. Right. Otherwise, if A doesn't know how to talk to be if it's B is in not in a subnet and it can find out by using the subnet mask. We're going to give an example, guys only. Now, wait a minute. Otherwise assigns it to someone who might know that gateway. That Gateway might not know, by the way. Right. Because the gateway eventually might talk to another gateway that might know, you know, you get the point, right? It's an elegant design. I absolutely of it. The Gateway has an IP address on each and each host should know its gateway. Right. And it also have multiple IP addresses depending on how many networks are. Dr. Doctor. Hey, find an example and I'm going to disappear here so you can see the whole picture. All right. So, host 192168.1.3. Wants to talk to 19216812, two, two, talk. I use the word talk here. I really literally mean sending data, sending packets which to translate it to application client front end back end speak. We're sending in DTP request, for example, right now that's talking, right? Assuming 1.1.2 is actually a server and 1 to 3 is actually a client in this case. Or you want to send a curl request or you do a server. That's what talks are really mean. And then to they as I search or not as DTP DNS, all of these fits into a nicely tucked in IP packet which has a destination IP and the source IP. And that's the beauty of this. Browsers don't care about the protocol. Some routers, unfortunately, do. They look at the content. They block certain protocols, but most don't. No, but let's let's take a look. 192168.12312.21.2. First of a question, try to answer. Are you in my subnet? It applies a subnet mask to itself and the destination IP address would subnet mask it's own subnet mask. Right, because that's the only thing it has. It doesn't have the subject matter of the other machine. It doesn't need it actually. It needs the subnet mask of you, which you have. Right. You can go to your network. You're going to have three things the your IP address, your subnet mask and your gateway. Without these, you are you cannot do anything. You're basically dead in the water. You cannot connect with anyone. Right. You need these three pieces. So you take this guy, do that to five, five, two, five, 5255.0. Apply that right. If you do the normal and operator, you know this is ones all of these are ones you'll find five via this one. Right and this is zero ones and anything one, if you add anything with a one, it becomes the same value. So when you add these bits, right, this is simple byte operations. Anything that you end with a one becomes the same value, right? So 192 becomes Raymond 992168 with 255 becomes one nine. Because at the end of these, these are one one. I wish I, I built a bitmap for you guys, but you get it right. One, one, one, one, one, one. And these are actual bits, like one zero, whatever. So anything you. And he's going to become the same value. Right. Two, five, five, and one becomes one. And there's the most important thing. Zero. And anything is zero. If you add anything with zero, just nulls it out. Right. So zero and three is zero. All right. Let's apply to the second IP address two and Z two and zero is zero. And this is the output from this. The second IP address, this is out of 19.168.101926.11.120. These are the same subnet. And that question leads to the first branch that we talked about which nobody took out. I don't need to rout it. I don't need you, guy. I don't need this guy. I actually do because it's a switch in this case. It's playing as a switch. We're going to talk about it in a minute. So I am sending this data to my writer, but the writer is just going up to layer two. In this case, it doesn't need the IP address. But why? Because it is in the same subnet, right? I believe maybe the router will go to that layer and looks at the as. But it is the same IP address. Right. So in this case, it will just say, look at the Mac address. This is all right. This Mac address is actually on this board. Boom. Let's send it right here. So the router will act as a switch in this case. So it will look all it really need just there to this particular case. And this, by the way, the second one, we're going to come to that. All right, let's spice things up a little bit. 192.168.1.3. Wants to talk to 192.168.222. This guy. This guy wants to talk to this guy. Well, it's a different network, right? How did I know? Well, apply them, ask. And all of a sudden, you know that this is 19216120i, I been one, two or three. Am I right? Uh, this is my network. The guy I want to talk to is 2.0 different subnet. I don't know what to do. This is different subnet. I need to routing to someone who does. The packet is sent to the default gateway. My default gateways 192168.1.1. Now you need to send the packet to the router. I can guess what you need. This is now loops back to the first scenario. What are you sending a horse to horse directly. Now you're as if sending that packet itself destined to this guy. But the Mac address, right, is the router. And here's where every attack possible happens. This is called are poisoning. If someone here to pretend to be the router, then all the packets can go through it. And that's how our poisoning happen as another topic for another day, right? Because the MAC addresses is I need to know what is the Mac address of my router because now I need to send them back it to my router. Right. And my author is in myself the same subnet as I am. Right? I sent this to Rather and the router turns around and understand that. Hey, okay, where is this guy? One night? So the router has actually another IP address on the other end. 19.68.22 201. So router lives, double lives. You know, it lives two lives. You know, that's that's what it does, you know. So it's living two lives effectively. It's a it's an it's an address here and it's an address here. So it's living it's a literally you have two representations. So this can talk to this and this kind of is and that's how this how basically it is done. All right. Let's summarize so talked about what an IP address as talked about what is the difference between a network portion and the host portion. And you can know what is your network today if you want and you can know how many host can it possibly happen and you can configure your router to assign more or less. Host Right. Based on that and we're talking about the subnet, we are told about the subnet mask, very, very critical concept to understand. And we're going to talk about more why this is a critical and in the final section actually where if your database is actually in a different subnet, then you're back in application. Then IP packets that are in form of TCP connection request, you know, from your back in application to that database is going through the router. And if this routing is congested, then you're going to see delays. So this is a little bit of a tease and spoiler alert, you know, to that section. But that's one of the thing that just makes sense. Do not put your database in a different subnet. You know, sometimes it's not that that to be honest, it it's okay to put it in a different subnet. But what happen if this rather is so congested, if it's if it's talking to thousands of the networks, right. Or so many networks and so many host it's routing packets, then there might be chances where the router will buffer will fill down. And your beautiful sequel statement request, which will transmit to multiple segments, will be posed in the router and it will never reach the database until a few millisecond later. And you will see those delays. You will knows why. I do have delays in my back in application. That's just tiny, something tiny and is immediately you understand it because now they understand what is actually happening. Now go check your back in configuration. Do you have your database different than your your application pull in the subnet because it costs you nothing. Right. In this case, put a switch. Put an actual switch. This is where network configuration comes into the picture, actually, right? Where you get to put an actual network switch and you talk a high performing network switch and you put the database and the application took it to the switch that the application has no business talking to the router, right? Don't use the router to run out application packets to your as a switch. You know that's just a cheap switch at this and right. Routers is not they are not designed to be switches that's why those high performance switches are cost a lot of money and they're price switches. And now again, I'm not very versed in this, but if you really want to go to that level, that's a network engineering, bread and butter. You know, that's where. And that's what I want. I want to bridge that gap. That's what I'm talking about, guys. Back in Engineer. The moment you understand this, you will ask the network engineer to do this configuration photo because you don't have no idea what you want. Right? Because they know everything, but they don't know what you want. Right? So if you are a network engine, you're talking. Of course. Thank you. But you now understand the requirements, right? But now we're bridging this gap. We're closing the wound, as they say. But yeah. And the default gateway, the concept of the default gateway also has been explained in this lecture. How about we jump into it and move to the next lecture or the first lecture of the IP packet RFP thing? You know, there there's so much, so much to discuss here. Let's jump into it.


### IP Packet vtt

Now that we have discussed the building blocks of the Internet protocol itself, I'd like to go and take a moment to. Take the lid off of this and dive into the IP packet itself. This is the anatomy of the IP packet as jump into it. I'll write the IP packet, the anatomy of the IP packet itself. You know, we're now we're going a little bit deeper. We've always as back end engineers and funded engineers, we always look at the IP packet as just a bunch of data with a targeted IP address and a source IP address, as at least I always visualize it this way, but it's critical to understand certain pieces. Not all, really. What I believe there are certain important information about the IP back itself, and this is what this lecture is about. So what does IP but IP recognize? Headers and data sections. Two sections, the data and the header. Most of the time we really care about the data. We don't really care about the headers. But if you want to debug certain problems or understand certain situations, you need to dive into this thing. One more important thing to understand is the size of the packet and headers come into the picture. If you add a 20 extra byte, that's the IP here, by the way. 20 byte extra to your data. That is a 20 by that you that's not really your data. It's it's the cost of doing business effectively. And guess what? This can go up to 60 bytes if there are certain options are enabled. So I believe those options are not always in. But if they are, you can go up to 60 bar it's forces to back it. So it is really important for these certain algorithms to kick in to save on sending like a single bite of packet, you know, with 20 header, 20 bytes header. It's just a waste of time, you know, it's just a waste of resources to send a single wide and then attach a 20 byte header to it. So that's what algorithm like. Nigel Algorithm and delayed acknowledgement try to solve effectively, you know, try not to send your else command through search as a single back is just a waste, you know. But sometimes you can escape that, you know. That's why we have the data section can go up to 65, 536, you know, and that's because the length of the data itself is as 16 bit. So you can only address up to 16 bit worth of content that's equal to the equivalent to 65,000. And to be honest, I never seen an IP packet that large at all because if you think about it right, and we're going to talk about this in later sections, there is something called the empty you name, which is the maximum transmission unit. So next transmission within the Internet is 1500. So what packet and avoid all the ideas or fragmentations you can't really shove more than 1500 words around IP packet without a fragmentation. So seeing that maybe in a cloud and Amazon where is that custom made hardware with empty you is I don't know a gigabyte gigabit empty use that I don't know if that's even possible but that will reach that but to me you will never reach that in single IP packets is almost the average is 1500 bytes, maybe 9000 certain situation, but that's sort of in jumble friends. Well, that's a bit premature. So yeah, I always like to add this IP packet to a back end engineer. It's a bunch of data destination IP address. So as IP areas, I for the longest time I always look at the IP back in like this now and effectively like this the right hand side has the destination and the left hand side is the source IP address. It doesn't look like this on. On paper. Right. But I personally like to do it this way because it fits like I met through a Scud doll, if you will. You know, I packets, vitamins, our frame, the segment fits into the data and so on. Right. I just like it. Do it this way. But here is how the actual IP packet looked like. Scary, isn't it? So this is how it actually looks like, you know, the IP packet itself and these are the references you can use that at this DA of C of the IP protocol, you know, that are broken. This is the Wikipedia that explains, you know, have a summary of all that stuff. What are we looking at here? This is, you know, horizontally from 0 to 31 is that is is four bytes, you know, so this is a bits a bits, Airbus air bits. You know, this is how it's organized. So four bytes and the first all four bytes and the second row for Biden, the third row for Biden in the fifth row and for Biden on the final one. So if you multiply by four by five rows, that gives you 20 bytes that we talked about. So this is the options and this is determined by the Internet headline. We're going to talk about it in a minute. By default, this is five which which explains these five bytes, you know, by four rows. And if it goes more than that, then that will define the length of the options effectively. And this goes up to 60 now. We'll get to talk about it so we can add stuff to these options. You know, and this is good. Those guys, whoever it to find the IP protocol, they defined it in a way to make it extensible. Unfortunately, from what I read is some writers effectively block the options for some reason, you know, because it's dangerous or whatever. So this is sometimes getting blocked. You know, that's the sad thing about the Internet. Not everybody follows the rules and as a result, you don't get predictable. So otherwise I go back in and I would have definitely use the IP packet to, to sneak in data, my own data in the packet itself. Imagine that how cool this is. You know, we always build on top of, you know, protocols on top of TCB, on top of DB to we build protocols on top of protocols and we forget how that this is available for us. You know, as programmers we can write a packet and we can send options, right? Unfortunately. What do you guarantee that these options are arrived safely? I don't know. Apparently not. You know, because these options sometimes are dropped. But it's pretty cool if you think about it. I don't know if these are user said or not, but regardless, this is the data portion we talked about. This goes up to 65 kilobytes, you know, 64 kilobytes to be specific. Know and let's go through this version. What's the version of this is either four or six? We don't have are versions of the IP protocol, right. So this is either for a specific bed. So we have zero one, two, three, you have four bits to describe that little bit of an overkill if you think about it. Right. Four bits. How much is that is that's 16 numbers. Right. Because to the bar. Eight. Exactly, 15. Right. That's a lot. So they embedded like hey, we will go up to version 15 of the IP protocol. But sadly these bits are never used, you know, because we only use for our six and now so we never lose the other bits. So it's a waste, but that's what we have today. Internet header length. We talked about this. This defines how long is the options and by default IDL is five I believe. Right. And you can add more say okay, I need 20 bytes. Right. Of options and that will in there will allow the router to read the options or not. If this is five, it will read one, two, three, four, five will read this five rows if you will. And the total length. Right. This describes the total long of the whole thing. You know, this includes the data and the header. So and this is extreme, but as we talked about, so eight plus eight. So you can go up to two to the power, uh, 16, which gives you the 65,000 bytes and that describes not just the data just above the whole header which is could be 20. You know, I do the, do the math 22 up to 60 based on the options if you have options there. But yeah, fragmentation, this is really a powerful concept. Sadly, this is very hard to get right. And from what I know, most implementations actually uh uh, are frowned upon. Fragmentation quick. Give us a good example at a quick every packet and quick the protocol, you know, disables IP fragmentation because it causes so all sorts of things, you know, because now if the packet is so large, what is the. Well, let's talk about the fragmentation really. We talked about empty. You write the the maximum transmission unit. Yeah. And you don't really need this. So again, talk about it right here. So we talk about them to you write the maximum transmission unit, which is the frame size. And we talked about the IP packet by default. The IP affected IP package should fit nicely into a single frame. And if it doesn't fit, that means if the IP packet is large for some reason, let's say the empty was 1500 and the IP Beckett is 2000. Then that is called. You have two options here. The IP package won't fit in the frame. You have two options. You can either tell the client whoever sent this Beckett. Hey, MTU you two too small. I can fit this large packet that you sent in. In that empty you fail. It will send a message. How does it send a message? We talked about the ICMP protocol. That Internet controller message protocol messages are sent through this protocol ping trace allowed things like that through the IP vehicle will say, Hey, I can throw this back at a still large write. That is only if you said that don't fragment bit but. The other option is. Fragment. This thing. The IP packet. 2000. All right, let's put 101 frame and the 500 goes to another frame. So you have one IP packet that had been fragmented into two frames. So those frames will be sent as two frames. So one packet two frames, the two frames will arrive. Guess what? None. Not necessarily in order. So the host has to understand that this frame belongs to a something that was fragmented and tried to assemble. So there is assemblage that needs to require assembly. The fragment is one of the most dangerous things. Right. Security wise. And just because people can fake fragmentation, you know, and then I won't go into details on this, but plus it's its cost that is on the whole site. So you can fragment if you want. Right. And this is how you do it. We're going to talk about it. But if you do understand the consequences, one frame might fail. You have to rescind it. And how do you send just part of the fragment? It's very complicated. And that's why the protection you know what? Let's not let's not fragment. Let's not. Hey, if you don't fit into our frame, just fail. So that's the responsibility of the client to send back it, that they know this should fit and all the empty use that the network will pass through. Back to the slides. Well, actually, I kind of like this, huh? Yeah, because there's nothing here, so. Yeah, well, the ID. This is the idea of the fragment. So if you have one packet that a fragment into four. So this is one, two, three, four, effectively write the ID of the frame flags like, hey, should I fragment or should I not fragment? So this is a flag that literally tells the client whether they're out there or not. Am I supposed to break this big packet if the empty is too small or not? So if there is a def, I believe here a flag will say, okay, you can, you can, you can fragment. The fragmented offset is that 0123 and this is ID, which is the unique identifier of the fragment itself now. So the all of these are just usually for a for far off fragmentation jumbo packages. You can get into this as well, right? If you have that large IP back, it's effectively time to live. We're going to talk about this as well. Right. Every IP packet has a bit actually eight bits, you know, a single byte that represent a counter. And this is very interesting. Before we talk about time to live, we're going to talk about a problem because what do we do in this course? We we always ask the question of why? Why does something exist? Why do we need to fragment back, as do a lot? Why do we have time to live? Because of the idea of routing that we talked about in a while. The packet can go into different routes. Right. And they can end up into a situation where they will go into an infinite loop. We go to this road, this road, and the cities are out there and there's road. And then back to the same rather. And the packet can go infinitely. And you have no idea if you visit this because the routers are not stateful, you know, they don't save the state. IP protocol is stateless, you know, they don't save the state of that protocol. Like, Oh, this package was visited before, unlike the layer layer for where we have states, we have session, we have all that stuff. IP is just stateless, no IP packets just passing through, so we don't know. State So how do I prevent packets from roaming around the internet forever? Me Time to live TTL You When you send a packet you say, okay, I estimate that this package will live for 100 routes. All right, so we put 100 year. As they go. This packet goes from you, it reaches the host. You know that gateway, of course, if you're sending it to the Internet, will go to your gateway, which is your router. The router will do minus one to the packet. So well that the IP packet arrival, the physical layer will be converted to a data link or be converted to frame the frames right will be converted to them. IP packets, you know one frame, let's assume simplicity. One frame goes into a single IP packet, that's biscuits. In audio. We have the IP packet and the router takes that. I'll be back and take the time to live decrement by one and send it again. So it goes back to the RSA model and goes off to the internet and every one of the writers and hosts that sees this packet decrements it, that's the responsibility, that's the whole contract. This way we kind of add the state right to the packet, you know, if have your program before the, you know, the cons of stateless versus def or stateless, we have to send that information with that with the request right there. Of course, in this case the IP packet. Right, send it with it. So the state always follows the data itself now and instead of saving it in the host. Right. So now if this time to have reached zero, whoever they come into the to zero must stop, you know, must drop the packet and must send back and ICM P message ICMP shows again. That's why I explained it very critical. The ICMP protocol shows up again so they a packet timeout, whatever. No, I don't remember that exact message and we'll say it, we'll send it back to whomever. Was that the source IP address. And that is exactly how traceroute works. Throw out works or router route. Same thing. Trace it out or traceroute you know I believe the the British call the router. Yeah. So routers you know the routers right. Will decrement that until it reaches zero. And there's a message to the sources said hey there's done. So the trace road actually does it this way. It will set a very small time to live and it will incremented. And every time we pass a hop, we reach the next hop. That router will tell us about its source IP address. So we know effectively we trace the entire road. Obviously some routers disable ICMP altogether, some firewalls disable ICMP. And if that is the case, that's where you get the dot, dot, dot, dot and trace it out. You cannot to basically trace it in that particular case. So that's down to that very, very important concept. How many hops can this packet survive? And this is usually 128 number or whatever. You set it as a large number, not too large, because eventually the packet will live for a long time. No, I believe that whatever the default number is enough. If you see your packet takes more, maybe longer path and they are dying, maybe the client will increase that number protocol and we they of saying what we we know we are in the IP address. No this is the protocol of what is the content that is inside this data. Now I always find this when I first saw this is like why that's kind of moot. I have to define the protocol and you have to know my protocol. I want to send the strand of data. Doesn't have to be a protocol. I want to invent my own protocol. Are you going to block me then? I thought about this, right? I say, Hey, just read the data and figure out the protocol. That's why that's what. That's the first thought that came to my mind. Why do you need an extra eight bits waste, you know? And then I thought about this. You know what? It's actually a good idea. This way the Raptors don't have to pass the entire data. They can just read that 20 bytes header and immediately figure out what is inside this and whether they should pull the data or not. You know, that's ingenious effectively. We always do this. You know, this method takes animated data to save performance and obviously it's probably decided to block or want to do anything else. You can just read this protocol. So our protocol could be ICMP is part of a would that it does this DCP or other UDP whatever the content of the IP, the IP data itself. That's the protocol here. There's a list and and Wikipedia. But if I can share it as well, showing you over a single protocol that for example, unspecified, I think that is like miscellaneous or something like that. You can put all that stuff here. There's there's obviously a big eight bit. So how many do we have? You can have up to 255 protocols, but that's it. Source on destination IP. By far the most important pieces of metadata here to the header. Where are you going and where are you coming from? Very important piece of information. Everybody can fake source IP errors, you know, you can try. That's why it's like, okay, all my IP address has been spoofed. You know, I hear a lot of people, it's like it's like, oh, this is not this. We've seen your IP address on our forums. No, that was not me. My IP address was spoofed. I was like, Yeah. Like, spoofing is something easy to do, huh? Spoofing is the idea of changing the source. Every packet that you send right from your machine, you effectively change the source IP address to be something else. You can do that, right? But guess what? Your ISP, the packet, the IP will go through the browser. Then the first router, it hits the major router. What is what's what's the first major? Rather it's your ISP because that's your link to the internet and it will say say hey, my source IP address actually whatever that guy, you know, you can put any number you want, you can you can write that simple code that do that. Does that rewrite every single IP packet? Right. The source IP would be something else, but your ISP, ISP will call you out like, wait a minute, that's not your source IP address. I know your IP address. I assigned it to you. So your ISP actually blocks it? No, you cannot do that unless. Unless every single Internet provider will have this block. You know, you cannot spoof anymore. You know, it's not easy to spoof IP addresses, you know, unless you build your own ISP, which is I don't think it's impossible. You can do it. And then you run your own Espoo, pay money to link yourself to the internet. Yeah, you can spoof all you want that. But even if you spoof, then how, how would you get responses back? Because that's what is used to the responses, you know. So yeah this the nation IP is where are you going source is where you're coming from very critical and these are, as we talked about four bytes because this is what this is the IPV four write explicit congestion notification. We're going to talk a bit about this more on the TCP IP section. But this bit is effectively is is from an A, so from a lower layer upon layer from an upper layer. This bit is set by that outer and this is what it means, explicit congestion notification before we talk about acceptable congestion notification. And again, we're talking about this in details and the future, but it doesn't hurt to talk about it. Now, congestion is when packets start to drop you. I.B. packets will be arrived at routers like floods. You know, everybody sending data. Everybody I am as I speak, I'm sending data. This is not Life City. But you get the point. And if you playing this, packets are coming through routers, you know, at the end of the day routers to process this because they need to send thin amount of memory called the buffer to put the packets in this memory fills up. If that fills up, if you have too many packets or your router is slower like it does a lot of stuff to pass. More work means more time. More time means queue will go longer and the buffer will fill and the buffer fills. That means you cannot accept more packets. That means you have to drop incoming packets. Any packet that comes the controller of the browser will drop the packet. And when that happens, that indicates something called congestion. That means, hey, the network is congested or these characters are having a having a hard time processing packets stop. So there is a whole solution to congestion control, control the traffic in the Internet, and that's called congestion control. For the longest time, routers always drop the packets, they drop the packet. I don't care. That's it doesn't even send any message or anything like that. Just drop it and leave it alone. You know, it doesn't even send an ICMP message. I don't think so. The client have to guess what the. Okay. My packet had timed out. I don't see an acknowledgment. I'm going to assume it's going as dropped and I'm going to assume congestion. That's what happens. It's a waste because that timeout is so long and we're wait. We need better communication. Meet explicit congestion notification the router when their buffer fills up because they are IP packets, they don't deal with layer three packets. They will take this packet and says. I'm about to drop this on my bucket. It's fall. But wait a minute. I'm going to actually I'm going to actually not drop it. My puffer is about to fill. I know I'm going to fill is in. Said the bit to one. Right. Whatever. Said it to true boom notification. I'm about to get filled and then it will actually process. So the receiver will see that bit that oh oh someone actually some of the routers experience congestion. I better tell the, uh, I better tell the receiver. And so the TCP layer takes control as we experience congestion. And then the client will start communicating at the higher levels of the TCP transport layer as again, we experience congestion. So the beauty here is with this small change, we manage to notify the both the client and the server. Right? The server will reply back to the lines with the same bit. So everybody will know eventually and and they just manage to know that there is congestion without any packets getting up. Beautiful design. I absolutely love how elegant such small to bits can do. Right. And and to be honest, I don't know why do we have to go bits right? We can go into more details, but this is the gist of it. Beautiful. I absolutely love this stuff. You know, we can learn so much as back in India from these elegant designs, you know, because we waste so much when we build applications, do we build all sort of you know, we allocate arrays like thousands and thousands of by Jason bloated Jason we duplicate keys all around. We took duplicate responses from the database. We put it everywhere. We send information we don't need. So all of this stuff really hurts me when I see a response from an API that that we just got back up, you know, we send the same value over and over again. These kind of things makes me crazy. You know, back in the old days, they had a limit to work with and they appreciate this limit today. Engineers, we don't have a limit from what? Islam I don't care. I have 700 gigabyte ram. I don't care. Let me I look at everything I want and unfortunately we lost that, you know, source of scarcity. This is just me ranting and that's pretty much this is the most important thing that I like to explain and when it comes to the header that I've ever had. All right. How about we summarize this IP packet? The IP packet is one of more elegant, you know, uh, anatomy that is, you know, the packet has headers, it's 20 bytes can go up to 60 if you have options and it is enabled. Data Section Guide Go to a 65,000 bytes. I never seen such such IP back of that lot because there is no empty you that fits it. First of all, you know, you can argue that hey, 16 bit is actually too much, but you never know. Maybe an Amazon cloned a cloud, Amazon Cloud or Microsoft Azure or Google. They build their own network and really with a large MTU that can effectively be 65,000. Who knows? They don't share this information with us. So if you have that such a large frame, you can send one IP, large IP backwards and then it fits it in a single frame. Now, I don't know the limitations of that. What can what can go wrong with that? It probably people will try to, but definitely will decrease the latency. Right. If you have your own network that all of these devices live tightly high bandwidth network I'm not going to the Internet is just between me it's on my own back in the database connect to the to the back in application all they are I don't know a hundred gigabit Ethernet and I'm to use 65 K to use and I'm going to use that. Sure you're not going to the Internet. So this is tightly local area network. So you're optimizing the heck out of everything. So I'd like to see this one day. So if someone ever, if there is an article, some wild love to read, someone actually taking advantage or if there is any limitation comes to empty use sizes. We talked about packets need to be good fragmented if it doesn't fit a frame unless you set the bed that says Hey, don't fragment the fly that we talked about, hey, don't fragment. And if you don't fragment, then if the package is too large for them to you, we fail, we drop the packet and we tell the client that, Hey, we couldn't fragment your stuff because you told us, don't fragment. Right? And that's where ICM. Uh, actually, this is where a black hole connection can happen. We're going to talk about this as well in the future. Black hole, TCP connection, Google that, right. That was the IP packet. How? I'm moving to the next lecture.


### ICMP, PING, TraceRoute vtt

All right, guys. ICMP is one of the most critical protocols that lives in layer three. And we talked about it a lot, but this is where we actually explain it. If you ever used Ping, that's ICMP. If you ever use Traceroute, that's ICMP. We're going to demystify this protocol and talk about how critical this thing is, how we jump into it. So ICMP stands for the Internet Control Message Protocol and lives in layer three domains. What does that mean? It means only there is a destination and source IPA. There is no concept of ports at that layer. There is no ports. There are no ports. Well, so just for information, messages between host. Beautiful. No, just very critical. That's why when I now we have the same language, we talk the same language. When I say layer three, we you understand what that means. When I say layer four, immediately ports. Think of ports. You know, think of DCP, think of congestion control. Think of that stuff. We're going to become more clear as the course goes. Designed for informational messages, hosted. Reachable. Hey, you send a message to a host that doesn't exist. ICMP Port Unreachable. Although port is a layer four concept, the ICMP is a layer three concept that is sent back from the server. So if you try to reach a port on a server that doesn't exist, it will send a ICMP message back if that's enabled, unless they had port unreachable, hosted, unreachable, fragmentation needed. We spent 5 minutes talking about that fragmentation. Hey, you need to fragment your packet because it's too large for them to you. Packet expiry, infinite loop and routers like hey, this is the time to live thing that we talked about, right? Until if we keep a decrement they trigger and they decrement thing until it reaches the head and then hey, back. It expired at zero. It's dead. You send an ICMP message so all the time uses IP directly ping and trace it out, uses it. We talked about that doesn't it doesn't require listeners or port to be opened. I love to add this kind of concept. You'll never find anything else this because our people take this for granted. But I like to kind of emphasize these point. There is no port to listen to get the ICMP messages. You know, you can send an ICMP message any time you want, as long as the host itself enables ICMP, that means IP packets with IP packets with the protocol set to ICMP. No ports, nothing like that. You can send these echo messages all you want. You know, this is how the ICMP header looks like. This is they're basically I throw this from the RC 792 can look at the ICMP message the very simple you know, there's a four by four octet as they say it. Right that one for a one by two by three by. So there's a white for the code obviously you can have up to 255 code, write a type of the message. Just the thing goes, this is the type, the subtype if you will know. And this is a list of everything here and all the list is here effectively checksum whether this message was corrupted or not. Rest of the headers, you know, some firewalls block ICMP for security reasons. You know, a lot of you know, that's the sad thing about the Internet. You know, you build something cool, you can't have nice things and then you build nice things and attackers find an exploit to use ICMP for bad thing. You know, some people use that for to do flooding attacks you know, to flood ICMP messages Desert Observer some use it some some some used it for back channel attacks to kind of probe the server has to find certain ports that are enabled. You know, just some firewalls actually block ICMP or some some some firewalls, time out ICMP messages like, okay, you can't you're doing a lot of ICMP requests right now. So I'm going to time you off for I think for our for example, this is why Bing might not work in these cases. So ever wonder that you pinging a machine that you absolutely know it's alive but your ping is not going through? It could be that it's not going through because the host is not available. But most of the time ping one of the packets, right. The router that you're going, your ping is going through your IP because ping is just an IP packet that has ICMP really nothing, nothing fancy there. And what it does is, is say, hey, one of those routers, drop the IP because I'll be back because the ICMP is blocked. So disabling ICMP can also cause real damage with connection establishment. This is called the TCP blackhole, effectively. So you would establish the connection, right? But then you started. So TCP IP through a handshake is we're going to talk about about but is tiny you know the data you're sending is so send and second. Right. But then so so that passes because your IP back is so tiny. But the moment you stop sending real data. Right. Right. And you say don't fragment. And then one of the empty use and the devices are so small and you need to fragment. The router will send will try to send you a message ICMP message saying hey, your, your, your IP packet is so large for my home to you, please make it smaller. But guess what? ICMP is blocked, right? If it's blocked, you will never reach you will never get the ICMP message. So all of a sudden you see that TB connection open, but your data is not going through what the heck is going on? It's like you open a black hole. That's literally what is called TCP black hole. Google it and we're going to do a ping demo a little bit in a minute here. Let me let me ping something here. I'm going to ping pong is available on literally every operating system, so let's pick something less. Bang. 19.168.2524.245. Or this is actually my router. So if you see this ping will send an ICMP sequence message, you know, and it will specify a title. Hey, we know this stuff now, right? TTL time to level 64. Title Time to live iterations and ascending 64 bytes. Right. And this is the time it takes from for the ICMP to reach this router and from the router back to me and a six millisecond. If you think about this a little bit slow, my daughter is right over there. Why is it taking me? Maybe through the Wi-Fi. Because I connect to the Wi-Fi. But look at this. Sometimes 11 milliseconds, sometimes 12. That is nuts. You know, but this is the ICMP sequence. It's beautiful to understand this stuff, right? So ICMP let's let's. Bingo. Google does comping google.com. So what pink does is like first of all, ping only works with IP addresses. Google.com is not IP. And also it does a DNS finds the IP address. Right. So Bing does more work here. Give me the IP address. And now we ping it and notice that it decided to do a larger deal. Now, just because they're Google, I guess it's farther. So it will estimate that let me put a larger title and this is like, look at this. Google is almost as close to my Rather it's like 12 minute, nine millisecond and my daughter is six millisecond. I really need to check like why is my routing is so slow and I need maybe to upgrade my router. Now, but this is really interesting, so I'll get back to the slides. All right, Pang. This is how Bing works. Right. This is the destination server. All right. Let's explain this beautiful diagram that I spent so much time trying to draw because I'm not a very good animator. Right. As you might have noticed already. So 192161 200 is the first. Rather, that three dot hundred is a second rather. So this is an ad network. This is a network. And these are their puppy devices for that network. Why don't you do the five video? These are that network and these are the devices tender handler. This is another net or so they were connected. We're connecting many networks together. So as we talked about, this, rather belongs to two networks. Right. This guy and this guy. The other belongs to two networks with rather built to do network. This rather to belong to one network. Right. And this is just one of the devices. So if I send a packet, what do we do is like I sent a total hundred ICMP echo request. That's what it means, right? Hey, I want to go to 19216, eight or 10 to 3. So this guy. 190.168.123. We do all this shebang that we talked about. No, do the subnet mask. Are you in my subnet? No, you're not. Let me send it. Right. And with the message, the packet will reach there. And we're going to decrement all the TTL. Right. So from 100, we reach 96. So, one, two, three, four. Right. Makes sense. Right. And I'm not sure if the host actually decrement that or not. We can we can we can study that and make sure like, okay, there is it 95 or 96. I really don't know. But I know that outers decrement the details. You know how we reach that. So that's that's helping. So we got the echo that server. If it supports ICMP, it will respond back with an ICMP echo or apply again. Or he said the TTL, the source, the destination is 1921688123 and the source is ten two three. Do all the subnet mask shebang and then boom, send it over and we get it. That's helping works. Let's go. A ping that is unreachable. I'm sending 219216 at a tender three, which is right there. And one of the two it is the one 1.1 to 3. Right. But look at the TTL. I just set it to three specifically just to time it out for for testing reasons here. So it which is here becomes to the packet reaches here, the router determines it becomes one right reaches here. Thereafter the committed becomes zero. This is. Wait a minute. It's not reachable. I have to drop it. So the router before it drops it, it drops it and then sends at an ICMP destination unreachable ICMP message because just an IP packet is just an IP packet. That's a beauty. Everything is just an IP packet to the layer three or outers here. You we sending all this beautiful stuff. 192168.123. Target is where I want you to send it back to. Yeah, look at this. We're sending what is sending to whoever the source was. But who is the source here now? This all becomes this, rather. When I don't succeed, it's 500. So now look at this. So now we got the machine, got back another packet, but it knows who sent it. This is the whole implementation of Try It Out because because the title dies right here and because that router sends back and ICMP destination unreachable or whatever. What happens here is effectively the we know who sent us, right? We know where the packet dies and that's beautiful. Might trace it out or route. Can you identify the entire path your IP packet takes? That's the question, Tracy, and I'll try to answer. Clever use of video right now. No problem. Just explain it a lot. Incremental slowly. And you will get the router IP address for each hop. So start with the one you're going to get immediately, right? Because that will decrement that VH zero and will immediately respond back with an ICMP message. Right. Doesn't always work as the path changes. Right. And ICMP might be blocked. Right. That's the problem here. Because if those emby is blocked, you're done. You cannot do anything. But the other thing is your packet might take one round, but the second title that you send might take another route, so it might get the wrong result. So Trace path is not really 100% correct. And here's an animation I did for the trace lot. So what do you do is we send a TTL. You saw this animation. I'm going to do it again because I wanted to appreciate my beautiful animation. Look at this TTL one. We send it one echo, send it to that destination and that immediately we get response from who? 100 says a destination. I'm reachable, man. Why? Because we set it as one. We immediately document that and we're done. So now we know that my packet passed through this guy. All right. Let me make it to. Now it survives. The first doubter minus one. It doesn't survive the second because minus one and then minus one becomes zero, because you send two, and then it dies right here. And now we get the second, rather. So we have the first. This is out, right? And it works. We know the first hope is this. The second help is this. All right. What's the third top? That's three. Then boom, boom, boom. Right. We give this four, we get it here. And then finally five, we're going to get an echo reply from the actual one. That means, hey, we actually find the destination, right? And that's pretty much it. You know, how about we do a demo? Clear. Uh, Trace. Artie, I. I believe it's in windows. Call Trace Artie in Mac and Linux could trace it out. So trace it out. Google dot com. Let's do something else. Do I have another network here? If I out my router, that's kind of pointless because there is only one hope. But let's do it. Exactly. So my gateway is immediately that right? So that's. That's. That's kind of pointless, but let's try it out. Yeah, let's. 3000. Another machine. Do I have another machine? I believe I do. All right, try this. I'm going to transfer out my other Mac book right here. 254 to 10, I believe. Right. And immediately, look at this beauty. So traceroute to this, right? We immediately go to that, right? There is no other hop despite us actually going through that outer because that's what we have to go through that outer to reach my packet. But my router here acted like a switch, right? If it acts like a switch, it goes up to what, layer two. So it never actually reaches a layer three. So there is no detail. The detail doesn't get affected if you're sending something to the same net or very critical to understand it. Right. Very, very, very important. Let's write out something, I guess, and another network. Can you hear me? All right? Google dot com. Sure, definitely. The first hop now is the gateway because wipe one fourth with the 50 to 72.142 is definitely not in my subnet. Why? Because my subnet has two five, five, two, five out of five is not me. So immediately we got to this as probably I'm going to block all this stuff because all of this is my ISP and I don't want to show any of this stuff. Right. But. It shows probably everything here, right? Shows you all the path that it took, all that until it reaches Google. Right. Now, I'm going to block part of this here for security reasons, but so I'm going to block a part of this stuff. But you can see that eventually we reached the Google network here. All right. How about we summarize? So what is ICMP? That's an IP level protocol that is used to send information back and forth. Very critical protocol. Fortunately, some browsers, some firewalls block it. And if it's blocked, that means you cannot ping. You cannot send critical information, messages that you can get into weird situations ping and try it out uses it don't fragment fragmentation needed all these messages it will use to tell you that whether your host is reachable or not, no port is reachable or not, you know, and that's basically very critical infrastructure, fundamental for responsible protocol that really needs to understand it so that you can explain things better when when it happens, you know, and in the future, who knows? Maybe we can extend this ICMP messages because we have so much tools in our, you know, as they could shed. But we never use this thing. We always build on top of other stuff. You know, I wonder if we can use the ICMP for our own purposes. Now on to the next lecture.


### ARP vtt

All right, meet our addressed resolution protocol. Although this is not entirely an IP protocol, it is used to identify one of the IP. Information, which is the IP address. Now, we talked about what a mac address is. One, an IP addresses. We talked about why do we have MAC addresses? We need them. We also talk about IO, which is we have MAC addresses. They are global. Why do we always use it? We can't the fortunately otherwise we'll be scanning the whole internet all the time. We need a way to index. We need a way to rout and eliminate half of the networks that we're scanning. We only need to go immediately to the thing we want. So that's why we need the IP address. But guess what? What if you know the IP is but you don't know the Mac, which is most of the situation, right? Why? Because most of the time, you know, users are doing what? Users know the address. People not don't memorize IP addresses, you know, and if you know the hostname, we use DNS to get the IP address. And if you know the IP address, you never almost never memorized the Mac. So in a way, to map the IP address down to a mac address, that's why we need to ARP because we almost always know the IP, but we never know the Mac because nobody memorize the Mac, right? And that's basically the goal here. ARP Address resolution protocol. I'm going to disappear in a minute here because there is not a this. The slides are a little bit crammed, the address resolution protocol. So we need the MAC address to send frames talked about this right frames layer two concept. Most of the time we know the IP address, but not the Mac. Our table is a cached representation that maps IP addresses to MAC mapping. That's what it is. That's called the our table right pose. Like how do you build it? So let's go through example. Here's a network who was both guys out in the same network and they want to send I guess a get request to port A380 I SDB here. So TCP so IP in this case, so frame has to be placed, right? So the IP address is ten 002. This is the large Mac address and this is its MAC address. So 3307 So we send target request. The destination port is 88. So that's part of the segment, layer four. So this is app, this is the application destination is 88. The IP layer three is ten 003. That's what I want to send to was the source while under port so we can know how to send back the packets to the application. 231 to I talk about that and the TCP section or the YouTube section as well. Source ten 002 But guess what? This is not enough because this is layer three. How do you actually send it? So switches can understand how to root out. Is the wrong word here. How to switch packets, how to switch frames. So now we put the destination 33. This is the destination, Mac, the source. Mac is all seven and that's how we send it. So this is the short format. Now if you CBB that means this guy is, this is the B and this is a is just for simplicity. So I took the first two digits here and this is three means ten 003 and two is mean two. Right. And it's much easier instead of dealing with whole thing and I remove the port. This is relevant here. So this is a simplified view of what we're about to send here. We want to send. Right. This guy want to send a get request. Which is will end up as a TCP segment, which will end up as an IP backup, which will end up as a frame one to send it to home to five. What is five? This guy. But guess what? I don't know that the Mac either is a five because I never talked to them because this is the our table today. Me as a AA I only know myself and this is my Mac address. It's a so I'm going to put that as a source, but I have no idea what's the destination. I know the IP address. I know because I did a DNS on this guys in 005 and I got, I got the, I got the IP address in 005 now, but I don't know that, Mac. How do I know that? Well, what do you do is. Well, this is the same thing we just talk about host two chicks if host five is within its subnet. This is very, very important concept. We always talk about this, right? So hosts do want to send to host five, right. But. Is it in the same subnet? Yes, it is. Why? Because it applies the subnet mask. Right. And it knows that isn't the same number on it. And if it if it is in the same subnet. So that means it can directly inserts the Mac address of that device. Because it's the same subnet. So it know it can know the Mac address. So it means that Mac address was five. So now host two checks. It's our table. Do I did I ever visited host life ever at all? Do I have the Mac address of five? Hmm. Never. Right. So what happens here is host two will send an all request broadcast to all machines on the network, says, hey, who has, uh, this the who has was the Mac address of IP office is ten 005. Everybody gets it. So this guy gets this guy's office. This guy gets it. Everybody who has the IP address. Ten 005. So host five replies. Okay, it's me. It's Didi. So immediately it updates it stable and then inserts that as its destination mac address. So it does an arbitrary. So the next one to want to talk to five. It doesn't need to do that auto broadcast anymore. Why? Because it has a cache in a stable. Beautiful. Banana spice things up a little bit. E Which is to want to talk to x one, two, three, four. It's a completely different server in the other side of the world. Well, what do we do? Well, the question is, are you on my subnet? Always. It is always the same question. Are you and my subnet? One, two, three, four applies the subnet mask. And definitely I'm not going to get the same network. Right. And as a result, it's not not on my subnet. So what do I do? Guess what? We need to talk to the gateway. We talked about this, right? If I don't know. Uh. If I if it's not on my network. So I need to send it to someone who might know. And my gateway will. Sure does. If even if my gateway doesn't know my. It will know someone who does. So that's that's the goal of the of this routing that we talked about. Right. But guess what? Now? If I want to talk to the Gateway, I need the Mac address of the Gateway. Now, as if you're sending a local message to your gateway. It's just another host to this point. We need to get this. If how will we doing, ARP? How do we do on up? Right. We do an op, we check the table. It's not there. So we're going to send to everybody. It's like, all right, who has the IP address? Ten 001. Right. Because that's my gateway. And how you might say how did it how did you know ten 001 as a gateway. It's baked into your machine itself. If you go now to your device, you'll find the gateway IP address. It's there. So now you get there, you can send an art request immediately, and then it'll ask a question. Who has ten zeros? You want a very dangerous question, because if you send that question, everybody will get it. And someone can fake that answer, says, Hey, I am I do have that gateway. I own ten 001 and machine four can reply with CC before that. And if it does, what happens? All the packets, all the frames that actually goes through this machine is that if the rather bad things happen because now you just passed all your traffic to a to I know that might be the attacker. This is called ARP poisoning. So in this case, in the benign case, gateway, it applies with F.F., right? Hey, this is my Mac address. And then we're going to put it right here. And this is where Nath kicks in. We're going to talk about Nat, and it's just a translation because it's not really an now to send a packet with a source as ten zero zero to this guy will never know how to write it back. We need to replace the external IP address and do a lot of magic that we're going to talk about in a future. So, Sarma, somebody. Summary. So all stands for addressed resolution protocol. We need the MAC address to send frames between machines. Right. That kind of makes sense. Right? Especially in a local network, even if you're talking to somewhere outside. Right. We still need to send frames to a gateway so that the gateway can talk to the outside world. And as a result, you need to know the Mac address of the gateway. So Mac addresses are very critical to understand because we only know the IP addresses and we need to make addresses and that's up. Almost always we have the openers but not the mac. Need a lookup protocol that gives us the mack from the IPL. US attacks can be performed on ARB gold. They are poisoning. And the problem here is people can fake the gateway by sending broadcast messages that hey, this is my Mac address and I am going to pretend to be ten 001 right? This is only done when you're writing custom software that bypasses the network stack. Obviously this is the kind of advanced stuff. Probably there are tools that does all that stuff. But this is also always very critical to understand, especially when for backing engineer for us, we don't need to see this page anymore. Come back. An engineer if you want to understand ARP, the R.P. Virtual Router Density Protocol. If you want to do a load balancing using a virtual IP address, ARP becomes very critical here. What do you want to you know, you want to have one IP address, but that IP address is shared between seven machines so that the IP address doesn't change. But every time you connect that the machine, if this machine goes down automatically, the IP address doesn't change, but automatically another machine tries to will will be there. The traffic will send to the other machine. No very powerful stuff really. You know, and R plays a big role in this. Will we basically trick, you know, who answers the questions of a. What? What is my Mac? Effectively becomes a question that is answered by multiple machines, you know. And as a result, one machine will go, will be the offline machine and one machine will be the online. And if this goes offline, this machine is not answering our request and all of a sudden the traffic goes to it. Beautiful design. You know, there are many, many ways to do it, but this is one part of it. We're going to talk about this as well in the future. Let's move to the next lecture.


### Capturing IP, ARP and ICMP Packets with TCPDUMP vtt

Hey, guys, now that we're almost done with this section, that is the IP section. I thought it would be good to exercise and show you guys how the our protocol looks like, how the ICMP protocol looks, how the IP packet looks like. You know, in a practical manner. That is a very nice tool called TCP dump. If you have a MacBook, this is in a cell by default. I believe all Linux have it by default. If not, you can easily install it on open toe or windows. Download the tool. It's a very, very cool tool. Well, this is called TCP dump. It's actually you can use it for any protocol, almost any protocol that includes TCP and anything underneath it or above it actually. IP or it detects or detects ICMP. So in this lecture, I thought, I'm going to go through this tool and capture packets for the IP packets and we're going to capture for ICMP and again capture for ARP. How about we get started? So to do to do that, you can do the TCP dump with many, many options, right? One of the options is called AI, which is the interface. And we talked about it a little bit. When you have a when you have a computer, you might have multiple network interfaces, one for your wi fi, one for your lan, one for your Docker container. You can create virtual interfaces. And my interface called I end zero here. If you specify that then is going to only capture for that interface. If you don't, then it's going to capture everything and you might need. If you do all, then you might need administrator privileges to do that. So I'm going to do it just in zero four starter. What I'm going to do is I'm going to capture let's capture the ARP that gets, you know, which which is ARP is a layer two framework. So it's not not there is nothing to do with IP. Right? So it's a very low level protocol. Give me the the Mac address for a given IP address. You know, and the IP data here becomes just information that goes into the frame. So let's go ahead and do that. And if I do that, I'm just going to wait here and look at all these announcements that is happening here. And this is obviously running on my home. So I have an Amazon thermostats, I have IP cameras, I have so much other stuff. So these are constantly asking for the IP addresses for certain things. So if we if we take a look at this and let's let's start reading and I'm going to go ahead and stop the capturing for now. We're going to listen on this interface. This is the date stamp. This is in microseconds. And this tells you this is the ARP address resolution protocol. There is an announcement. So someone just announced that, hey, this hostname puro and my network and this is the basically the that that's the our packet and we can ask for more information by doing V to actually see the actual packets, you know, the IP addresses and stuff like that. This is just an overview. So then there is another request. Who has the gateway to the home dot local? This is my router by the way. So someone is trying to send a packet to the router but they do not know the IP address for the router. So they are asking for that particular thing. And TCP dumps usually tries to show you all the hostnames. So what are we going to do instead? We can do a dash in to show only numbers. Don't try to be smart, you know DC be dumb. Don't show me hostnames. This is slightly better and more readable. Yeah, here is a hit. It is. So now we get more information, right? So to get who has this IP address, now we get the actual IP address and instead we get basically that's the MAC address. Please. Who whoever have this tell 192168.245.144, which is this MacBook. Right. And here is a reply. This is the request. Now we get a reply from what this guy basically the one who actually got the answer and. But this is the Mac address is at this. So we get the Mac address. It's a pretty cool thing. So you can see all these ARP requests going through. So let's do the ARP. That's enough ARP here. Let's show the ICMP this way. If I say ICMP no. Right. Which we use, the we we talked about that and traceroute is using traceroute used in ping. So I'm going to do that. I'm just going to wait. Obviously no one is doing anything I see in blue eyes here, but let's actually do something. I'm going to open another tap and again, a ping. Mm. Example dot com. I let it ping. This is the iPad, for example. Let's go. And as we go, look at this, we're going to get a the source IP. No, this is my machine. And we're going to 93.1 84.216. Now we're actually looking at an IP packet, unlike ARP. Right? So this is the IP packet source address, destination address. Right. And this is an ICMP echo request we're requesting. That's the unique identifier. That's the sequence. And that's the length of the IP packet. Now, that includes all the headers, obviously, and this is where we get the reply. That applies exactly the opposite. Their server is replying to us with an echo reply and we got that back right and we send another a ping. The ping are going right obviously, so we can stop here. And this is the way we can measure the time and do all that stuff. ICMP sequences, you can see the sequences will be increasing as we continue. It's like a sequence 12, 13, 14, 15. The sequence is what we basically use to reply back. So that's a unique identifier for that ICMP packet. Now. It's pretty cool stuff. And there is a nice feature in TCP dump that allows us to show a little bit more verbose information. So let's go ahead and do Dash V here, right? Dash V will be a verbose mode. And now if we continue pinging and continue playing game, now we're going to get more information, you guys. And this lecture is at the end of the IP section. So you might have already been familiar with the IP packet itself. You know, the IP packet how it looks like. So pretty cool stuff. So the IP packet and now we notice that this is the additional stuff we received here, right? That this is a bit in the IP packet which is this services time to live 60 for the ID right off set whether we have a fragmentation offset and flags this these are the don't fragment we remember don't fragment more fragments or the fragmentation so we don't have any flags here. And this is the protocol field, which is number one here. That's the basically the protocol field in the IP header now. And that's the link of the IP packet entirely, including the the headers of the IP, which is around 20 bytes. Right. And this you're going to see a new line and then space here and that's the content of the IP packet. Right, which we'll have which will have the obviously the source IP and the destination IP, ICMP echo request. Now we know. So the TCP IP dump actually parsed the packets and says, hey, this is actually an ICMP request and this is all the information that we get. And then that's the second IP packet. The third IP packet and the fourth IP back. No, actually, this is the entire IP packet. Well, that's another IP back. It's pretty cool stuff. So you can play with this TCP IP dump you guys and do a man TCP dump and see all the options. You know, let's go through the description here. Read a little bit. You know, this IP dump prints out a description of the content of packets on a network interface that matches the Boolean expression. This is something we didn't do yet. You can actually filter stuff here, which is pretty neat, right? The description is preceded by a timestamp which we saw printed by default as hours, minutes, seconds and fractions of a second since. But it can also be run with a dash w which is less than four. Right, which causes it to save the packet to a file. So this is pretty cool. A lot of people save their TCP dump for analysis and you can send this TCP down file and then you can read it. Unfortunately, to read it, you need also TCP dump. It's not a text file, so you can dump it and then read it back. So let's go ahead and do another thing. Let's quit here. And do what? What can we do? Let me do a filter. How about that? All right, so let's just tell this filtering thing that we talked about, right? We're going to do the filtering. Let's say we have a lot of packets going on like let's say I'm pinging example dot com, right. And I'm also pinging engine XCOM. So another server. Right. So we have this IP address and this IP address and all of a sudden, if I do like this, we're going to be flooded with stuff, right? All sorts of stuff. So how do they I then filter the things that only from, let's say, engine X. Dot com right index dot com as or request time out so in Gen-X doesn't is not allowing me to tie to paying them so ICMP is disabled in that area. It's not really a big deal. We can ping Google then ping google dot com. See, it's good to see these kind of things live so you can know. So let's ping Google. This is Google and obviously someone is querying Google doing all that sorts of stuff, you know, and what are we going to do is I really want only the traffic to two example dot com. Right. Okay cool. So now that we have all sorts of traffic going on, I'm really interested in an only specific IP address so we can do an expression after that. It's RC stands for the source IP address in this case. Right. And you can do Z and then you basically specify the address. So this means please capture IP packets coming from this IP address. That means I'm not going to capture the packets going to this address, I'm going to capture packets coming to me. So that means in this particular case, either a ICMP echo or a replies, if I'm pinging or ICMP requests that are someone is bringing me, that's pretty cool. So if I do this now, I'm only capturing, as you guys see the replies from that and only that I'm not capturing the Google requests. Right? Because this is the example that come we're stopping in Google too, but I'm not getting anything here. So this is the way to clean your TCP dump. That's pretty nice, right? You can see all this stuff here, all the details, pretty cool stuff. So what we can also do as like, well, not only I'm going to capture the source, I want this to be in the destination. Right? Like things I am going to. So what do you do is do or destination de sd then. Based this way, we're going to capture both. If the destination is this, all of the sources this please capture them. So that's where you can go. Really a very interesting thing. And notice that we didn't talk about ports at all or TCP or UDP because that that's all going to come in another section when we get to discuss TCP or UDP where ports are introduced again and the IP, we don't talk about ports. IP is just destination source, the destination IP address and the source IP address, that's it. And few other headers. But ports are introduced in the transmission control layer and the transport layer. I hope you enjoy this lecture. I'm going to see you in the next one. Guys, enjoy the course.


### Routing Example vtt

It won't be a networking course without actually explaining around the example. This is very critical too. I want to see how my packets are rooted in my between switches, between routers, between devices, how packets are flowing. So this lecture will be a routing example, for example, to explain how things actually works. Let's jump into the routing example, how IP packets are rooted in switches and routers. Very important that I said IP package. I'm not talking about connection here yet. No, it's just no matter what the protocol is, HDP, FTP is CDB, 3G, RTC, everything becomes an IP packet, everything is shoved into the segment. Everything will have ports. I don't care. The port becomes data as part of the IP and then will be shoved as an IP packet. With what? Destination IP address. So I suppose that's what all what I care about. And hey, we can explain this now. This is the configuration that we have today. So here's what I'm going to do. I'm going to send an IP back from A to B, which is this guy to this guy going to explain what will happen. I'm going to send a packet from D to X, which is this guy to this guy. All right. So that goes through a switch in order. And finally, from B to G, which is all the way outside, we're going to explain what all of this beauty is. How how about we jump into it, A, to B, first thing or scenario A to B? I'm going to explain from sending a packet from A to B. What does that mean? Well, if I'm sending epic A to B, all I need to know. I know the address. I need the Mac address. Right. What do we need to do? We do an ARP. Hey, everybody. Who has the route or who has the Mac? Who has the IP address? Ten 004. So will reply back. Right. So who who'll get the outbreak was because this guy will get it. This guy will get it this way. We'll get it. That's pretty much it. Our request won't traverse other networks because they are designed for local stuff. So this guy won't get it, you know? So we four of these guys will get it effectively, right? And the switch will be smart enough to understand where to send the packages, the packets to here. Let's. Let's go on. So now I know the Mac address of B, right. And before we we jump into that. No, A knows the Mac address of B ram. What also when all these machines power up they send a message with that a mac address. Right. As a board goes, hey, I am ten 005 and this is my IP address and this is my MAC address. So. CS directly connected to the switch so that it has a port B is connected to the switch, it has another port is connected, the switch has another port and so on. So as long as we keep sending some sort of data, any core a message, even if it's an ICMP message, anything, right? There should be anything. The switch will keep track of the Mac address belonging to the sport so it knows that. See, assume this. These letters are actually the MAC addresses or C is the MAC address of C effectively. And so the switch knows that this border, the C, this port is B, the sport is eight and this is the end. This port is ah which is the rather rather is just another device. So it has a mac address. So now this which is a layer two protocol, so it knows these MAC addresses, knows the frames. So now if I want to send A to B and I know the Mac address of B, I send a frame and I send an IP back in which tracks it and frame a single frame. No fragmentation. My packet goes here, goes to this, which the switch will look at layer two, right? Because it's an IP packet, but it will receive it from the physical layer. Right. And then transmit it to the data link and we'll look at the destination. Mac So you want to go to B, I know where B is. B is right in this port. Let me send the frames back in this port. It will not touch this port. Will not touch this port. It will not touch this port. The power of a switch. If this was a hub, the hub is dump will just. Hey, okay. I'm just replicating the electric signal and all of them are bom bom bom bom bom. Waste of bandwidth. A waste, complete waste said. But a switch is smart, so switch knows that. Oh, it will immediately switch you here. And there's complicated switch. We have virtual lands and stuff like that where it can isolate devices. This is again, this is what I want. Explain to virtual lines and switches that elegant of the switches and all this stuff. This is the networking aspects of thing, that deep networking stuff. Right. I'm interested in in host also communication a little bit higher level and that's that's pretty much it so EHS and data to be immediately and only B gets it that's a part of the switch right so that's that's a simple example here D will send it to X all right D is ten 003x is a completely different network 192168.1.2. The router belongs to do network then zero zero 100 is a gateway for this network and ten 926811 is the gateway for this poppy no deal will ask question audio in my subnet find by doing the subnet mask and we'll find out 99216 definitely not in the subnet because it will upload we'll find out that it's not ten 00x. Right. So it's not so what what does it need to do? Well, it needs to send to the gateway. What's its gateway? The gateway is this ten zero zero hundred. Right. And find this out because it ran a DHC protocol before and knows the IP address of the gateway. But now it needs the MAC address of the gateway, which it doesn't have. So we'll send. Hey, hey, guys. Who has who has ten zero zero hundred? It will send it to this guy and this guy and this guy and this guy, right? Why? Because as a broadcast and the broadcast message frame is literally 000, I believe, and the switch will say help a broadcast. So I, I don't have choice. I need to send it to everybody. Right. So that's what the broadcast really means here. So the router will get sent and will reply back with its ah, that's my mike address. So we'll send it here and guys with router will send it back only to D. Right. Because the one sent to other is not a broadcast. I doesn't have to tell everybody. It only tells whoever asked the question because the frame that had the broadcast will have the source Mac of D and the destination Mac of 000 which is everyone. Right. So all will reply back to D and guess what? The switch will get back them. Frame says okay, this is the ARP message responds you want to go to D these right here. Boom doesn't touch the other ports. So if you have 100 ports, one touch them, switch will switch it directly to that. This is where high grade switches O'Reilly plays around. You need a switch that is expensive that can switch as fast as possible and doesn't drop frames. So now we have the beautiful, beautiful. Mac address off are now ice and frame that has the IP packet that says hey I want to go to 192168.122. Right. But so that's the destination IP address. But the MAC address is destined to r so the packet goes all the way up the the frame goes to R so so which says, hey, you want to go to R, I'm going to go to R or can get you to R, but R will check first if the frame is destined to it. Yeah, it's just in to me. Right. And then it will check the destination IP address. Where do you want to go. Oh you want to go to 192168. One two, two. Ah I know I'm a network on the other guy. I am the other side here. I'll get around you there so we'll keep the source IP address. Austin 003 send them message. All right. And then it will do the exact same scenario. The doctor will do r a who has 19210212 and it will send it locally to all of us. It's X, so we'll send the message to X effectively, and then x can apply back exactly the same thing, right? X will reply back to their outer. Right, not the outer. I will reply back with a from layer two perspective defined destination is that outer from layer three perspective, the final destination is D which is ten 003. Same thing here, right? When do you want to send a message to X layer two? Final destination is R because that's the network from layer three perspective, the IP address, the final destination is this guy. Cool stuff, as good to understand all this stuff. And finally, the routing example of B to G, which is there this guy wants to talk to G all the way, eight, eight, eight, eight, which is the DNS resolver or a cursor, I believe, of Google. So G and obviously doesn't do the Mac address or G because it's all over the other globe, right? So it does a ten 0041728888. Definitely not in my subnet. So I'm going to talk it to the router, talk to the rafters and the frame directly to the router saying I want to go to eight, eight, eight, eight. The browser receives the frame of first the switcher who the frame that switch will know you're talking to. Also it only sends it to the router. I don't. That device thereafter will get the message beautifully done. The author would say hello ten 004i want to go to 88880, this is not even a local network. This is the global internet. I need to use my public IP address. So the router uses its public IP address. It actually changes the IP packet itself. The source IP address no longer becomes ten 004. What we're going to talk about this and net network address translation, it becomes one, two, three, four and some Portland land import and it builds in that table and does all that jazz right and since sends the packet right here to the end through the Internet. And again, thousands of switches and browsers will play the same role here almost. It's a little bit more complicated, but it receives the eight, eight, eight, eight. That's basically it. So that's a full routing example with three different example I thought about. So yeah, as a summary, we'll talk about routing examples. You know, you can route through a hub. The hub will broadcast that message to all its ports, not effective, not really used anymore. So which is smart. It's a layer to a device that look at the Mac address actually passes it, look at the destination Mac and says, okay, which port that is connected has this Mac address? Oh, it's this guy I listened to there. Right. So it saves a lot of bandwidth. You know, there is there is no more garbage data well sent and will be dropped. Right now we're talking about switches, bridges kind of plays the same role as well as just separate machines on the same network and kind of eliminate the little bit of a bandwidth as well. But Switch kind of plays the major role router router vector, which plays the role of layer three. Right, and also plays the role of switch. If you don't have a switch, you know, if you want to send it to the same as to the same network, you can use the router to do that. But a lot of those are layer three protocols. They allow it packets to different networks effectively, and that includes the Internet effectively. All right. So that was the routing example. Let's jump to the next lecture.


### Private IP addresses (Alaska Airlines WIFI example) vtt

Hey, guys, what's going on? So I wasn't on a trip recently. I connected to the airplane wi fi, and I got this information on my phone, and I thought I was like, you know what might be a good idea to talk through this in a lecture, in a specific lecture, so I can show you the link, the real world with whatever we've been talking about in this course. And I thought it would be a good idea. I know hopefully you guys enjoy this. So when I connect it to this Alaska Wi-Fi, give me a IP address, a subnet mask and a router. So if you notice this IP address is private or my my, you might say, how do you know? Well, the list of private IP addresses are well defined and well known. So let's go through them a little bit before we actually go back to the picture here. So these are the well known private IP address. That means if they ever detected on the public Internet, they will be dropped. That doesn't make any sense to send IP packets with these IP addresses in the Internet. They only make sense in local configuration effectively. And some of the common thing is you might have seen this 10.000 up until ten 224525, four or five. This is the whole range. So it's the 24 bit block. So that's the host size, but it's slash eight. Effectively. When I say slash eight, we talked about this, it means the first eight bits, right? Because this is eight eight, eight, 832 because IP for the first eight bit is the network portion and that will what will basically create the mask which will define our network. So we have this much addresses, right, because you can do the math. The second network actually it's more interesting, it starts with 172 and starts 16. What does that mean? Right. It's it's actually a 12 bit network. So you take the first wolf bit from the left. That's your network. So that's eight plus four. And those guys defined the last bit, you know. So let me bring in the calculator programming calculator so I can show you exactly what I mean here. So the 16 the value 16 here, if you noticed, is what is this? Right. So that's the that's the bit. So this is part of your network. But this as a result has to be one, right? You start from one and this four bits and the rest of the eight bits here and another eight base which is 20 total bits because you have 16 bits and four you have 20 total bit. You can start adding 11111000 and then 1000 and until it reaches 1111, all of them. So what is the last bit? If you think about it, the last bit will be 31, right? Because all of them this has to remain one. That's why 172, it's 16.00 up until 172, 31 to 2 five five and to five five. So that's that's the private that's that second subnet. That is private. So if you ever seen. As 001111. That's just a normal IP if you ever seen, for example, 32.32 or 33 or anything above that, that's just a normal public IP address. They've got to be careful with that here. So now obviously the last one is a slash 16, which is the 192168. That's other popular private. What are we seeing here? We're seeing that we are, as we were assigned, one 17.19. So it's within the range. This is 16 to 31. So that's the range and that one, 31.48 because any these can be anything as long as this is the range from 16 to 31. Right. So that's that's what we're seeing here. And this is the router. That means this is the gateway. And we talked about what they did is gateway, whereas if you are sending a an IP packet which contains anything at TCP IP, you know, connection or database or, you know, just TB request, the destination IP address, if I apply the subnet and it's not in my network, all bets are off, I send it to my router and that default gateway should better know how to handle it and talk to them, send it to other networks as well. So that's the definition. So here, look at this. Right. The root are defined the subnet mask here. And this has nothing to do with this, by the way, guys. Right. This is the subnet mask. What is the question here is how many hosts? Do you think this airplane can't carry? If you think about it, well, we have zero here. That means this is eight bits. Right. And then this is 254. What is 254? Let's bring out the calculator again. Why did I close it to 54? Look at this. 254. All of these values, all of these values are set except this one. So this is the network portion. And obviously, this is also two five, five, two, five, five. Right. So I said so this is 11111111111, one one. Except that last bit. So we have a free bit here. So the host side is the eight bit right here, plus an additional bit. So we have nine bits. So if you take two to the power of nine, that gives you, what, two? If 512. Right. Well, switch. There isn't a power here in the programmers one. Oh, man. No. Right to. To the power. Yeah. Programmers don't like to do power, apparently. Right. Like, come on, come on, come on. Give me a power here. So if you're 512, so that's it. So based on the, I guess, number of seats and the number of devices, they said, okay, they could have gone with two, five, five here, which gives them one 256 addresses or IP addresses that, but that's a little bit low. So they just added an additional bit and that gives them an additional 256. Yeah, I just find it interesting to kind of look at these and actually linking reality, linking the real life with what we actually studied because this is what we miss effectively in the, in all the courses. Even in my personal studies like 20 years ago when I, when you went to university, it's like, yeah, you keep teaching us this IP address, but I, I don't know, what do they mean? It's like, what does that mean? Link it to the real world and I'll try. I might fail, obviously, but some. Sometimes I try here. So here. If. Well, if if passengers brought, I don't know, five if each passenger, let's say the, uh, the aircraft carries on and all 200 passengers, if each passengers brought two devices and connected to the wi fi. That's what that's 400 IP addresses, right? If each passenger brought three devices and they connected, then it's 600. That will fail. Effectively, the router will run all the IP addresses. In that particular case, what they think about is like, okay, what are the chances that this might fail? And the second thing that one will I want to discuss later is I'd ask how Wi-Fi actually prevent me from accessing. Certain services, right? They they allow me to use WhatsApp and messages and certain messages to do text only. But. They block request to do any images. I can't browse anything in the web, so port four for three is closed. Funny enough, I think port 80 is open for some reason. So they allow it to connect insecure so they can effectively sniff any traffic you might have, but they block four for three. So anything secure you can do DNS, you can do it, you can do the and support 53 is open but the for the WhatsApp and I messages I looked up and actually those guys the services they built these messages they do it on different ports. Which is fascinating. So what's up? Want to try to retrieve text? It uses certain port, but if it tries to retrieve images, it uses a different port. It uses a different set of, you know, services. And those is well defined ports. And I think this is all an agreement between the Y for between the airplanes and basically like firewall services, you know, big companies and those messaging services like OC, please separate them because if they don't. When a when an IP packet in a segment in this case goes through the router and it goes to all one port, you don't know what that means, right? So by separating the application to give it like meaning based based on a port, they can block certain ports and allow others and as a result, block images, but allow text. It's fascinating. So we're going to talk about this maybe in another lecture. Hope you enjoy this one. I'm going to see in the next one.


## User Datagram Protocol (UDP)


### What Is UDP vtt

All right. The fun began UDP the user data gram protocol, a very critical protocol that is used very widely and is sits right on top of the Internet protocol. So if you don't want to make sure to watch the IP section, the Internet bureau is actually because that's the most important thing to understand, because everything now and top of IP, any data is on top of IP, but this is one of the most critical ones. So how about we jump into the UDP section? Very critical. The user data gram protocol. I only get confused like why is it called user data protocol? I over I talked about that many times like why is it user like I thought everything is used by users at the end of the day. Why, why this particular thing is called user. No, but yeah, it's a data gram effectively you and the. Yeah. So it stands for user data. Gram protocol is a layer for Oracle, so it sits right on top of layer three, which is the IP protocol, and it has the ability to address something that we never did before with the IP address. Host Hey, this is the IP address of this. Host Boom. But with the UDP, this is the first time we can actually address processes in a host using the concept of ports. Now you can have this host can have many applications running and we can send to the same IP address, multiple requests that goes through multiple application in the same host and uniquely identify them by the port. And that is critical concept here. Simple protocol to send and receive data was designed for this particular and I don't know which one was designed for the TCP or UDP, but I decided to talk about you to be forced just because it's so much simpler. And it is basically it's like a simpler set to keep includes kind of UDP if you think about it. Right. It just it does the same job, but UDP is way simpler, doesn't have all the bells and whistles that TCP does, but simple protocol to send and receive data. And here's the thing. Prior communication doesn't is not required. This is actually a double edged sword, if you think about it. Right. It's a stateless protocol. And you will notice me using this word all the time. Stateful stateless. Stateful stateless. Because that's how we think in engineering, software engineering. You know, it's a stateless versus stateful. UDP stateless. There is no state stored in the server, just like IP. We don't store anything in the router or the machine about the IP packets, about the UDP. Same thing. There's nothing stored. You send a datagram it arrives and it goes away. There is unlike TCP you were going to see that TCP is still the host cause all sorts of nuts stuff from the server. Like we start at the window size, we start the sequences, we make sure the sequences are correct with the flow window controls, the buffer, so much stuff we store that DCP but UDP is stateless. Another result, prior communication is not required. Hey, you can just send a data game to anyone stateless both about it. No knowledge is required and it's a very tiny header. Yes, we still need headers unfortunately for UDP. So the IP package has a 20 by todo a very large rate if you think about it and then add eight on top of that for UDP in eight byte header diagonal. Here's a use cases. Some use cases at least video streaming. Right. You want to send video, right. UDP is a great because you really don't want you don't care if some of the frames a frame here. I mean the video frames, you know, not the layer two frames. So you don't really care if some of the frames are dropped. You know, it's like you're going to get some distorted, right? UDP is not really guaranteed delivery. That's one thing we should really talk about, right? So as a result, if you want to send a lot of huge data that you don't really care about consistency hundred percent than good VPN. Same thing because you can't use re VPN with TCP. Right. Is just a bad idea. You get something called TCP meltdown because you already communicate with DHCP most of the time. And then if the VPN itself packets everything into segments, then you have to retransmit the retransmission. It's just becomes really annoying, you know? So VPNs, you mostly use the UDP easily, most VPNs use IP directly. So it's IP in IP, it's called, hey, put an IP in another IP backer. So your IP packet that gets sent regardless the naked IP packet, the VPN will put in another IP packet and encrypts that right and that IP packet will be destined to the VPN server. So all where your ISP sees is IP, a flood of IP packets going to one server. It's like what is going on, right? So immediately you go to they go you see the IP packets going to one direction, but the VPN will unpack that IP packet and we'll see another IP packet inside it that goes to the actual correct destination. That's why the VPN knows where you're going, but the ISP doesn't make sense. So we kind of kind of talk about VPN here. So UDP uses VPN some some implementations uses UDP. I believe open VPN uses UDP, if I'm not mistaken, DNS. Domain name server. We don't know. We didn't talk about it. Specifically here, but it's a very well known protocol that resolves the hostname to an IP address because most users only deal with the hostnames, you know, Google dot com, Hosain and also dot com. But that resolves down to an IP address that IP packets can be sent to because you cannot send an IP packet to a name string. So you need this protocol to. Flip the hostname to an IP and every time you have a mapping you have poisoning. So there is something called DNS poisoning at all. Convert IP addresses to Mac. There's something called our poisoning because you poison the well as a rule. So anything anytime you have a mapping, you have a problem. So there's DNS poisoning where someone intercept this UDP back and change them to point to somewhere else. Very nasty stuff if it happens. Because someone can go to google dot com, send a UDP packet, dadgum right intercepts it and the response back was like, hey, okay, this is actually Google does go and put your IP the IP areas of an attacker server. So you just direct someone to resolve to your IP address instead of the actual Google IP address. Very scary stuff. Web RTC, Web RTC, Web real time communication. I made a whole video about it on YouTube. It's a very popular web protocol that uses UDP because you see UDP is not exposed directly as a raw socket in the browser. I guess for security reasons, nothing is really exposed as the raw sockets either. Use Wan-Bissaka's which uses TCP, which uses HDTV, which uses DCB, or you can use web RTC to communicate between two peers directly through UDP effectively. You know, and web RTC is the protocol for that. So it uses UDP and and there are many, many other cases that I didn't mention here. You can build your own UDP protocol game or as a game implementation, usually use UDP because they don't want the extra overhead that TCP has. And I just GCP does so much and it's a good thing for if you want out-of-the-box communication, but sometimes if it does too much you go to the you to be. So thank god we have a protocol that is simple like UDP, you know, peer to peer directly. So let's talk about multiplexing and multiplexing. So very simple. The idea of multiplexing is take multiple input and shove it into one. The idea of demarcating flexing. Take one input and. Put it into multiple output. And that's what it is because the idea we have now different apps each have different port, but we have one IP address. All of these will be shoved into one wire as a single IP packet that goes to a single destination. So you're multiplexing multiple inputs into one and then you're multiplexing it into the actual target applications. Right. And that's the power here. So you can send to Port 53, consider packets to Port 68, and you can send a packet from to Port 69, 78. And then you always need a source port. I get this question all the time. Why do you need the source port? Because how do you know how to send the data back. Yeah, we know that I source IP and but how do I know which apps and that. Packet write we are data gram. To be specific you would need the source port. We need the destination party. So that's basically it, right? Ports. Now identify the app or the process effectively. And I just realized that I'm covering that thing, right. So this is this is what we had here. So port 53, 68 did take over something in the here. I don't believe I did. So yeah. App y port 68 and then app Z is 6097 there. So app one can talk to app the app two can talk to app x, app three can talk to app z. It doesn't matter as long as there is a unique identifier of these four pair or four or stuff. Source Port Source IP destination IP destination port. And I use this to uniquely identify where you're going. Every data gram is uniquely identified and once you understand this that ECP will be easy because it is identical. Same concept here. Alright, let's take an example. App one on ten 001 sends data to app excellent and 002. So app one is port 55, five, 55, 55 and then app. X is part of history, by the way, spoiler alert 53 is actually a DNS server port. So there's attention paid for the 53. Right. And this is what the packet looks like, right? The IP packet will have their destination. IP URLs ten zero zero to the source ip address ten 001. But the destination port is 53 and this is that data. This is layer four. The green is layer for the orange is layer three. Right. So 53 is the destination. And then five, five, five, five is the source. There's other stuff as well we're going to talk about in the next lecture of the actual UDP header. But this is it and app X responds. How does it respond? Well, it puts the destination as the IP address destination. It puts the destination port in the destination port segment. Right. In the UDP section. Right. And then the source is 53 and the destination, the source IP resistance 002. That's building that very simple stuff. So as a summary, UDP is a very simple layer four protocol. It has a destination port, has a source port, has a bunch of data you can put the data in. That's it. Stateless doesn't have any really state doesn't have any prior knowledge. You don't need a connection established. That is a constant. Is the connection less that has no concept of a connection that I establish between the client and server to connect it. Because they know each other. They connect. They don't need to know each other. They just send packets. And then the packets are arrived at this destination, you know which port you're going to, you know which port you want to respond back. That's that UDP. Very elegant, very simple, very popular use cases are really limitless. Video streaming DNS, Web RTC, many, many use cases. Let's jump to the next lecture. I'm going to talk about the UDP packet anatomy.


### User Datagram Structure vtt

Alright guys. The UDP data grabbed the anatomy of the UDP diagram, guys. This is the lecture where we're going to actually take a peek in how the data actually looks like. All right. The UDP header is only for a88 byte. Actually, an IPV four is actually different than IPV six data come slides into an IP packet as data. So that acronym is exactly with its headers with its ports. Would the data that you sent the DNS request, for example, slides into the IP packets as data. Now, I probably, probably are bored of hearing this, but I'm going to keep repeating it right as just layer four slides into layer three as data. So the IP packet doesn't really care what's in it. You know, there is a bit that says protocol. So the protocol IP header now says UDP in this case get it that's that's the power here and ports are 16 bit from 0 to 60 5535. That's how much ports you can address effectively in your hosts. Right. You can address this many applications. Some of them are reserved, obviously, but this is kind of all the ports. All right. I'm not covering anything. So should be good. This is how the UDP data looks like. Again, four bytes here, right? So eight bits it bits, eight bits, eight bits. And we have one row and another row and a third row. Yeah. Right for the data. So only literally one, two, three, four, five, six, seven, eight. Right. So eight bytes header, that's pretty much it. Let's talk about the source port and destination port. Very simple to understand. You have 16 bit, two by two for each port, the source port and destination port. We talked about this source port identified where this application is coming from, where this process coming from. Right. And it's unique. The unique identifier here is E each process has a unique port. You can do the math if you think about it. Let's assume all of these guys, right? Let's assume I'm going to fix the destination port. This was some little magic here. I'm going to do a port. 80. I'm going to what port? That's wrong because her it is not UDP 53, which is the DNS. And we have the source board assuming this is fixed. Assuming the IP address is also fixed, which is the IP is what is the is the layer three, right? If you fix the destination port, if you fix the destination IP address and if you fix the source IP address, let's say this host isn't a jet. How many applications can send DNS packets to the same destination? Port Asia, which is 53 to the same server, which is whatever the server is fixed and from the same host. You can only 65,000. Apps or processes can uniquely send. If you spin up more than 65, five, five, three sec, whatever that number. You will fail. You cannot because you are going to occupy all your effectively ports. So that's that's something just to understand it, because I made a video about this, like what is the maximum connection limit or what is the maximum? Is there a limit? Of course there is a limit. Will you reach it? I don't know. Right, but. But this is it, basically. If you fix all these, it's all map of end of the day, you fix the destination board, you fix the destination IP address, you fix that is the source IP address and you change the source port. You can only have this much applications sending stuff from your host. Right. And that's it if you fix it. But it's almost never the case because you're never fixing things or I'm ready because what how why are all your applications sending DNS requests like different DNS requests right. To this. Say, maybe you if you have like if you built a specific app, right, that sends spin up a process and sends a DNS request, you will die after 65,000 requests because assuming they will be in parallel and they will expect there will be the response will be resolved by the operating system. Right at the end of the day, you're going to reach your limit. So just think about it here. Link that you examined. Very simple, basic. Think of what's the link of this data and then what's the sum of this whole thing. So a small checksum that is used to tell you that, hey, did this packet get corrupted or not in the way? Because at the end of the day, all of these packets go to the wire and voltage attenuation and electrical stuff and light stuff, and then a nature happens, you know, so, so a bit can be flipped. And if the bits can be flipped, one thing can be something else. So you need to check some to kind of verify the whole thing. So even if the checksum is bad, right, you still can know that this back at is better. So it's unlikely that a bit will be flipped and the checks and will be flipped by the powers of nature that will match. It's unlikely they will happen. So this is used to data. This will be used to check the integrity if that happens. All right. How about we jump into the next lecture? We're going to talk about the pros and cons of UDP. Let's do it.


### UDP Pros & Cons vtt

Alright guys, Europe is great and all. But everything has drawbacks. Everything has a power. So this lecture we're going to talk about the pros and cons of the UDP protocol. So UDP is a simple protocol. That's the process here. It's a very elegant, simple protocol. It's does it does exactly what most developers want. That's why, like, I've seen a lot of applications like gaming application where we want to do multiplayer gaming, they use UDP is like, Hey, I don't want to all the bells and whistles. ATC does so much for me like at least congestion controller. I'm controlling everything on my network. I know what I'm doing and only if you know what you're doing. You can use lower level protocols, you know that to do that stuff. So the simple protocol, it does exactly that. No. Hey, data would be corrupted. I don't care. It's okay. I'm going to fix it. And I can deal with a certain level of corruption. Let me deal with the corruption. Don't just rescind my packet, but just because it's corrupted or it's just it's lost. No, I can deal with certain losses. Please deliver the packets as fast as possible. Latency is what matters sometimes in certain applications. Correctness is also another thing, right? And this is your job as a back end engineer and a fund manager to decide do you care about correctness or consistency as the database people call it, or do you care about latency? There is a slider that you move. Hey, I am willing to get eventual consistency for those database people. This is kind of a reference or a rather good performance rather than complete consistency or strong consistency. Yeah. Let me get eventual consistency. But. I know. Rather good performance. Oh, no. I am pessimistic. I have pessimistic concurrency control. I need full consistency, realizable isolation level, and I need everything to be consistent. I don't care if it takes a few millisecond extra. But I am getting correct result. You can sacrifice. Sometimes you can. Sometimes you can't. So header size is small. That's why as the results. So data grams are so small. Use this list better with others. All right. And it's stateless. The beauty of statelessness is it doesn't occur by memory on the back end. And the GRONDIN goes stateless that there is no state so scales nicely. This up while it takes a little bit of motor memory, it adds up and you can run out of these resources. Right. If you have so many connections, the grand scheme of things, it's not really a big deal, but everything adds up. The end of the day. So it consumes this memory, the nothing stored in the server or the client. There's no state. It is stateless, you know, and low latency. There's no handshake. You can literally send the data and immediately reach there. There is no latency. You know, there is no prior knowledge communication. There is no order. That has to be. Faust. You know, there is no sequences that labeled every package or segment is labeled with sequences. And you have to they they these segments better arrive in order. If they are not in order, ask the client to rescind it, to retransmit that and acknowledge only that it is already transmitted and guarantee that it doesn't have any of that. So yeah, it's not really great when it comes to correctness, but some developer says, hey, I'm okay with that. It's I'm building and building a video streaming app. And and so if some of the frames are dead because I'm going to I'm going to figure out I'm going to use A.I. to reconstruct the image on the client side, because I can. You know, speculate how it'll look like. So latency is low latency is very critical for me. So pros and cons. Nothing is perfect. But now let's talk about the cause. It's actually the causes, just the flip of the pause. Fiona one no acknowledgement. So you really, if you send something, you really don't know if it reaches or not. And that can be bad in certain situations because you really need to make sure that the message has arrived, you know, to make it to to to to I need to update certain state and your clients. I do special for database application you can really use UDP because although otherwise you'll corrupt your database immediately. Right. There's no guarantee delivery. Right. I guess that was a pros and it's also I can't zone the there is no guarantee delivery. If you send something it won't arrive in or arrive in order to be to be specific. Right. And if it's no guarantee delivery, then there's going to be a problem because my packets will not arrive. And if the doesn't arrive, then the server I do. I have no idea. First, there is no acknowledgement. I don't have an idea that the server actually got it and the server state was updated and the database was in this state or not, I don't know. And this lack of knowledge is scary sometimes connection less because it's connection less because the parties are not authenticated per say. This is like a soft authentication, right? Anyone can send anything to anyone and that's why UDP is used for attacks, right? One popular attack is actually a DNS flooding attack where you use it, you flood a server with three UDP packets and. The server has to process UDP. Why? Because UDP is stateless. You don't know if this is. It doesn't check. Did you talk to me before? There is no such question. It just processes everything. Right. So you can literally flood a server with UDP packets or detect. I say packets now. You get the point, right? UDP data comes, you can flood a server with data grams and the server better process that. And it will take finite amount of time to process it. And as a result, you can dose the server. You can you can create a denial of service. Another way some users do that is some attacker, they would say is use a DNS flooding by sending UDP requests to normal DNS resolvers and forging their source IP, assuming they can write the source IP, making all the source IP that target the victim. So. If all these millions of machines talking to one server and forging all their IP addresses the source to be their victim. The DNA sort of cursors. They are scalable, right? There are thousands. They will reply back with a reply to that victim. And the victim will says What is going on? Why am I receiving the illness? Replies I never asked for one. So. So you will. You'll start. The operating system has to process that. And when you process this much data, grams yield effectively die and the server shut down. So yeah, no prior knowledge is actually bad. No flow control. No congestion control, no order packets. All of this are lumped because they are prose of the TCB. If you send data grams to this data delegate's server, you have no idea of the server can handle their sets of data crumbs you're sending so you don't have a flow control. It was like, How much can I send? Can I said, seven packets? And I said, 800. Because I have I can send all day data on one. But you have no clue whether the target can handle it because you don't know the buffer of that target. I guess you can build this at the application layer if you want. You know, and by doing that, you will be a layer for protocol. Frankly speaking, you know, if you think about it, you will be you will build the flow. Control in the application itself says, okay, you can let the server responds with an woodpecker that says, Hey, I can handle thousand bytes. I can handle 500 K. Okay. That's a lot, though. Yeah. So you can build that because some people will say that, hey, okay, I don't want full control as too much. I'll build my own, you know? Primitive rock and roll and the sort of will let me know this is how much it handle and can do it. Yeah, a lot of elite, if you say engineers who build like multiplayer badass game heroes, they do this, you know, they use the European, they build the features they want on the wall, the simple protocol. They don't build everything right because these DCB does a lot and it's good and bad at the same time. No congestion control right now. Yeah, we talked about how the server can, can or cannot handle it. That's the flow control. How much can the server and the congestion because those like how how much the rotors can handle are my IP packets are going into the routes. Right. It's going through a rout as many, many others night and then more out there is the more latency and the more congestion it can congest. UDP doesn't have any of that. You don't do you don't know if you're congested or not. Talked about that a little bit, right? There is none of that. So and I have no idea how do you build that? Sometimes you don't care to say I don't want congestion control because you can never congest the network itself. Right. Or authors are protected against congestion. They will have certain amount of buffer so that you don't basically eat the whole network, no order packets. Talking about that and security, it can be easily spoofed because now, since there is no knowledge, anyone can send a packet to anyone. Right. Uh. You can send that. You can start sending data to anyone and they will have to, unfortunately, process it. Right. And the act of processing it. Cost. There is a cost to that and that cost it from the CPU, from the memory. And as a result, if you do it a lot, you can dose the machine, the denial of service. That is, you cannot do this with DCP. You can't spoof DCP, DCP. There is a prior handshake that is required. Do the handshake and then let's talk. If you never talk. You cannot just send data through the ECB, just like, who are you? It's like, What do you want? Leave me alone. DCB People say that. So yeah, this is the pros and cons of the UDP and this kind of end the UDP. Lecture. Oh, we're going to do a little bit of an exercise where we're going to spin up a UDP server using Node.js. We're going to spin up a web server using C, and we're going to talk about the different things and we're going to send some messaging, going to have a little bit of fun. And after that, we're going to move to GCP. Let's do it.


### Sockets, Connections and Kernel Queues vtt

All right. Welcome to the nitty gritty. We'll talk about connections, sockets and queues. And I'll split this into two lectures. The first part we'll talk about the in both cases we'll talk about the core kernel network data structures. But in this particular thing I'm going to talk about the sockets and connections. And then in the next one we'll talk about specifically how to read from connections and what kernel data structures does the kernel use to actually implement that. Let's jump into it. This is going to be a beautiful. And let's just swim you guys here. Let's swim and don't memorize anything. Just swim. So. First concept. Concept of a socket and a socket really when it comes to actual data structure is nothing but a struct. A struct really in C. But it's also a file descriptor, at least in Linux. No, in windows becomes an object, because in Linux everything is a is a file. But what happens here is when I listen and we talked about what is the concept of listening. Right. And the fundamentals like hey, I want to accept connections on a specific IP address and a specific Port Hussein. Use the word specific. Can I have more than an IP address in one machine? Of course, in every computer you have multiple NICs or network interface face cards. And not only that, you can technically create unlimited Virtual Network interface card, each with its own IP address. Because it's all you know, you can do anything you want, right? But ideally, you have a physical network and you have a mapped mount which have essentially that network, uh, virtual IP, not virtual IP. Sorry. It will have a neck exposed to you that you say, hey, this is my IP address. This is my subnet mask. This is my, uh, default gateway. This is my DNS resolver, IP address, all of that. It's my proxy. Stuff like that. Yeah. So you can listen on an IP and a port. It has to be those two things. Right. So that means I have four network cards. I'm listening on the IP address of this network card. You have to specify it, you know, and then you specify the port. And if you're if you've ever built a back end, you'll always see that the default is, I don't know, 888 8 or 880. The port and the and the IP is the loopback default Nic, which points to myself for development purposes. Right. So that's the tcp IP 127.0.0.1. That's the IP version four for IPv6 loopback it's bracket colon colon one. That's the loopback for IPv6. But that's basically the games we're playing here. You might say I'm saying I don't have to do anything. When I listen I just call in Node.js. I could call listen and then I close the bracket. I don't know any of this stuff. Well, this is what we need to talk to you guys. When you listen, you can give the option to listen on all interfaces, which is absolutely horrible idea. Horrible. And I'm really mad that this is a default. Apparently we have to make it the default because it's too much to look up an IP. An IP can changes and and to make it easy, let's listen on all interfaces. And you do that by doing 0.0.0.04 zeros. Or I think in IPv6 bracket colon colon bracket that listens on I o IPv6 versus all IPv4 interfaces because you can have both. Right. Watch out for this because you're building a small app that does, I don't know opens a port and you accidentally deployed it in production and you wanted like really to run it locally. You just and all of a sudden you, you run it and it will listen on all production index, which happens to one of them happens to be a public IP address or a public or an IP address that is essentially mapped to some load balancer. It's public eventually. Right. So now you exposed your app to the public. That's how every single MongoDB instance and Elasticsearch data got breached, because people just listen to all interfaces and deploy it on the cloud. And and literally attackers will scan the whole web, scan all the IPv4, uh, you know, ranges looking for the Elasticsearch, Mongo, DB ports and then connect and then brute force their way. So that's really dangerous. You really want to listen on a specific port that you actually know, right? It's hard. It's difficult. It's not calm. It's it's a it it's it requires some more work, but it's secure. Now then we have listened on a port, an IP. Hussein, when you do that you call a system called code. Listen and you get back a beautiful file descriptor called socket. And this is I'm going to coin a name for it, call it listening socket file descriptor because a connection is also a socket. But that's kind of a different kind of a socket. It's the same data structure. Right. And the kernel kind of stores them in the same file, the same table data structure. But you can see that the socket has only listening socket. It doesn't have like a source IP or destination IP. It doesn't have any of that. But a connection actually is is a is an is an entity that has two sides. It has source a client and a server. So a connection is different than a socket. But I'm going to call the connection socket for now. We have a socket. Beautiful. What do we get. We get a file descriptor. Where does it live. It lives in the process ID where PCB. We know all these terms now guys we can talk safely with them. So circuit is a file. What happens if I fork got. Guess what. If you fork uh the the the files are shared right. Because you get a copy of everything that the process had at that point including file descriptors. So watch out. Maybe this is, this is by design and this is good. So when you fork careful what you fork you fork you get everything. You get a copy. Complete copy now. Copy on write can kick in, of course, and only gets you unique stuff, but essentially you have a completely new version of that. So if I listened on a process or have process A, I listened. I got a file descriptor that's the socket, the listening socket. It's absolutely useless. Just that second listening socket by itself. We need to do something to it to get actual connections. But then what happened? You fork, you get a brand new process, so you get a brand new file descriptor that technically point to the same socket. So you have one file descriptor, one file descriptor. They are two different file descriptors, but they point to the same essentially same socket. So you can accept connections here and accept connections here. And that's how essentially how socket, how you can scale acceptance of a process, which is our next topic. So we talked about let's let's go back to our single process. We have a single socket in my process for every listening socket that we create you guys we get uh something called two things. You get two queues. Uh, don't get hung up on the word queue because they are not actually implemented as a queue, because a queue is a very it's not very efficient in this particular case, actually, one of my student called that out. So I was saying like the queue seems to be very inefficient, but I and I looked at the source code. It's not a queue at all. It has a hash table, it turns out, but we talk about it as a queue because it's easier to talk through it this way. So we get something called a syn Q and this belongs to the socket. S and we get another Q called the accept. Q what are those for? Well, if you're if you're here you probably know how to establish a TCP connection. Right. We do a send and then the server sends back a syn ack. And then the client sends back an Ack. And that's called the three way handshake. And using that we get a beautiful, uh, connection. Right. But that's the handshake. To handle the handshake we, we need this sends, which is the synchronization. We use them to, to synchronize the sequence numbers that we use to label every segment, essentially. So that's the first thing syn. Q so if you have a lot of listening you get a lot of Syn cuz essentially you also get an accept Q this stores completed connections because if I did, if I am a client, I want to connect to you. I first send you a send by just sending you a send. I don't have a connection yet. We take your sin and put it in the sin queue. We don't have a full fledged connection. We only have a full fledged connection if we successfully completed the three way handshake. Right? So then you can see that as I'm putting sends here from multiple clients and from one client in India, one client in Germany, one client coming from an IP address from Lebanon, one from Bahrain, one you see, you can see that this can fill up. And then we complete the connection. So we move this stuff to the accept queue. And we have full fledged connection from Germany, from Russia. We have these connections ready. But how many can I keep in my queues before it fills up. There is a size to those, and this size is called the backlog. And you can essentially when you call listen you can specify the backlog. And this is actually exposed all the way to the client. Even in node JS you can node and you can specify how much is your backlog. Really pretty cool if you think about it. You might need a large backlog. If you have so many connections coming your way and you want to essentially you're not accepting connections fast enough. We'll talk about accepting in a minute. Now, because connections that lives here still is not with us. The back end. This is still all kernel. Okay. So now let's talk about actually creating connection. We don't we up to here we don't have a connection yet. We only have a listening socket. That is one file descriptor. Now let's talk about connections. What happened here is like completed connections are placed in the accept queue. We talked about that after you finish when you put a send put here and then the send entry comes here until we send back the synack. And then the client will send back an ack. And then we match the ack with the syn here. And then we remove it from the queue and we add it to the accept queue. And now we have a full fledged connection, but it still is not ready. You have to accept it. Who? Who is you? It's the back end application. You might say I never call this function in my life. Well, you don't, but Node.js does. Bun does. Django does. All your framework does do behind the scenes. You must call accept which which creates the file descriptor for that connection. Yes, there is a dedicated files connect descriptor file identifier for each connection. And when you do that, then we create additional two queues for the actual data that are going to be sent. Right. So when you do that you accept you're going to create a connection. That connection is going to be a file descriptor which lives on whomever or whoever, whom or who, whomever. The process that called accept. If I am process a I called accept, then the file descriptor that comes back will live in process A. If I'm process B and happen to have access to the socket somehow, and I called accept, then the file descriptor that gets created will live in my process B's PCB process control block, and it's going to be a chain of file descriptors. Maybe I'll accept 100 connections. Okay. With everyone, you get two queues. Send the queue or receive the queue. Will will come to that in the next lecture, but each of them, the send queue will go for your outgoing data. So as a server, as a back end, if I send a response to my rest API call that goes to the send queue. If my client sends a request, it goes to the receive queue. Does that make sense? And vice versa. The client also has a connection on its end as a client, and it will have a send queue and it has a receive queue. The client does not have a socket. It does not have a listening socket. It has a just a connection right. So it has a send queue and receive queue. So if a client sends a request it goes to the send queue. And then eventually the kernel pushes it out to the neck. And then it holds the receive queue cue where the response from the server will go into before we actually take the data and push it into the user space and do some stuff with it. Let's go through the simple connection establishment. Three way handshake. Three way handshake. Beautiful. We need it. We need the handshake. Otherwise we cannot exchange information we need. We need to trust. We need to exchange certain metadata between the window size and all that stuff. We need the handshake. So we do a syn syn ack ack ack. And so what happens in the back end? This is some stuff we just talked about right. A server listens on the on an address port. Client connects a kernel does a handshake. The kernel does the handshake. This is not you you you as a back end application you only call. Listen, you did not do any of this stuff. The handshake, the connections, Actions the the sink, the the accept cue. All of this is handled by the kernel, including the connection management, the three way handshake. But the kernel completes the connection, puts and accepts you. It's all the onus on you as a back end process to call accept, to accept the connection. Otherwise it's going to sit there in the accept queue and it fills up the accept queue. And I do. And I actually in my uncover in discovering a backend bottleneck course, it's a little bit intermediate to advanced course I show you, I fill up the accept queue. It's so cool. Check out that course. I fill up the accept queue and I try to connect and it's just the connection fails. Times out. It's so nice to force errors when you actually control errors because you completely understand what's going on. So I suggest you take that course to dive deep into to troubleshooting back in application. I don't think this this venue is the right venue for that because we're talking about the fundamentals of operating systems here. Right. So let's go through this a little bit. So the kernel creates a socket gets two queues send and accept client sends send. Right. So the client library understand how to speak the TCP protocol and sends a send. And it's probably done by the kernel as well. Right. Because all the library does like hey create a connection and the kernel does send and synack and the kernel adds the syn received by the client to the send queue and replies back with a syn ack and then waits in this waiting. That wait is dangerous. People exploited that wait. You know where I added a syn and I replied with the synack and I'm waiting. We're waiting for one more thing. An act, a final act. This guy, we're waiting for that final act to complete the connection. Malicious clients. That was years ago. Of course this problem was solved. Malicious clients, uh, will send us in. Will. Will will receive a snack, but will never acknowledge. What does that mean? You have a sin occupied on the sin queue on the back end listener socket. Sin queue. Now you do it again. As an attacker. I do another sin. I get a snack. We just filled up another sin slot in the sin queue. You then just say, hey, never mind, I'm not going to reply with that. So two you do it again and again and again and again. You fill up the sin queue. You just did a DDoS attack at DOS. Not really a distributed a single user could could take on the whole client. This was solved with syn cookies, right? Which has been. Now this has all the problem has been solved long, long time ago with timeouts and stuff like that. Hey, if you don't send me the syn within 520 milliseconds, you're done or something. 20 is too small, like 100 milliseconds. You can configure that timeout as well. So kernel finishes the connection kernel, removes the syn from the syn queue, so frees up an entry in the syn. Queue and the kernel adds a full connection to the accept queue and the back end accepts a connection. Removes it from the accept queue. If you accept a connection, it's removed from the accept queue. If you do not accept the connection for any reason, let's say your application froze or you're just too darn slow. As a back end application, you're doing all sorts of things and you're not calling except fast enough because you have a lot of connections. You're doing all this work as a single process. You can only do so much. If you don't accept it, then it's going to rot. And the more connections there are live in the accept queue that will fail. Completed connections from being moved to the queue because the the client, the kernel cannot can no longer accept more sins because the accept queue is full and you don't really want to get into that case. You want to accept as many connections as possible, but you will also need to know if those connections are malicious or not. That's where Cloudflare and all these guys just, you know, spent tons of money and research on on understanding the behavior of attackers versus legitimate users. It's a very interesting word indeed. And of course, we talked about this. You get a beautiful file descriptor for every connection you accept. Let's go through some animations. So here is my client. Here is my process ID on the back end that is running. And of course these two lives in the kernel, which nothing but a mapped area in the process to the kernel stack, uh, to the kernel stage. Right. So accessing those queues. Right. Of course, require kernel mode, right, to be accessed. You cannot access them directly from the process. These are forbidden. Right? Uh, again, I o u ring kind of brain breaks this rule. But for normal operation, you cannot access them. They are protected. So the sin comes in here and it sits in the sin sink. Right. So the sin will have a from IP address, a from port, a destination IP address and a destination port. And that's the four tuples that uniquely identify this guy. And you put it right here. Beautiful. Now we have a Syn entry. You immediately as a kernel reply back by syn ack where the from and two from ip and from port is the whatever listening socket IP we listened to. Right. And actually whatever the client decided to connect to let's say I'm listening on all IPS, right. So the Syn the client cannot connect to all IPS. It has to specify one. And by knowing that one that becomes the source IP. In this particular case, if and only if there is a matching corresponding socket, because I drew this as a line. But there's so much stuff going on going on here. What is what is going on? If I am doing a sin and my destination IP is 1.1.1.1 and that's Cloudflare and the port is 80. Then the colonel first looks up the destination IP in certain table we're going to talk about. And that table is all right. Do I have a matching, uh, colonel entry that says I'm listening on this particular socket on this IP? Yes or no? If yes, let's do it. I accept you. If no, we get an error that no listening socket or we get an error. All right. So you see there is a matching that needs to happen. We need to look up. Imagine yourself actually implementing this. How would you do it. There's like billions of ways to do it. I'm not exaggerating, but the colonel does it a certain way. This lookup should be massively fast, possibly. Hopefully in the in the in the cache, in the CPU. All of these structures really need to be cached because you don't want to go look up that massive table with all the listeners, you know, you want to quickly look it up, and from this you would know based on that entry you would know which process it belongs to. Absolutely beautiful. So now we return the snack. Let's say we have a behaving client that replies back with an ack! Now we have a full fledged connection. The kernel removes it and adds it to the accept queue. Now we have a free connection here. Now then the process calls accept which will copy that connection, creates the content, creates a file descriptor and copy it to the to the user's file space, and it will create an entry here that says, all right, now we have an actual connection and it will create the two receive queue, the receiver queue and the send queue for that particular connection. Right now we have a file descriptor right here. So problems with accepting connections. Let's talk about some problems. Backend doesn't accept fast enough clients who don't. Ack. And we have a small backlog. If you have a small backlog you might you might have a problem with them. You know, you call accept. By the way, if you call accept and this is empty, you're blocked. Right. And that's why accept is also a candidate for asynchronous I o which we're going to talk about in another lecture. Beautiful stuff you guys. Let's talk about circuit sharding a little bit. Um. There are two different approaches to this. You know, I'll talk about this is the second one, which is literally getting a different distinct socket on the same port. So we have two processes that are not talking to each other yet listening on the same socket. So you have two sockets pointing to the same um IP port pair listener versus the other approach where you have a listening socket and you fork the process to get a copy of the file descriptor, which happens to point to the same logical physical, sorry, socket. So if those two guys, we're sharing one pair of sin and accept cue here, this is an approach where we're getting two distinct socket pointing to the same entry. And you do this using the socket option reuse port. Because normally if you listen on the same port and IP you get an error. Remember like socket already listening. That's a common error. But with this you can actually get two another socket and you can do it any number of time. Now this poses and we'll talk about this now in a minute. Nginx does it. Envoy does it HAProxy does it. And essentially what you do is you have the listener and the acceptors and the readers write the acceptors. All of these are just different threads or different processes. And this is what's called the socket sharding, where this process will have a socket and this process will also have a socket and both represent the same IP port. Now this poses a problem because if a client came in with a Syn right. And what do we do in this particular case. Which socket gets the syn. And now I'm imagining and I don't know the detail about this. And I'm imagining either you can have two sends or one syn queue for both sockets but different accept queues for sure. Right? So in this particular case process one will have an accept queue. Process two will have another accept queue for the same port. If I get a Syn on this port, complete the connection ready. The kernel will do a load balancing. It will throw the accept queue right. It will throw the first connection on the first accept queue the second one and the second, the third one and the third and so on. Right. And that's how we that's how they do it. So it will distribute the connections on different accept queues. And if it happens to have on this accept queues. This process calls accept on its own socket which will point to their own accept queue. And in this particular case we call accept on my socket which happens to use, but both are on the same Nic on the same IP and port. This is a common configuration that people do in order to scale acceptance. Right. And this is referred to as socket sharding or sharding the socket. But the Linux kernel in this particular case does the load balancing. So there is when they first introduced it, there was a lot of bugs with this because as connections close and open, we lost the balance of uh, we have a connection. You're done. The connection is like you have a nice file descriptor to go to. The problem with this is only when you accept connections. The load balancing becomes really interesting as as sockets get destroyed and recreated again. Right. So as I found this very interesting. So I thought I added another just dedicated slide for that summary. So the kernel manages the TCP IP part of networking. At least every socket represents a port, an IP right. You can with a specific socket options. When you listen, you can say, hey, I want to listen on the same port and the same IP despite someone else is using it. But it's not straightforward. You those two processes have to agree on certain, you know, on certain cookie or certain key to allow them to listen on the same port. And this is basically determined by the first process. If you don't know that key then you can't listen. Because guess what? What stops me from running a process in a server that happened to be listening on port 80. And I do. Hey socket reuse 80. I can hijack packets as a malicious process that is a bad idea. That was one of the bugs. Early, early. So to to fix that, you need to know what was the key that was used in the listening and was socket option reuse port actually allowed or not? Can you listen on the same port or not? So it's very, very interesting to learn about all of this stuff. So each connected client gets a connection. We talked about that. And kernel manages the dodos beautiful data structures you know and there is an associated cost to all this you guys you know, and it's not very clear from just the listening part because you don't really shove a lot of data. But for the connection acceptance I mean, not acceptance per se, but but just just receiving and actually dealing with packets and parsing those packets. There is so much stuff here that I'm not going to go through, and that's a course by itself. That's a book, you know, because you're talking about the actual packets. How do they. And there's like when you receive a packet from the from the receive data from the neck, the kernel creates a data structure for it. It's called skip socket buffers. Right. And then adds data to it. And then that there is metadata associated with it as overhead. So the kernel does all sorts of tricks to merge and to merge packets together and have less headers do so much stuff. So you need if you are like a beefy machine, you need to assign a core or two just for the networking stack, you know. So all right, I'm going to leave 1 or 2 just for the kernel to do this. All this complex networking stuff, you know, especially if you have like a beefy CPU. All right, guys, so let's move to the next lecture.


### UDP Server with Javascript using NodeJS vtt

All right, guys, now that we understand the basic fundamentals of the UDP protocol. I apologize for my voice. It's very early in the morning and what we wanted to do is to build one UDP server and I'm going to use multiple languages in this course and I'm going to use a very, very high level language assembly. And he was a very kind of low level language and I put JavaScript because I'm comfortable with it as one UDP server and I'm going to make another lecture to do a C in the future. Or where C is is very low level as lower as it gets to assembly where you're allocating buffers and it's different. So how about we go ahead and build the UDP server? I'm going to go ahead and open a project here and I'm going to create a folder and I'm going to call it an auction course. That's going to set my project up here. Yeah, it's called this UDP server. And JavaScript, and I'm going to use Node.js for that. So. So if you want to follow along, make sure to have Node.js installed. And Visual Studio Code is my editor in this case. So I'm going to go ahead and create a new index done MGC folder. And what we're going to do here is we're going to need a library that supports the data gram creation, which is the socket for the UDP. Right. This is called import diagram from diagram. We're going to install this in a minute. But because this doesn't exist. Right. By default. But what we're going to do here is get to create a socket using that library. And we talked about sockets in this course right at the corner of the socket. It tells you what kind of type of this socket is. Right. And I'm interested in you have two types here, UDP four, which is the IPv4 version of UDP and UDP six. I'm going to go with you to be before because it's just simpler. The next thing we're going to do is now this is a socket. It doesn't have any meaning. Right. This is think of it like of the file descriptor that we talked about again or building a server here. Right. So it's going to listen on our commands. So the thing we're going to do is going to do socket, double bind and the bind command tells us what port the owner bind to. What do you want to listen to? And we talked about this, the address as well. And so I'm going to go ahead and listen on .5500, you know, just random port I picked and I'm only interested on listening on the local host loopback IPV four version and careful not just doing this right the default address what is the default address here and option is that an optional address? If it does not specify, the upper system will attempt to listen on all addresses. Just be careful of that. We don't want to haphazardly list on all addresses. I think I believe I talk about this at the end of the course, know where there's a danger about this because your computer might have multiple network addresses. One is the local horse, which is the loopback, which are literally an IP address that is just sent to itself. Others might be. You had the Wi-Fi IP address. Yeah, the Ethernet IP IP address. You might have a public IP address. So if you're building like an admin, local or only API, you don't want to listen in on addresses so people can access it from the wild, wild internet. You don't want to do that. So always be specific about this. Yeah. If you're testing and building a small app. Sure. So that's why we're listing on the next thing is what do we want? What are we going to do when we receive a message? So in an event of a message, I'm going to add a handler effectively. When we get a message, give me that message. This is how it functions working JavaScript and this is the info object which will have the socket that information itself. So I don't know why they just used one object, but this is how the API is built. So info will have the socket that is coming from the client. Know where it has the source, the ip address, the source port, all the beautiful stuff. Right. So I'm going to do a console that look print message here that says, okay, my server got let's do the thing. That's my. My server got an datagram. This is the data message. Right. And from from where info. And for that what? There is info to address family port size. The size of the packet. The port. That's the source port of their client. That the random what we talked about. Right. And the address which is the IP address of the client and the family, which is IPV four, IPV six. It's called family for some reason. And then we also need to put the port here. I think that's pretty much it. So we're ready. So now if I go, I'm going to go to create a new terminal here and I want to install my right. I'm going to initialize my AMPM here and I'm going to install diagram just to make sure everything is happy and dandy. And let's go ahead and debug. I'm going to put not. Yes. And we are up and running. We should be up and running. So how do we test this? We're going to use a utility that effectively establishes TCP connection, datagram connection. And this utility is available on almost all operating systems and it's called net cat, right? You can decide to build another Node.js client or JavaScript client or any language client that connect to it. But I find this easier. So if you do net cat and do slash view, that means you are in the UDP mode and then you specify the address and the port. So I want to connect to a127001, which is localhost and 5500. That was my board, right? i500 and the message. So when you hit enter, you're now entering the interactive terminal and you say, okay, I'm going to send a message called I. Enter immediately we get trigger write to this is a debug breakpoint I put is the beauty look at this we got three bytes, the first by this edge. The second byte is I. I. The third one is the courage return, which is ten. And that tells you the leg. That tells you that all that other stuff as well. I don't know why I can't expand. Did you get the point? His email address. Family Board. Look at this. The LOC report 53618 and the size which is three bytes. And he can do that. And he is it then. My server got a diagram. Hi from WaPo blah blah blah. Like I can't continue as anything. Saying also. We will get it to you. We just. Just like that. Built a UDP server. A very simple, very elegant. But not Node.js. The API that has on here has built most of this stuff for us. You know, we didn't have to worry about buffers. It didn't have to worry about, you know, file descriptors and all that stuff. You know, we didn't have to worry about looping and accepting all of this stuff that's done for us, you know? And when we write a C code, you you'll see how this is actually a little bit more complicated than this song and small code. And if you go to the help of the datagram, you can find multiple events like event. When we first run the server, right, when we first successfully listen to the port and all that stuff, you know. All right. That's what I want to talk about on the UDP server. Hope you enjoyed it. I'm going to make the code available. Obviously, you guys, for you as a as a source code, you can just download it here and there. GitHub repo as well. Let's jump into it.


### UDP Server with C vtt

All right, guys, now that we have built a UDP server using JavaScript harbor, we go to the lowest level and try to see how is it built a UDP server. Using. See? So this is a code I took from Nicole. This is the GitHub page, if you're interested to see the whole code. I did a few changes, but in general this is the gist of how to run UDP server to listen on on incoming data grams. Right. So let's go through this. These are the libraries that we require. So a studio for printf on this lab string socket. Definitely, we're going to need the socket working with socket of NetApp and iiNet edge. So I'm going to create a main page, main function. This is where the first function that is going to get called by default. No. And we're going to hardcoded the port. I'm going to listen on UDP port 5501 very critically. I'm going to create a socket file descriptor. This is the integer that we talked about. Very critical. Right. Talking about like how is this socket file the was created. It's not created until you actually create a socket. You know, so this is these are the structures. So this is for the remote address. This is for my address. And effectively, this is the remote address. There's whoever connects to me, I'm going to allocate a structure of this type structure address because to us, Node.js, remember, it was it was just literally a string 127001. But boom, done here. No, there is a structure that's called address and you had to create two now, one for you and one for the destination. Again, this is just the address. It's not really I think it does have that port as well. We allocate a buffer of 1024. So this is an array of 1024 bytes. And this we were going to use this to receive data from the data columns, right? So you can allocate that based on different sizes, you know, by the this is how much you can handle at once. Derek. Adam Wise again, this is just a count that this doesn't include the eight byte headers of the UDP write the circuit length. Even the length of the address sizes has a structure itself. So you could define that. And here's the fun part creating a socket, just like we did that we do UDP with Node.js, we're creating a socket and we're going to create EFI in it, which is IPV four version, The Family. And then psychedelic. I mean I want to UDP and have forgot was that last zero one so once we cleared that I sign it to the socket file the script that I'm just like that we get I have a file descriptor which is just a number. This number is used by the operating system to identify when a data gram comes, it will hash all the, you know, the source destination, blah, blah, blah and find out what is the socket description lives. It's just a table at the end of the this is a pointer to a table with all the actual information stored in memory. And so it's socket file descriptor and you have a certain amount limit and how much, how many you can open and certain thing. I believe that defaults 10,000, but you definitely can increase out. I said memory something we have never have to do with JavaScript because it's dynamically allocated. This is you have to, you know, do everything yourself here and see. So I want to set the size of the address in my address. So I look at specific memory. This is the size of it, the structure and stuff populating the structure. The family is a definite name and. The port this the port specified and the address. I want only to listen to 127001. And this is the function that converts a string into an actual address in its address. And finally, the bind I want to actually listen listen on this socket, which is we created it, right? So every time you create that, you get a socket for the scatter, but only when you bind things actually happen. Because when you bind, what do you want this to? Who? Who are you? I have no idea. This didn't actually specify anything. You just tell me this is an IPV four UDP. But where do you want to bind this socket to or a binding it to this address? Which address, which board and exactly what would you do? We specify, hey, I want to bind it to this address and see you have to specify the size for, you know, that's that's how C works. You have to tell me everything. Basically, it's kind of double edged sword. I kind of like it. I love I want to go back to C and learn it. The last time I wrote C card was 2001. Maybe when I was in uni. That's pretty much it. But yeah, this tells you the address size and here is it. You're binding and now you want to receive data and the moment you receive data you will receive it on what you tell the operating system. I want to receive on this socket file descriptor and put the data in this buffer and here's the size of the buffer. I don't remember was zero odd, but this is the socket address. Whoever sent that datagram place their address right here. And this is the size of that address. Effectively, and then we're going to print off that buffer. How about we actually run this? And I'm like, Telescoped. You going to have to compile? Remember those days, guys, when you compose stuff? It is GTC by default in a stolen everyone in windows. I don't believe it is. So you have to install the the GCSE compiler mean let's see. And that's the file. That's my file. Right. Do you mean the C and Mac in in Mac and Linux. Are you going to get a executable and in Windows are going to get not E if by which you double click to execute you do what is the output error out. I don't know how to to control this name. Right. I believe that you can do a space and then specify that. And just like that, we're ready to listen how to connect to this thing. Well, beautiful, beautiful notes, cat. Same thing we did with UDP and C, right? Dash you udp mode 127.0.01 and I already forgot what the port was. The port by 501 by 501. I'm going to hit enter and c i c udp server sub. And I'm. Would I enter? That program is immediately terminated. There is no loop or anything like that. Guess what? When you call, you have to actually continue to receive data. Right. But this program was so simple that the receipt of the first data is received. It just shuts down. Right. Got data. Hi. So. And why? Because there is no loop. And we see her saying, wow, I don't have to look, but you have to. Right. Because your C program is doing exactly what you tell it. You receive data, you print it and you exit. So you have to do a loop to continue receiving data if you want. Right. So there was the kind of a deep dive, if you will, of what is exactly happening. And I can choose to print that the not addressed because I have that right here. I can print that information, you know, and I can print who is the source, you know, port and all that information as well. Print it right here. All right. But we enjoyed it. We're going to see on the next one.


### Capturing UDP traffic with TCPDUMP vtt

Now that we have an idea about the UDP protocol, how about we actually see it in action, your guys? So for this lecture, we're going to use this fantastic tool that's called TCP dump. And if you haven't already watched the IP section section lectures, you would have seen this tool already. We used it to capture IP packets, ICMP and ARP. Now we're going to use it to capture UDP packets. And the most popular UDP protocol is actually DNS. So how about we actually do a DNS query and then capture it using TCP DOM because yeah, TCP done while it was called TCP. Right. It actually can capture almost all protocols. Let's get started. So what are we going to do here is I want dash end, which means give me all the only the numbers I'm interested in the numbers do in the verbose mode. So I need as much details as possible. Capture on end zero. Again, if you want to find your interface, do an if config or just remove that altogether, it will capture more stuff. Right. And what we the next thing we're going to do is actually do a filter. Right? And since I'll be using Google's DNS server to find out the IP address of my domain hosting also dot com. So in order in this case, what we're going to do is actually I want to filter on eight, eight, eight, eight, whether it's the source or if it's the destination. So if I'm sending a DNS datagram, a UDP datagram to eight, eight, eight, eight or if I'm receiving a reply DNS reply, I want to capture both. So with that being captured, let is let us have a capture and now I'm going to do a DNS query. How do we do DNS? There's this beautiful command that's called DNS lookup and I know it's a little bit early in the course right there. Talk about DNS. I have a dedicated DNS lecture that you're going to see in the later in the course that's in the protocol section. So make sure to check that out for more details about the. But we know DNS, right? DNS is the protocol that allows us to given a domain. Give me the IP address so I can connect to it right through TCP or, you know, UDP. And what I'm going to look up is hosting also dotcom, which is my website. And in this lookup, this tool takes two parameters actually more than that, right? It takes what do you whatever you want to look up and what resolver you want to use, which is pretty cool, right? Eight, eight, eight is the popular Google result DNS resolver. So I'm going to go ahead and do boom and immediately we get an answer. So I used eight, eight, eight, eight and I got this is my IP address. And actually I have multiple ones because technically I'm hosted on Google my website. So I have like four different IP addresses. Pretty cool load balancing there. Thank you, Google. So now I want to go back and see what we captured. So this is the first IP packet that we captured, right? This IP IP packet. Remember, DNS keeps saying it is UDP sits inside of IP. Right. And. This IP packet. This is the time that it was issued. This is the differentiated services zero total time to live 64 The ID, I'm not sure what this is. I keep forgetting. What is this I.D.? I believe this is the I.D. of the fragment, but I would expect it this to be zero because I don't have any fragments. But I guess every every IP packet is fragmented by default. If it's not fragmented, it has a zero offset. And that's the ID maybe. And there is no flag. So it's not fragmented or anything like that. This is the IP header. Again, we're still looking at the IP header. So the protocol version in the IP header that is 17 which correspond to UDP. So now TCP dump actually detected that the I am the content inside it actually UDP and the length of that IP packet is around 63 bytes. Right. That includes obviously the content which is the UDP datagram plus the IP hitters which is that on 20. And now we go deeper a little bit. Now we're inside, right? This is still these two pieces right, that I'm highlighting. These are the IP, the destination IP, the source IP. So they are in the IP packet, but I want you to pay attention to the last dot. Right. This is obviously not an IP address. If you look at it this way, it's an invalid run, right? But the TCP dump adds another dot and that's the port. Now that we know what ports are, right? UDP right and TCP uses the console of ports. It's a layer four concept. So now .58 63 five is my source port because I'm sending a packet from my machine, which is this IP address, 144. This is my source port, which is a randomly generated right. And then I am going to eight, eight, eight, eight. Right. Which is the Google DNS server and this is the port I'm going to. So that's the four pairs that we keep talking about. Right. Why do we need a source port? Because we need a way for for Google to reply back to us. And if it just replied using the IP address, it won't know which application to reply to because I might have thousands of applications and running and processes running on my server. So it needs this unique identifier to know exactly what to deliver the packet coming back. Cool. So the next thing, once we see a colon IP packet is done right and now we're kind of parsing the content and try TCP. I'm trying to pass the content and show you more information. Yeah. So yeah, again, these are the port. This port are part of the UDP packet diagram. So this number is what they call the DNS query ID, right? Because the UDP is stateless, you need a tag, you need a unique identifier that you send in the UDP packet so that when they reply back they give you the same number. This way if you send like 700 DNS queries and you bring them back, you know? Exactly. Oh, this query belongs to this query ID, right? So that's, that's how, you know, basically you might argue that why can't we use the source IP to source port? I guess you can, but it might not be reliable if you're in a different in an asset environment And with Nat. Right. So use a query ID saver and this is I'm asking for the record, which means I want the IP address right of that record. And this is Hosain also dot com. And yeah, that was the, that's the request I sent. Right. And 35 maybe the length of the actual UDP, I might be wrong here, but then the second IP here packet and that's a response. Right. How do I know right there. I'm going to find out again. Doesn't look looks normal. It's a high it has a higher total for some reason. It's very interesting if you think about it. Right. This IP packet coming back from Google Response, responding to us has a 120. The one we send has only 64. So maybe it took it took it was 60, but then it was increased because it didn't reach me. Right. That's one theory I guess Then protocol is 17. We know that. And the link then that's, that's large, right? I mean, when 24, it was not that large, but in grand scheme of things it is larger, right. If you think about it and then we have this is the source IP sending us. Right. We're receiving from 8888 port 53 because that's the one we targeted. Now we're flipping them. Exactly. The destination is me, my IP address, and this is the port that I originally sent. So now my machine knows exactly what process to deliver to which is in this case a the NFS lockup app that was running. Right. And then this is the query ID, This is some numbers for what they mean and this is basically the domain name right then also the code and then a record to one 621 the second 3820 one third and fourth, and that's 99. So I believe 99 if you if you 99 plus the IP headers, that will give you around 127. Yeah, that's about right. Right. So 99 is the UDP length. Right. And if you compare the A's is the answers we received. So that was actually just an investigation using TCP dump to look at the UDP packet. So see how simple it is. UDP is so simple there is nothing else except ports. That's the only thing we we actually see here. The rest of the stuff is just the content of the DNS protocol specifically. Right. This is the UDP. I hope you enjoy this lecture. Let's jump to the next one.


## Transmission Control Protocol (TCP)


### What is TCP vtt

The Transmission control protocol, one of the most popular protocols there is, and most applications are built on top of it. Most reliable application. It is the reliable protocol, and we use it all the time. It has its pros and definitely has its cons. And this is the section that we're going to talk about it, transmission control protocol. Let's discuss this. Let's unpack and let's demystify this thing. Let's jump into it. The transmission control protocol and I'm going to disappear off if I'm covering a slide or not. Right. As as necessary. But it stands for the transmission control protocol and the focus on this or transmission and control. It controls the transmission because usually it does transmission, but it doesn't control it at all. Just sends stuff. You know, this is controlling the protocol and it's a for protocol. Exactly like UDP. That means we get beautiful ports and we get more control there. We get so much algorithms here. We're going to talk about all the algorithms that TCP uses to control the transmission. What does that mean? Ability to address processes in host using ports? Yeah, same concept. We talked about UDP exist in TCB. The ability now that we have ports. Right, which is something did exist in IP. Right. And third protocol now we can send multiple requests to the same host, but two different applications. So process one can receive a packet process to kind of see what different kind of content is really kind of different all using the same, you know, IP, IP host effectively. And this is ideal for multiplexing and multiplexing, right? It controls the transmission, unlike UDP, which is literally a firehose. Just we can send it and this virus can be dangerous. And that's why firewall firewalls sometimes block UDP because of this problem, because of firehose, firehose and firewall. It's a tongue twister, man. Now, that is a concept of a connection, something that never existing, never existed, and the IP never existed in the UDP connection, which requires the mother connection. That means a session. That means there is knowledge between the client and the server and this knowledge is a state they store on both the server and the client. I miss his memory. There is a state, there is a stateful. And to build that, a state required a handshake. And look at this. Oh, my God. How many? How much hater does it need? It needs a 20 bytes. How much? The IP packet needs 20. So 24 headers for the IP packet and another 20 byte header for this segment. Right. So it's a TCP segment. It's an IP packet. It's a layer two frame, Ethernet frame. And it can go up to 60. Exactly like the IP. So you can worst case scenario, you can have 120 bytes in a TCP IP transmission just off nasty headers. That's it. That's the maximum. I don't know if you can ever go there. Theoretically speaking, but yeah, 60 plus 60. That's 120 bytes image of sending just the less and the that's a search command and then add another one to any bytes that will speak of a wasteful. So stateful is a very stateful protocol. Use cases at I'm not going to cover my head. There is B, here is another server. Here is you can see it. It's B, that's it. That's what it is right here. I didn't see my saw so I can see my hand yesterday. I talk a lot. I move my hand a lot. So reliable communication that I mentioned that this slides are available. So go to the first lecture and download this entire slide and you can see everything from reliable communication. That's what you use cases for these to be reliable communication. If you've want to chat, you probably don't want their message to be received scrambled and you cannot possibly construct that message. It's a message. How would you how would you know what the person wanted to say? You cannot is not a video where you get, okay, this this frame looks like this frame you can extrapolate. But this is text says if I say hello, you cannot say, hey, go away. You can just change the message, right? So reliable communications, very critical of remote shell or database connections. You probably don't want to send a sequel and that sequel is jumbled to do something else. Imagine like a you send an insert or a select staff from employees and it's scrambled to try and get table employees. That would be a disaster, wouldn't it? Probably that that's kind of a exaggeration. You get the point. You might just like select turns into truncate she's all right. So web communication yet all web stuff is done through the HDTV protocol. All right. SDB is built on DHCP at the is to be one and CDB two. TV three is built in quick, which is built on UDP, which is a whole revamping of the TCP. Quick is exactly like TCP, but it it, it is rebuilt with more features and with awareness when it comes to the content that is being sent, you know, there's streams of calls of streams, channels, if you want to call them. I like the word channel better than stream, but is not our topic. Anybody that excellent communication a can talk to BMB can talk to a you know. A Kentucky Derby and beat Kentucky. There is no order. Right. SDP is actually a request response. So you send a request synchronously and you wait for a response and we will send back the response or request response. TCP is not a request response system. It's a it's a bi directional communication. Just dislike web sockets if you're familiar with that. All right. TCP connection. The concept of a connection, guys. The concept of a connection here is now. We have a state. We have knowledge between the client of the server. We establish knowledge. We handshake. And then once I know you, I'm going to save a state. I'm going to save a session and layer a five that identifies you. And if you better exist in this list, if you send me something in TCP and you claim to be a TCP segment and you don't have a connection with me, I'm going to drop you. So there is no spoofing. You can't spoof Merman. Connection is an agreement between a client and server. It's an agreement. And we were going to talk about how this agreement is done in a minute. Must create a connection to send data. That is one of the most powerful concept in DCP and also latency inducing, if you will, because now you want you to do this whole handshake just to send data. Oh, my gosh. Why? Right. The reason is because of security. Reason. Because of. Really? That's pretty much it. I think security is one of the major reasons of the connection. They can just send data. You know, we have an establishment. We have authentication. I know you write. Well, it's a mini authenticate. I don't call it full authentication because literally anyone can connect to anyone. I don't know if you're an attacker or a good, good guy. You know, someone is connected to me. I don't know who you are, but at least I know that you connected to me. I established. Then you can just send me data without connection. I can choose to block you if I want to. And connection is identified by four properties. We talked about this source IP, source port, destination IP, destination port. All these four pairs which are both from layer after layer three are taken the operating system, hash it and preserve one hash. And that hash is locked up in a lookup in the operating system and that matches something called a file descriptor in the operating system. And I find the sculpture contains the session effectively the state and this is usually in memory but also can be preserved disk. I'm not quite sure about that, but it takes finite amount of memory. So that's how you know, if there's a connection or not, you hash it and you look up. If it's there, if it's there, you have a connection. If it's not there, fail or drop the segment or return ICMP message saying, Hey, who are you? Can't send data outside the nation. Talk about that sometimes called socket or find the sculptor. We talked about that the concept of a socket or if I discover is a layer five. I think we talked about that because it's a session there. Now we're talking about not just stateless segments being sent. Know I have knowledge. All right. I have knowledge about this. Think I have a session that is established. I'm stalling the session. That's why layer five is effectively state stateful protocol. Right now. We have this session and then it requires a three way it handshake, which you're going to we're going to do in a minute. Right. And segments that you send every segment. So you start with a sequence. He's won and then he's ended. And then you add the length of the package and then you send two, three, four, five. So these segments are arriving in a certain order. If they arrive out of order, which will believe me, they will, because each segment has sequence one and segment two has sequence two. Segment three has sequence three. Right. They are shoved into our into an IP. Beckett IP package has no guaranteed delivery. It doesn't have a fixed path because each path will might go to this router and thereafter decide, oh, this path is a little bit busy and congested limit. Will it send it this route? So IP packets are disrupted. They don't know what's the content of this IBM. They don't care. Right. So the IP packets, while they will take certain path of the destination and server, all the same, they will take almost the same path. And don't don't think of a path as if a hard coded that all is the same. They can change. But for most scenarios, it's almost it's a it's almost the same path. Right. A HTC big action will take a different path based on the source end of the station. That's why there is another protocol called the Multi Path TCP, which introduces a beautiful idea of using your wi fi interface and your Ethernet. You know, if you're a mobile phone, you can use your LTE and your Wi-Fi to establish two CCP connections. And then you effectively, I guess, multiplex you d multiplex, you take your request and you're sending them through different connections and then you order them on the back end. That's probably something I shouldn't have talked about, but I don't want to confuse you. But this is another thing. Multiple DCB. So keep it on your back in your head. But back to the CCP here. So TB takes a path now. But this backend change because the IP packet at the end of the day there is no path right to it. And the segments. Yeah. We talked about I was I was talking about the path because IP packets will arrive out of order because the path might be different. You know, one writer can send and then IP packets will be arrived out of order. But if you unpack, you'll see or segment two, segment three and segment one org and order them or one, two, three or a segment three, segment two. But we're doing a segment one. Oh, I'm sorry. I can't, I guess, acknowledge this stuff because I didn't know if segment one is missing retransmit police. And that's what happens here. Lost segments are retransmitted. Powerful stuff here and the power of retransmission. Right. Is what makes TCP reliable here. Multiplexing most of us. I believe we talked about this. This is a identical slide really from the udp slide as ip targets. Host only we talk about the host runs many apps and to identify an app you need a port that consumable port and receiver so the sender multiplexes all the app and to the TCP connections and then all of those are then the multiplex back to its apps. Right. And this is done by the pair of four that we talked about. Now the operating system will have to define okay, do we have a connection? Will not deliver immediately. It will only deliver if there is a connection. If there is not a connection, then we drop it and sometimes the app will close and shut down without cleanly close the connection. And that can cause leakage and stuff that is really unpredictable, nasty stuff, may I say? And yet again. I was covering that. So it's just AP zap. Why ABC? And I'm going to go I'm going to disappear for a moment because the next slides are really crowded. All right. Let's talk about connection establishment, you guys. App one on ten 001 Want to send data to app x on ten 002? This is how that connection established or the three way handshake happens. App one The first thing is since called something called sin. Right? So there's a bit in the segment and the header itself says, okay, this is a sin, which means synchronize. Why? Because we need to establish a connection so we can agree on the sequences. I need to know your sequences. And you need to know my sequences because. You can send data by that action. All right. That's part of it. Right. And you can think about this and say, okay, oh, bidirectional. Right. What if I want to build a unique directional system? If you know you have the only directional system, then you know that it's only one way. The other way won't send you data, so you can lose that if you want. If you want to build a back end application that doesn't have bi directional, you know, that's just a force for thought there. So yeah, it's in the send request on Port 22 of the. So this is this is it. Right. So as such is running on topics here and ten 002. And then once we do that, we're going to send that. And the source is Phi Phi Phi Phi, which is that's the port that the app. Right. That since it ten 001 is the source IP and then all of a sudden we reach here. So that's hey, this is this is my sequence. I start from sequence thousand, for example, and then ten 002 will reply back with something called Sinovac. So will send its own sequences. So I start from sequence 5000. And all of a sudden you reach here is all right. Got it. And then once. The final one receives, we ask, and that is complete the handshake. And all of a sudden we end up with a file descriptor right here. So we end up with a file descriptor right here that says ten 0015555. The hash is ten 0022222, two, two, two, two. So therefore, pair that we talked about uniquely identifies this file, this sort of app to want to send data to the same port on the same machine. It will error because as who are you? You're 77128. I don't have a file descriptor for you. You need to establish a connection for that app. All right. And all of a sudden now we have both side actually you have a file descriptor here. And if I look at the here and now, you have a connection and that's how you synchronize the numbers. Right. And that's part of the state. You have to see if the numbers the sequence numbers here. So this is a finite amount of states that you need to store in the server, not just the file, the scatter, the sequences, the window sizes that we're going to talk about. So all take memory, right? And they have to be there and this memory will increase with the number of connections. So if you have a lot of connections, at the end of the day, your server will have a limit. I believe WhatsApp, that's the highest I've seen. WhatsApp can handle up to 3 million VCP connections per server in Mosaic. How can you go that hard? I thought. I thought the port is 65,000. Well, of course, because that is all of them are going to the same port and all of them to the same IP. But they are but they are coming from different IP addresses in different parts. Of course, you can go unlimited with that one, right? We'll just just multiply the IP addresses with 65,000 and then how many IP addresses are in the world? Right. 4 billion. That there are no limits when it comes to the source and the. Yeah, you're making both this variable. So I know that what's reached 3 million and that's pretty much it. I don't know other public information about how much connections can, can a server handle because there is a limit at the end of the day after which the hashing that is happening becomes well, the hash is really fast because you take these numbers and you immediately look up the table, right? Because it's a big old one, you immediately find the connection. That's not slow. All right. But then the problems like what if if if a connection is destroyed. Right. You need to remove that cache. And the table, the hash table changes sides. And we know what happened with the hash changes size the hash get screwed set result and it can cause trouble in certain situations. But in memory is the limit and CPU is another limit, right? If you have a lot flooded of connection, the hashing is what the take of the CPU. Right? The CPU cycles to hash these four pairs, the four base, the four entries. That is what basically takes a lot of CPU. If you're doing it all the time, then it will take CPU and it will take memory because they have finite amount of memory. Whatever, two 5G each multiplied by 3 million, you get a ratio limit eventually. Sending data so now app once and data to app X now well yeah will sending again less right to A to do a search or remote in doing that we did the bash or we're doing an LZ right now that I have the data I have the connection I'm just I'm an actual data app one encapsulate the data into segment and send it so that's the destination for the source port. And now we get it over here. That segment is received, right? The operating system says, okay, do you have a connection with me, sir? Bom bom bom bom. Hash bom immediately found it. Oh, there's your five disruptor. Yes, you're good. Let me take the data. What do you. What do you want to go? Where do you want to go? Oh 22 oh the two is actually app x. Here's the app. Excuse your data. It just sends this and probably sends all the whole thing, right? Send it to the app x. An event is triggered and as a search takes over and does the result and effectively responds with the but before it responds, it actually acknowledges the data. It says, okay, I acknowledged whatever the sequence number was there and boom, I acknowledge this data is this is do reply guys. We acknowledge the recipient of the LSA command in this case the segment and that's pretty much it. But here's a hint here I want you to think about it. Can app one send news segment before EC before the EC of the old segment arise? That's a good question because now I think the command can I send another command before I get back that the actual server receives it? Absolutely, you can. But there is a limit to how much you can send. You can send as many segments that you want. But there is a limit. All that kind of contradicts my statements. There is a limit, right? If you send a segment and you have to wait for the attack. You're paying the roundtrip time here. It's so slow. Imagine sending and waiting. Sending and waiting. It's a new way to know. You can send it up. It's up to the server. Does it handle it? And it's up to another one, which is the routers. Can the routers, the little boxes handle it? So this is called the flow control. This is called the congestion, the draw. And only then you can send as many segments as you want. And we're going to talk about all of that. I know I'm jumping the gun a little bit. There's another important concept. So sequence one, sequence to sequence three. So we're sending three segments here in parallel, if you will. Right. One, two, three. Right. All of these are IP packets at the end of the day. Right. They will arrive. Out of order. Maybe some of them might fail. But eventually, I say, all of them are right. All right. So this guy gets sequence one, sequence two and six, one, three. You have a connection, sir. You're good. The host here can acknowledge, and the acknowledgement here works by actually specifying what sequence audio acknowledging. So if you are acknowledging three, that means you receive two and one and three, right? If you acknowledge two, that means you receive anything two and one. If you are congenial, it means you are received only one, right? So that's how I acknowledge you can acknowledge multiple segments with one act. Very powerful concept here. And so acknowledgement is a very important concept. And you get you noticed that I'm not talking in details here and they're showing you actual numbers because this is not the purpose of the course, the purpose of the course to then just say the high level of how things work, you know, and to take us as close as possible to the application here, not to dive into the weeds of networking. And I try to, as much as possible, separate that. I hope I'm doing a good job. I don't know if I am. Here's an example where a lost data can happen. So up once in segment one and two and three, one, two or three, but sigma three is lost or maybe timed out, maybe stuck in one of the rafters that needed to be get fragmented. And it's so big. There are terrorists sent back and ICMP message said, hey is still big, right? Any reason can go away, right? So this guy received only sequence one and two. So what? It happens. It does. Okay. I receive one and two at the same time. I'm going to acknowledge, too. That's it. So that means two on one was arrived immediately. The sender will know. Wait a minute. What happened to three? I sent you a three as well. Was it alright if you didn't, if you didn't receive three, if you only acknowledge one and two of them is you didn't receive three, I'm going to rescind it and that's part of the retransmission logic. So you just sent the data that you already sent. I so that's part of the retransmission to guarantee I'm always going to send it unless I make sure that you get it. So segment three are sent again and then once we are, we get it. We acknowledge three and now boom. Keep in mind that we always have the file descriptor on both sides. Or I thought about closing the connection. This is another concept. If you open a connection, you have to close it. Right. And this is done by a four way handshake, effectively. And then one can initiate the closing the connection. One can receive that closing session. Right. So you cannot close something that doesn't exist. Same concept. So if you send me a friend, I'm going to check if there is a session for you first before closing something randomly. And you cannot just forge and close connections that are not yours. Right? So here's an attack that can happen here also. So that that can happen is working. If I know the source IP address and the source destination and the destination IP address and this destination port, I can send a fake. Uh, then request to do to shut down an existing connection for a, for a given IP address. Right, if you think about it. Because what will happen is I'm going to forge my IP errors. Right. Spoofed, if you will. Again, it's not easy, but if you can't do that, then the server will check. Oh, oh, this source IP destination. I'll be here that matches. Let me close the connection so you can effectively shut down some other computer connections. Right, while you're spoofing. Right. That's an that's a nasty thing to do. So I punched in the fin that's how it outdone he's an event to the port destination local all the jazz and then the server will acknowledge that fin just defend it will acknowledge the fin the server will acknowledge that fin and immediately after that the server will send its own fin right telling the set of okay, I'm ready to close telling the client, sorry, I'm ready to close, go ahead and close it. And then the client when a moment the client receives that since that ach that is when the connection is completely closed. So now when the services that final act it just it can destroy the file descriptor unfortunately the file descriptor for the person who since the fin remains because. It is remain in a something called time weight state. If you ever didn't, it's that you going to see all this time waits. Whoever initiated the connection end up in that time weight state to wait for any old segment that have been. Lost and retransmitted. To arrive and in order to clean it out. Because you don't want to. Ah. We don't want to close and then open and another file descriptor. And all of a sudden we have, we receive an old segment that belongs to an old session that we killed. It will be a will. It will cause corruption and, you know, certain undesirable things. But that's basically how you close the connection. ia4 way handshake, Finn. Ach, then. Ach. And this remains the the policy to what? I mean I believe in it like 4 minutes or whatever. Uh, it's basically double the maximum segment size. We got to talk about that in a minute as well, you know, in the future. Summer. Read a so-called summary here to summarize the TCP IP so that CCP protocols stand for transmission control protocol uses the count of ports just like UDP, but gives you flow control, gives you order packets, give you guaranteed delivery. Right. You can you can argue congestion control. We're going to talk about them in different lectures, but gives you all these nice features. It's a stateful. It has a concept of a connection. So there's a file descriptor on both server and the client that maintains the connection. That is so it it it lives in memory. It lives in the Ramos or takes a finite amount of rain and ran. So there is a little bit of cost for you. So there is a limit to how much connections you can open at the end of the day. We talked about how WhatsApp has that limit of 3 million connection, probably that there are multiple servers that now have exceeded that. I don't know personally how much is their limit today for a single server, but yeah, this is a very, very powerful protocol. TCP transmission control protocol. How about we jump into it and next dive into the control. What exactly is the level of control that we can get with? DCB? Let's jump into it.


### TCP Segment vtt

Just like we did with the IP packet. We dive into the anatomy of the IP packet just like we did with the UDP, live into the UDP anatomy diagram itself. We're going to do the same thing with the TCP segment. What is the anatomy of the segment? How does it look like? What are the different headers? Let's jump into it. So the DCP segment header is 20 bytes. We talked about that a little bit and it can go up to 60 bytes based on additional options that you might add. DCP segment slides into an IP packet as data just takes that segment and it becomes normal IP packet data. The ports are 16 bits just like that stuff, but we have all sorts of stuff. We have sequences we talked about that, we have acknowledgement, we have floor control, we have more and more and more stuff. So that 20 bytes is basically fully taken and here's how it looks like. So there is a there's not much here, right? I'm going to cover my face, but it's exact I'm not covering anything here. So this is the DCP segment here because this is how it looks like these are the orifices if you want to look deep into how it looks like in details. But let's demystified again. Four bytes, right? Four octets. How many rows do we have in for bytes? One for bytes to for bytes. Three, four and. Five and then the same thing as the IP. You can go with options and increase that if you want. That gives you like one, two, three, four, five, five times four. That gives you 20 bytes by default. All right. Just start with the portions of the source port, destination port. I thought we talked about this. This is going to clear. You have 16 bit. That's pretty much enough. That's more than enough, actually. 16 bit ports. On the source side, the 64 bit port on the destinations are enough for us to address most of the applications that we want to address. Yeah. And sequence numbers. We talked about that. Remember when I said, okay, go to that, send. The less sequence is synchronize our sequence numbers. Every time you send a SIG segment, you include the sequence number for that segment. And this keeps increasing, increasing, increasing. And this is what to at the board 23 that means it's a 4 billion something. It's a lot, but it can't run out easily. You're sending a lot of transmission. That's why the sequence will go back, will be will go back to zero. It will do a roundabout. But the sequence, at the end of the day, they will run out the 4 billion. Just like that. Right. Well, if you sending a lot of transmission, the same connection. So they all go back to zero. And they will be identified as the newest sequences. And there are all sorts of, you know, remedies to solve these kind of problems in the sequence roundabout, as they say. Right. We had the same problems, Postgres and their transaction, the transactions also 32 bit. So it means you have you can have up to 32 to the bar. 32 also means only 4 billion transactions. So you can easily. Deplete your transactions in and phosphorous. And we talked about that in my database cause I can always remember exactly the same thing. So if the ACC flag is set, what is the act like? There's a ACH flag. That means what will happen is you're acknowledging that sequence number effectively. So that's the acknowledgment number of that. And why do you might say why? Why am I why do I have sequence number to acknowledge the number? Can I just have a bit? And then the sequence number would be like an order number? No, because you can send data and you're acknowledging you can send data while you're acknowledging something else. Right. So you need its own number. Flow control, windows, eyes. How much does the do I effectively as a server can handle? If I if I had to play back to you with a segment and I specified this window size this is to bar 16. That means what 65 K that's the maximum default, but it can go higher than that. We're going to explain how like 65 kilobytes, that's the maximum I can handle. That's too low. If you think about it, it was like, what? You can only have 65 K, that's nothing. So there isn't a little flag that can make the window size up to one GB. So talk about flow control, right? But that's basically you as a responder telling the client that, hey, this is how much I can handle. That's my window size. Don't go above that, please. Nine bit flags. These this flags are used for all sorts of thing. So there is a sin flag. We talked about this. This is the initial handshake. That is a fin request. I say this is how close the connection. Please reset. I don't know what happened. All bets are off. You're sending me something bad? Reset the connection Push. You're pushing data night with some acknowledgement you're acknowledging something argent of this segments. Argent I never seen this use actually. I don't know if there is anything on it so easy. And this is, this is part of the congestion control that we talked about, the flag that says, okay, this is we we set a flag. We talked about routers not dropping packets in case of congestion by sitting something called the in the congestion notification. That's that kind of fit a bit that says the receiver received that notification. And now we're telling the sender that, hey, by the way, the road there was about to be a congestion just through careful, right. Or the moment you do that you get to reduce your congestion window. The CW or that means congestion window reduce that's when that's the sender telling the client that's that's interesting the sort of I by the way I actually listen to you and I reduce the congestion window. We're going to talk about all that stuff. Right. But this is part of that congestion window. And this is kind of another bid that goes into this whole thing, which is the knowns, the notification knowns, which is part of the congestion control. So all these flags, man, as part of the congestion controls. One of the interesting thing to me that I'm really enjoying learning about. The maximum segments eyes. I think this is one of the most critical thing to understand, you know, because when you send a bigger request or you're uploading a file. Right. It's critical to understand what is the maximum segment size. Again, we're in layer four land here. So how much data I can send so that it can pack nicely in one segment? It all comes back to the packet size and it all comes back to what, the maximum transmission unit of your network, which is fixed in your network. And so the segments are depends on the empty MTA of the network. Usually it's 512, but can go up to 146, 14, 60. And this 1460 is how it is calculated is because the default empty of the internet is really 1500. Even in my Mac, if you go to the Wi-Fi network, it says 1500. That's the that's the maximum transmission unit. That's the maximum frame data that you can send. That's it. 1500 bytes, 1.5 kilobytes in a single frame. Right. So. That data includes the IP back and include the maximum the segment to sell. Right. That means include all the garbage headers that the IP has, ads and the segments that are for TCP ads. Right. So 20 byte, remember, 20 bytes in the IP and 20 bytes in what, in TCP? So we have 100 -40. It gives you 140 611460. But are the idea of the jumbo frames as well and to you goes up to 9000, right. About 9000. I've never seen anything above that. Right. But if you have, you can dust cloud, Amazon Cloud and all these cloud providers. If they can go beyond that, it's probably a good idea. I don't know if there are limitations for those jumbo frames, but yeah, and this is that can go beyond that if your frames can support it. Now, let's go to the next lecture. Talk more about flow control.


### Flow Control vtt

All right. The idea of floor control. You know, this is one of the concerns that confused me the longest. You know, I always confused the flow control with congestion because they sound the same, but they are not, believe me. Flow control was invented first. Right. And try to answer this questions. How much data the receiver can handle. I'm standing there. I'm a sender. Like I can send all data on one. I have a huge bandwidth in my internet how much that a server can actually handle it. Because that's not the case, right? Back in the old days, you have all the bandwidth, but you don't know of the server can actually handle that or the receiver doesn't have to be a server, you know, and that Flow Control tries to answer that. Now I'm going to disappear here because all the slides are really crowded. Well, so let's go ahead and talk about flow control, how much the servers can handle. So they want to sentence segments to be less this simplify and say, hey, I want to send ten segments. You can do it this way. Right? Essence Segment one to be send one segment. And then wait. You have all the segments ready and the client side. But. You're sending in one by one. You send a segment and you wait. You are faithful to the acknowledgement. You receive acknowledgement. Then you send the second segment. You wait for the acknowledgement. So the third segment, then you see for acknowledgement that you are paying for each segment. What however latency between you and Amby that means all that outers that are in the middle. That means all the routers, the work that they do, all the switches, all that time to live shenanigans that the routers do or you know, they they decrement the time will live, they rewrite the packet they run, they do an acting, all the work, all the latency that is added between and B or your your enforcing it every segment, you know, it is very slow. You never do this. You never send one segment and then wait. And I made it green because now we're in in layer four land only. I don't care about layer three that no right. Or, or playing with this land. So this is without any floor control that anything like that. So the idea here, this is what we think thought about is like okay listen to segment it's way bad idea nobody told us that we need to be able to send multiple segments and then get one ACH because now I can I can take the latency once with seven segments, for example, and then wait. The question is how much can I send? Now we agree I can send multiple segment, but the question is how much? That's beautiful, right? I can send one, two or three. In parallel almost. And then be will immediately acknowledge all of them with one act, right. Act three, all of them acknowledged powerful. But the trick here is you assumed that B can handle three segments. So you sent them and you were lucky. The question is how much A can send and the other question is how much beacon handle? Right. That's the answer. Right to the answer. That's the question that I tried to answer. And this is called floor control. The flow controls the window that this guy need to maintain. Right. Such that be is not overloaded with packets. Second example, when do you be segments arrived? They are put in something called the receivers buffer. You know, they don't just put in the air, you know, be or receive them. They put it in the buffer. No. And this buffer has a limit like that outers we talked about. Right. There are a limit to these buffers. And then. If you send so much data, so much segments, if the buffer is full, those packets will be dropped, no question. Right. So the solution here is we need to let the sender know how much you can handle as be. Right. You can. You have to send to this window. Right. In this case, that buffer need to be notified to a. So. Hey, dude, I can handle only this much segment. What? What? There's not a number of segments per se, because each segment has the maximum segment size, but it's actually a number of bytes. Okay. Hey, I can handle 8000 bytes. Whatever I. And back to those them. Back to us. Right. We talked about this. This is called the receiver window. It's called the flow controls, all window size and also called the receiver window. Right. And this is basically it. Right. So the receiver sends a segment replying or an acknowledgement, for example, or something, and it can include its window size in the acknowledgement or in the date it has since they zone, which I know both actually both guys do not just the receiver because everyone is this in done or receiver and TCB because it's a bidirectional protocol, right. So whenever you see RW and that's the receiver window. Right. And that's just the short. Way of saying it. So it's 16 bit. So it can go up to 65, 55, whatever, right? 64 kilobyte and it is updated with each acknowledgement. It will tell you that, hey, my receiver windows this, my receiver windows this, my receiver and does actually this. And guess what? Why is with the window size change? Because technically, if you think about it, it can calculate based on the acknowledgement how much the current window is. But guess what? You're not the only one talking to me, son. I have thousands of other clients and thousands of other connections. So I'll tell you if I'm full, if I decided halfway through that, hey, other applications are taking a lot of memory so I can already use this or I am. Hey, I'm going. I have a lot of little more buffer now. Send more data. So this or receiver windows are always changing and the client is always notified of this new change. So let's take an example. I send segment one, get it there, and then window size is okay. Hey, I can handle up to three acknowledged and it can send three segments. Yeah, you can send more. Don't be shy. Sending just one. Send more. Said two more. So Athens three. Boom, boom, boom. And it's felt and wait. I acknowledge four. Right. And it tells you, hey, I still I still have room for three. Don't send more. All right. And then five, six, seven digit it received acknowledged. Talk about sliding window. So sliding window here is really the same concept. Right. But it's cinder. Property. But it's for the same thing for the receiver window. The major problem with this is like the client or this, and I can't really keep waiting for all the acknowledgement for all the segments. Right. So if I send 1 to 3 and I got a laundry for two, right. That should be enough for me to know how much I can send. More. Right. I can just like. Oh, right. Oh, I, I can't. Can I send more data or not? Or am I stuck with the am I stuck with three now or two? Well, how much can I send? All right. You can effectively just slide the window. Right. Let's take an example here. So if I am initially my window sizes three. Right, which is basically identical to the receiver, you say one, two, three, you send the 1 to 3. But B for some reason or another does not because it three was lost necessarily. Right. Could be is the different process. But acknowledge only two. So acknowledge two and one are acknowledged. So you can. Keep the window size fixed but just slide it so that keep three. Because we might need to send three again. Right. And four and five. So now I can basically discard the one and two because I know they are acknowledged and I'm going to just slide my window one more and and I can choose to send three, four or five again if I want to if I want to decide the transmission or I can decide to wait a little bit more. So here I'm going to send four and five because this way for a little bit more for three. So I send four and five and now I get the acknowledgement of three. So the more I can only three, I can I can still shift my I slide my window a little bit to the right. Right. And now I acknowledge I can basically drop these three my buffer a little bit. Four, five, six now and I now here I can send the six right or I can send four or five. Six. Right. So we were sliding. That's called a sliding window, if you ever heard about this. So we're sliding. The more the more acknowledgement we get, the more we slight to the right. Right. While the window sizes remain the same effect. Okay, let's talk about window scaling. So we talked about the receiver window being 64 kilowatt is ridiculous, right? It's just too low these days. Imagine like I want to send a one gigabyte, a file or an apple or something. 64 kilobyte is not is not going to finish at all. So and guess what? We're stuck with the segment being 16 bit. We can't really increase the segments. I kind of play with that header. It's done. It's written in stone, right. What was all those guys thinking? All right. 16 but only store to a small but no, they actually solve this problem elegantly. They introduce a little with scaling factor, which is just basically a power factor. You there is another option in the TCP segments that you can enable and this is called the window scaling and this is negotiated during the exchange and we'll say, hey, my scaling is 14. So what you do is basically multiply to the board 16 which is the window size times, whatever the value currently and the window size multiplied by to table 14, that is huge as more than enough. So now you can go up to one gbps and window size. That is more than enough I think. Right. And you can play with this. The the scaling factor is negotiated only once. Remember during the handshake and that's it, right? That's why I. Wireshark when you do a wireshark all right if you miss the handshake, you the Wireshark cannot guess, unfortunately, the actual accurate window size because there is an accurate actual window size. And there is the estimated one, which is a window size in the in the bit, the 16 bit, that's not enough. That's definitely not enough. So window scaling is always almost used, right? So that option is always enabled. Right. Summary So we don't know why the receiver has a host has a limit like anything else, there is all limit because we have resources, we have RAM and CPU and we have the end of the day we can do everything infinitely. So we need to let the sender know how much I can handle. Right, and how much you should send. The receiver window is in the segment. Right? This can be controlled by the concept of the scaling factor, right? And the sliding window can be increased and slid in the moment. We have more acknowledgement so that we don't we're not stuck effectively in the same values right now. Sliding window will keep increasing and. And yeah, the scaling effectively controls all of that. Guys, flow control, very critical causes. So if you don't like TCP. You better include some sort of a simpler control at your end, in the back end, to build your own flow control if you want it. Otherwise you might. You might say, Hey, I know my servers is strong and are strong. I know my servers. Beefy. Right. Let's go with phase one. Yeah, I know my server as beefy and I know it's I will handle it. I don't need to implement any of that stuff. I know that my servers all is going to be beefy. You can skip all that and use UDP and don't it prevents or control a little bit of a slippery slope if you think about right. But yeah, it's like machines. These they are always can handle stuff. Right. But. The end of the day, if you have a lot of different clients connected to you, you're going to reach your limit really quick, right? But yeah, if you if you can do without flood control, maybe you you'll just you can ditch DCT, CBN, use UDP natively and then build anything you need. Flow control, maybe it's like, hey, I'm going to use for controlling the stateful manner and we'll get a negotiated once in in the UDP as data. So this is how much I can handle. Leave me alone. No need for this. Window sizes and all this sliding window mumbo jumbo. I don't need any of that stuff. But the next one is actually more critical, which is the congestion control. You want to stay tuned for that one. See you there.


### Congestion Control vtt

All right, guys, we talked about the flow control, how much the receiver can handle and how much the sender can send. But now we have to talk about the congestion control, how much the network can handle, how much the middle box is in the middle that my IP, my beautiful IP packets pass through. How much can it handle and how fast can I send the data? Because we we we agreed we cannot send a segment and wait. That's just dumb. No, because I said in sentiment, I take the hit of latency. So if I can send a lot of stuff and take the hit once, I'll be better off because I rather send you as much as there as you can handle and as the network can handle, such that we can increase the flow of the data. And you can get all this data once and I take the head of the roundtrip once instead of multiple times. That's the congestion control, how much the network can handle. Let's jump into it. I'm going to disappear. Congestion control. The receiver might handle the load, but the middle boxes might not. We're going to take an example that others in the middle have limit, just like any other machine. Think of all this as the flow control of the routers, if you want, and that will pan nicely. If you if you think about it, we don't want to congest the network with data. We need to avoid congestion. It's a new window. It's called the congestion window. And this is a property of the sender because the sender is responsible for congestion. And this is an example here. So we're be we have the flow control window and the router is the here's how much buffer they can handle. So if you skip sending a lot of data, the routers might drop it. And when they drop it, you might need to retransmission. You might do lot a lot of retransmission. And that will decrease the performance of your network. There are two congestion control algorithm. One that starts the beginning is called TCP slow start, if you heard about it. And then once that takes over, it's called congestion avoidance. That that that kicks in after slow start is finished. Right. Second example. So what is slow start slow start is actually the funny thing is like slow start actually is faster than congestion avoidance. So it's really odd but it starts slow and here's where our works, your CW and DE, which is the congestion window. You. It increases with one maximum segment size after each acknowledgment. With every acknowledgment that we get, we increase one RMS one, RMS one, RMS one of us. You're effectively so you're keeping increasing one one maximum segment size. So maximum physical size can be what, 1460 bytes. So all in bytes at the end of the day, right? So you can increase an increase and increase or congestion can go up until what you cannot obviously exceed the receiver on the right receiver window. We established receiver will handle a lot up to one gbps some receivers will handle, but will your window congestion window can handle up to one gbps. I really doubt it. Right. Your networks in the middle will die, right? So you send one RMS after each ACH for every single acknowledgement, right? If you send a segment right and you receive an acknowledgement, that's an additional one. RMS On your conversion window, if you send if you if you got another acknowledgement, add another acknowledgement, add, but congestion avoidance, once the slow start finishes, this is what kicks in. And here's what you add. You add a slower you add the number of the the the congestion window is actually becomes linear here becomes exponential and here becomes linear and you only add one RMS after a complete roundtrip time. So if you send like five segments, you know that you send five segments, right? You got to wait for the five to be acknowledged. So let's say you acknowledge number two and then number three and finally acknowledge five. That's one round trip, right? So the whole one round trip effectively will increase by one, which is way slower, right? So effectively what's happening here is you're increasing exponentially and then increasing linearly. We're going to show that in a minute. So slow start and the congestion avoidance. Right. And the goal here is like how how large can this be? Right. Let's take an example. Slow start, conduction windows start with one mass, or sometimes more like Cloudflare, for instance. The cloud provider sets them as to be really large, right? The current conduction window just so that you start faster because they know you can they you can handle they can handle your traffic. So send you send one one segment and wait for an ACH. What's OC? This is my current window congestion window. Let's send one segment and they just wait the month you get an economical for that segment, you add one, right? You add another segment which is you added a maximum segment size. But for simplicity here I'm showing it as segments. So now your CW and your conjunction Windows two and you send those two puppies two and three, and now let's say I receive two and I receive three. Usually you get to receive one technology, but I'm exaggerating here now. So those you get acknowledgement to enclosure three, we first add one for first acknowledgement and then add another one, right for the second approach. So we added two here. So now look at that, we exponentially increase that. So with every segment, with every acknowledgement, we're getting almost doubled the number of segments, right? It depends. If if the if this if the receiver decide to wait and send me one acknowledgement, I'm not going to get two. But you get the idea, right? This is just a simplicity sake. So now we have this is my kind of my congestion window is four. So I'm just sending four. So you can see I'm increasing very fast, although I don't know why it's called slow start. Right. The key here with each acknowledgement, that's the algorithm. And it took me a while to read and understand that right through the ref. See, it's with every acknowledgement. Right received you increment by one MSAs or one one maximum segment. Sorry so you can send to more write each acknowledgement. We're going to see the congestion avoiding. We increment only with one roundtrip. So in the case this is one roundtrip. This is one roundtrip and this is one roundtrip, right? The one router was like how much data you sent and then that that can that, that, that determines a roundtrip. And roundtrip is you send seven packets, seven segments. At the same time you have to wait for the whole seven segment to be acknowledged. Even if you receive one acknowledgement for the seventh one, that's one roundtrip, as long as all of them are acknowledged. So until we reach slow start threshold. So there is another threshold that I mention here, which is the threshold of the congestion control that says, okay, this is my threshold that that you reach, that the congestion window reach that threshold. You switch that algorithm. What's the next one? That's the next algorithm. What do you do is that you send the congestion control worth of segments and wait for an ACH. So again, let's start from one for simplicity here. Maybe it's going to be more than that, but for example, six here. We sent one. You get an acknowledgement ad one. Now it's two you send to. You get you get an acknowledgement. You get to acknowledgement. But guess what? I don't care. You. This was my round trip I sent to. I got even if I got to acknowledgement, you get one that's that because that's one roundtrip. So for each one roundtrip, the whole thing, you get one MSAs and that's it. So even if you send note, if the four or five six, I send it and I get acknowledgement six. All right, I'm going to get one only when all the segments are act add up to one MSA. So it's very it's much, much slower. It's actually linear. If you think about it versus a slow start, the more accurate. If you get a lot of X with every single any act, you get that it will just increase, increase, increasing it. So this is it doesn't go by the number of acts. It goes by the roundtrip itself, right? Oh, it's certainly two, three. Hey, I consider this roundtrip done when you send me the ACH for two and three. And you did send me your accounts. Yeah, sure. But this is one roundtrip. I know you sent me to the acknowledgment, but it's the same thing even here. Four or five. Six, if you send me ach four ach 5xx, this is one roundtrip so I'm going to increase one in slow start if you send ach for ach 5x6, this is plus one plus one plus one. So you add three to the congestion control. Wow. You know, so this is the algorithm actually it's not exactly. Plus one is actually this is the algorithm, you know, it's a conduction conduction plus the MSAs over the congestion control window times. RMS So this is always going to be a fraction for sure, right? And this times this so it barely hits one. It's less than one. Mostly it's less than one. Right? It was only going to be one of the if the S is equal to the congestion window. But the congestion window is definitely going to be greater than RMS. So this is going to be a fraction and a fraction is down anything. It's going to be lower than the RMS, right? So it's slower than once. No one identical is definitely less than one. So it's almost like a fraction that it's actually if you think about it, it goes slower. Here's something we talked about. Congestion is a fixation. Remember that, guys. So we don't want routers to drop packets when they hit congestion. Can they let us know that, hey, I'm about to drop this, but I'm not going to drop it because I'm about to hit congestion. But I'm going to let you know, this is the explicit congestion notification that we talked about. So in the IP because that's the routers can see it's only these layer three. Right did that rhyme that's the router can see it only can see layer three. Oh my gosh that rhymed now I'm awesome man. So meet ECN explicit congestion notification. So in the layer three IP packet, the router will set the bit. This is easy and this is an explicit congestion certification. You go and let your sender know and I'm going to let you know right now. So now routers and mailboxes can tag IP packets with ECN and the receiver will copy this bit back to the sender. And a saying is an IP header that we talked about that. Right. So routers don't drop the packet, just let me know you're reaching your limit so that once you set that bit, all the algorithm can kick in as is oh, someone is about to get congestion. Let me half the congestion window. Right. And that's what will happen with the moment you have it. There is another bit that we talked about. The CW reduce the CW are congestion, window reduction, so we just reduce this window. So you let the receiver know. So yeah, summary, summary, summary, summary. While the receiver may handle large data, middle boxes might not. And so that's what congestion is all about. Middle routers buffer my Philip because at the end of the day to process data, you need a buffer, you need a specific memory, right? And you can assign it infinity. I mean you whatever you can handle of the router that the grade eight routers will be just doing that routing is not going to do anything else. So they probably assign 16 gigabytes or maybe more. Why not? Let's add more, more way more ram. But now they want this much ram. You need CPU, right? That equivalent to process all this stuff in your memory. So you have to beef up the routers as much as possible. So how much can you handle? That's another great stuff that I frankly I don't know anything about, but routers and the quality of a router, that's another discussion, to be honest. Right. So you need to control the congestion of the network. So we agree about that. Right? Then this will allow us to send a lot of data and obviously it's controlled and restricted by the flow. How much your the receiver can handle. But it's still good, right? The sender can send segments up to CW and D, which is the congestion window or the receiver window without acknowledgement. And the whole thing is the whole power is this how much can I send without acknowledgement? Right. Because if you send something on, you need to wait for acknowledgement. You you talk ahead on the latency, the distance between you and the destination. Right. So you want to send data as much as possible, but without waiting for acknowledgement. That's the power here. Right. Because if you can't wait to send send annoyance and in a way that kind of defeats the purpose because it's going to be slow. This is normally not normally not a problem in host the crew through land because if you have land and you probably have ten gigabit or 100 gigabit and you don't have that limit. Right, unless you have a very chatty APS, maybe. Right. Then you need to take a look at the congestion control again. If you have Chadi APS, I would put them in their own subnet and put a switches so that they don't cross routers. Right. And only go to the router. Absolutely. If you need to talk between subnets. Right. Otherwise, take the hit and dot directly. All right, guys, let's jump to the next topic. Network address translation.


### Slow Start vs Congestion Avoidance vtt

So we talked about the congestion control in general, but I wanted to make another lecture detailing the slow start and congestion avoidance algorithm and when do they kick in and how does the transmission look like in the charts? Right. So this is a little bit of an advance lecture. People ask about it, so I thought, I'll make a video detailing that part. Let's jump into it and discuss. So there are two conjunction algorithm basically, and they're actually more than that. But I'm sticking to this explaining those two, the slow start and the congestion avoidance. The slow start is called slow start because we start from one where the congestion window is one one MSC's maximum segment size, basically. Right. And then we start from there. So we start very slow. But also the slow start algorithm is actually increases aggressively increases the window size exponentially. And how is that work? Basically, every acknowledgement we receive, we add one. So every acknowledgement, focus on that, every acknowledgement. So if you send ten packets and you got ten acknowledgment, you're going to get ten added ten worth of RMS sizes, add it to your congestion window. Right? So that's how basically the slow start algorithm. So it's very aggressive. Compare that to the congestion avoidance, which is the second phase of the algorithm, the second phase of the congestion control, where we flip from the slow start to the congestion avoidance because there is a threshold that slow start have and once we reach that threshold flip become more conservative. What does that mean? It means once the slow start reaches a threshold, this kicks in. What does that mean? It means you're going to increase the you can continue to increase the congestion window, but only for the entire roundtrip, not every acknowledgement. What does that mean? A roundtrip is basically you send a whole roundtrip to us, like back end engineers. When you send an HTTP request, when you get back that response from the activity course, that's a roundtrip at the application layer, right? You send one request and you get back a response. That's one router. But what does that mean to the TCP? Right. A request might have 30 packets. Right. But what if only one? You got a response for one of those packets? That's not a response. That's not a whole round trip. Right. A whole round table is like when you acknowledge and receive the entire response. Right. So for the TCP, if you have a conduction window, let's say, of four packets, that's the size and you send four forth packets. A round trip is when you receive acknowledgement for the entire fourth packet. A roundtrip in this case is if you receive a an acknowledgement for the four packets, that's a roundtrip and only then you're going to get an additional one MSG size to a congestion window. So it's a little bit more conservative obviously if you think about it. So it adds slower if you think about it, right. So obviously congestion when talking must not increase the receiver window, otherwise the flow control will basically be useless in this case. Right. So we cannot exceed that. So congestion detection, what happens when we actually detect congestion? And that's basically a question that being trying to redefine what that means. There is a paper out there called the Homer paper that tries to reinvent TCP for the data center entirely. And this is one of the big things they are redefining the concept of a congestion you see in TCP. When you detect a dropped packet, that means there is a transmission timer, you send it. And if that timer ran out before we got an acknowledgement, that's a dropped packet, we consider that lost. And when that packet is lost, we detect, we assume there is a congestion. So routers in the middle, their buffer sizes have have exceeded their limit and my package was dropped. So that's what the congestion here, the home paper redefines what all that means. Go watch my coverage on my YouTube channel. If you are interested in that and if you can read the paper, you can read the paper. And if you finish this course, you can easily read that paper and understand everything in it. And that because you have the fundamentals by the by the end of this course, you should be able to. And that's that's my goal with this with these courses basically lay down the fundamentals. So so what happens when we detect conduction? Two things happen. The first thing you remember, we talked about the slow start threshold. There is a threshold where we hit. When we hit the threshold for the congestion window, we flip to the congestion avoidance, we reduce that threshold. We make it even less by how much, by whatever the number of packets that are unacknowledged that we send and we lost and we didn't get acknowledgement. However, a number of bytes is called the flight size divided by two. So I know a lot of people and some some implementation and talking about that they do just add whatever the congestion one divided by two. That's not entirely correct because the congestion window is actually greater than whatever is the flight size because you might have one segment in flight and that was detected a congestion, right. So you divide the flight size of that segment by two, not the whole congestion window. Right. Because you're going to get a larger value in this case and you can get into errors and bugs. So it's actually whatever the flight size, this number is less than this, but it can be equal. Of course, you might have your entire worth of CW and D might be lost. And that's in this case, you might be right, but most of the times you will be wrong. The second we think we do is after reducing the slow start threshold, we also reset the value back to 11s. So we essentially start over. But now when we start over, we're going to kick back the slow start algorithm Y because the CW and D is less than the threshold. So we start with the slow start, and this way we're going to reach this slow start threshold quicker than before. So it's slightly faster, but still it's going to ramp up back, right? So that's that's what we're doing. So you might say, okay, we're reducing the slow start, but isn't it eventually going to hit zero? That's bad. Well, there is a limit. The minimum slow start threshold is actually two times the maximum segment size. And this is all defined in the RFC. I'm going to reference it and it's attached to this lecture. You can look at it. I highlighted the part that is important, at least in my opinion. So take a look at that. It's cool. Pretty cool stuff. Here's the chart for the slow start versus the congestion avoidance algorithm. What exactly happened? So in the Y axis here we have the number of bytes for the congestion window. You can see we start from one segment, maximum segment size. Right. Why did I add another s? Just one maximum segment size. And then we in the in the x axis, this is the time and this is the algorithm. I copied it from the ref. See page seven. Take a look at it. But the orange thing is slow. Start. Go to them. The white is the congestion avoiders. You can see this is linear, this is exponential. So let's take an example. And obviously one thing is this slow start threshold. See the dotted lines, these are the slow start thresholds and you can see they are decreasing every time. So we start with them says we increase the increase increasing curves as we get more acknowledgment up until we hit the threshold. Once we had a threshold flip the algorithm, congestion triggered the data and then once once we hit a congestion, we detected a congestion. That means some of the packets are doing drop and my transmission timer has been expired. Go down to one that really hits. If you hit this, that really hits hard, you're on your application, you're going to feel it because now you're your transmission slows down to back to one. So now your window size reduced back to one RMS, which is, what, 1500 bytes? It's actually less than that. But you get the point based on your MTU, right, the maximum transmission unit and then we start over. But look what we did. We also reduce the threshold by how much in this particular case, this much. Right. And this is basically however many in-flight packets were there that are an engine. We divide it by two and that's how we reduce them. And now we start over. De de de de de de de de de de de de de de. Had that again. Right almost there. I need to fix that chart. But yeah, once we had that threshold, we flipped to the congestion control and then we hit it again. We reduce back to one, reduce the threshold, and then we do it again. This low start, we hit that threshold, flip to the congestion and then we detected a congestion. We go down. But look at that. We already reached the two M's, which is this basically. So this is. This is what, two? We can't go lower than that based on this algorithm because we're taking the max of the flight size and then the two MSAs. Right. So that's the the maximum we can get here. The minimum, I guess. Right. So then we move on and then you can see the chart will remain this. There are a different chart based on another algorithm called the fast retransmission. That's another algorithm. I'm not going to cover it in this lecture, but maybe in the future lecture, if you're interested, drop me a question or a comment and then the course and I'll be happy to cover it. Yeah. So I try to keep it simple and not boring as much as possible. So that was the slow start versus the congestion avoidance. Hope you enjoy this lecture. I'm going to see you in the next one. Enjoy the course, guys. Thank you so much.


### NAT vtt

So we talked about how the IPV four has a limit of 4 billion addresses, which allows in 2022 is is literally nothing. You know, we have everyone is connected to the Internet. But the problem here remains, like, how can we get more than 4 billion devices on the Internet if we have a limit of IPV four? One solution is IPV six, but nobody is bringing that up because supporting a new protocol is just you need to update all the routers and it's a huge job so people start using it before so it's there. But how do we expose my internal thousands of devices network, you know, keeping them private while still they can access the Internet, meet the network address translation. And the goal of this is you can have one public IP address, which is your gateway. It's right behind me actually that has a public IP android. But now I'm using my Mac, I using my other Mac as he my windows machine is in my other windows machine. He has in my phone. My wife has a phone. My kids have phone everywhere. My TV is connected in all my other devices, Iot devices, device connected. All of them are connected. The Internet, not all of them have public IP addresses, but all of them have the same public IP address. Exactly. All of them have the same public IP address. Can make a big talk about this stuff a little bit a summary before we start. I like to do that. So all of my devices. Have the same public IP address, which is my router. But now how do they differentiate each other? Guess what, guys? The source IP address and the source port is being mapped to a different source port and a different source IP address. That's all that's required. Liz. So there is a limit to how much you can connect. And we're going to talk about this. But this nat table, the network address is in table, lives in the router and maps the private IP addresses, which is start with ten zero zero or 192168 or 172 to a public image. So that if Google want to respond back to my I don't know Reg right. That has a private IP address, it responds back to all of my order. It doesn't even know it is actually a private device, doesn't know it just responds to the public rather than gives you the the source port, which is the station bar in this case and thereafter does the mapping oh source seven seven, seven, seven going to that l oh that was actually that particular machine. So let me read, translate that back to the private IP address and then send that back as all going to be clear, we're going to have some diagrams for you here to explain all that. Let's jump into it. Network Address Translation How the when sees your internal devices. The wireless the wide area network. So this started that this started as as a as a solution to the limitation of IPV four. But now but by then evolved to to fascinating use cases. We're going to talk about this but in the back end if if you have seen in one of my videos I talk about load balancing and layer four load balancing NAT is as one of the tricks you can do it to do layer for load balancing. Fascinating stuff. Private versus public IP addresses. You know, while we didn't talk about this, it's really straightforward to to talk about the inner whatever the authority that manages IP addresses reserved certain subnet for private use. That means they are not publicly relatable. If if any router sees ten 00. x, it drops that back. It it doesn't make any sense. It never roots it because these are considered private and you assign it for private users only and they are used only in internal routers. You know, these are the things 192168x6 or ten 2000x and these are private and not readable. You can assign hosts based on these networks. And only a router technically needs a public IP address. None of these internal devices need a public IP address. One public openness that everyone can talk to and reach. And that's pretty much enough. The router can translate requests to and from it back to the device that originated through the magic of network address translation. We're going to talk about that because you see, if I have a private IP address as my phone connected to the wi fi, I cannot technically send an IP packet with that source IP. In the wild, wild internet, you know, it will be immediately obvious like what is this source IP address 191 saying I It doesn't make any sense at all but. So you need a public presence when you're in the Wild West. You cannot go naked like this. The thereafter. Takes that responsibility almost like a proxy it's proxies you as a layer three you know and layer three proxy into the public changes the iPad as a. All righty. It height aside, since we have important stuff to explain here in a local area, if you have a Node.js server on machines, I mean, this is the Mac address, right? And it's listening on for 8080. So what happens if this machine you want to send a get request, for example, to this Node.js server in the same network? What do you do as you build the packet, an IP packet, put the data in the destination IP address the source IP address the destination for the source port. And this USB port here happens to be eight, nine, nine. Too many random port that gets picked up, right? If you want to send this across. We talked about this is isn't the same network. I asked myself, are you in my network? I applied the subnet mask. Yes, you are. I don't need to go to the wild, wild internet. I find your Mac and I put your I do an R by finding our Mac and I put the frame and I send the message. Right. But the response is very similar. You get the packet use, you literally flip everything else. So this is my JSON response. For example, my the destination this time is 192. I'm responding. I'm not just responding. Go to 8992 and the source is me. And this is the source part by example. Just flip it. So now we're sending it from here down to the client, you know, responding, but not as spy things. I believe that you're here. You're a private IP address. This is your router. This is a public presence of the router. Right. Obviously, the router also have a private, private network as part in your network. Right. Let's get as your gateway effectively. And this is an IP address all the way across the Internet, you know, and it has has this make out. We don't really care about that map Mac address. But here's the thing. If I now want to send a get request to this IP address. No, to this port from this IP address. From this port, we asked the same question we did. Are you in my subnet? Oh, well, you are not, because I applied my mask to 552552550. Your network is five five 1122 definitely not mine, which is 19216811. So all bets are off. I need to send this to my router. We talked about this. Get that Mac address of that out there. Didn't send that frame. Goes to the router, but the destination IP doesn't change. Right? You still take that send it to that outer. And here's what we need to talk about rule. But so now the destination remains 55412288. My source IP address was 192168.122 and this was my port 89929. And we looked at this, it says, wait a minute, the router will say, wait a minute, yes, I got this packet. Yes, I was the destination frame for it, but I can't let you go out like this. 192168.1.2. No way. Because the moment I send this packet as is is going to get screwed up and then tonight it would be dropped. So I need to change you temporarily change. Rewrite the IP packet so that I will. Present to you, my friend, in the wild, dangerous world of the Internet. So I'm going to put my Abia is 4411 570 which is a public racquetball address in this. Beckett And I'm going to generate a random port. Seven, seven, seven, seven. You might say, Hussein, if the moment you change that, how do you know the original one? Oh, don't worry. Not as stateful. We're going to keep our record in my table that says, Hey, 192168.1.2. On 8892. Right. That was going to five, five one. One, two, two, three, three, eight, eight. Is actually this public presence. Four, four, 115, seven, seven, seven, seven. So now if you ever get a response on this board, that means that means it's not only destined for the router that output is poor thing. Browser never actually gets it out. There is never almost never the final destination of anything. It's always it's a temporary thing to a final destination. So that's what happened. We're going to send this packet. This is the packet. The request still goes on. Now we're sending a request from here to here through the Internet. And this is the how the packet looks like in the wild world internet. You know, there's no mention of anything has to do with net. Anything had to do with private thing. There's nothing to to anyone here. Someone on this public IP address want to talk through this to to this guy and that's it. So Node.js receives it, processes it, written for JSON document. Like, for example, I'm receiving a Jason Dock here and it puts the destination. As the source. You know, it was the source and the port now becomes the destination. So I wanna reply back to the same connection. Right. Seven, seven, seven, 744115, 17. As the destination. And you are coming from me. Five, five, one, one, two, two, three, three, eight, eight. And you send that request. Where do you send it? To that outer. Well, all you care. This is not. You don't even know it. So rather I'll write it to you. It's just the host you hear being in. Yes, we get it. The destination for 115 17 777. Rather gets his like, wait a minute. All right, I got a packet. Is this really for me? Yeah, almost never. The only time Ralph receives an actual packet that is actually for that outer is when you'll go to the admin and you want to manage your router, and that's pretty much it. You know, when you go to the page of that outer, you know, the gateway and you want to like change the password or your Wi-Fi or something, that's the only time you're actually visiting your router, you know, that's the final destination. Otherwise it's almost never the final destination. Then now we got this IP address, we got this destination, our port. But wait a minute, let me look over. Might there not table. This is actually this guy and it not only looks this up because this could be dupes. So we need we need to look up all of that the destination to make sure we're fully unique here. Right. So we look up everything matches. Yep. This is definitely this guy. So now I'm going to rewrite the packet, the IP packet, so that they're actually not really the packet. It actually arrives at layer four. So it plays that and both layer four and layer three. So the browser here in net mode, it actually plays. Both roles. Layer three and layer four, not just rewriting the IPS. It's not just reading like normal Routh, just read the IP address. And that said, it doesn't rewrite anything. Right? Uh, maybe, maybe just that ECN better that we talked about to notify for congestion, but that's pretty much it. But Nat routers actually rewrite the IP and the port and that makes them a layer four and they are three and the router since back to the packet and that's how you get the response all the way to the client from the server. So if two clients locally want to talk to the same server, that's absolutely fine. Because why? Because we're going to get a completely different random IP addresses and not them, not iberia's random port that are assigned to to completely create any unique identity here. Right. That's why I changed the port here. I made I use the different port to show you that, hey, it's just another number. You're going to get another number. Another number to uniquely identify every host. So now who can tell me? Trying to do a quiz? This is not life. But you get the point. Who can tell me how many times? What is the maximum record in the NAT table when this machine want to visit the same destination on the same port? Right. So the same machine 192168.122. Fixed the destination 50 511, two, two, three, three. Right. And the destination port is always 80, 80. So now the only variable is what is the source port. Right. So now the same app can only make 65,000 connections. That's it. You cannot make any more right. Because 65,000. Because that's the 60 myth. Yeah, because 65,000. And it's maybe less than that because some of the ports are reserved. Right. So the rotor rotor will have 65,003, after which it will it will reach its limit because they say, wait a minute, I don't have any more random source for to assign you. Right. Because it's the same IP and it's making all these requests. Right. So there is a limit at the end of the day when it comes to an act. Right. Because at the end of the day, all of these random ports will be assigned right at the at the root and the public presence. Right. And not only that, the problem is even other clients will be, uh, will be starved of these 16 bit. So there is a limit when it comes to local area network and that right. If you think about it, I want to talk about net applications. So not applications, the first app, private or public translation? Why? So because we don't run out of AI by force because you can't use assign your fridge and I a public IPV four. That doesn't make any sense, right? Not this better. This case. Another use case that was born accidentally if you well, port forwarding, you know here's the trick. If you're a back, an engineer, you guys, I don't think that's much to face here. We'll all go back to those lines of support forwarding. This is the topic that we're going to talk about you. If you're back in June, you most definitely try to listen on a Linux box on Port 80 and you got an error, says, hey, this is a port that is less than 1024 and you cannot listen on that because there's a system maintained for Port one. A workaround is to do Sudo, which is a bad idea and never run your application as a root because if someone exploited your app, they have it with access. Yikes. Bad stuff, right? So what you can do is do port forwarding. Listen, on port 88, no kind of a port near port, but then add a rule in your net in your router such that if someone made a request to port 80 on a certain on your own you as a router, if someone tries to connect to you and port 80 rewrite the packet and forwarded to this machine on port 8080 port forwarded. This is a completely net solution, right? You because you're using Nat to do the mapping, but now you've forced an entry. This is not an automatic entry that is created by default. No, you added an entry in net that says if someone visited 80 go to here let the hardcoded kind of a port forward and you got port forwarding for free. Kind of neat, isn't it. So yeah, if you add an and that's entry in the router too for a package support to another machine, you learn no need to have root access as we talked about. And you can expose your local web server publicly. I mean, pretty cool stuff. All right. Let's talk about this layer for load balancing. And I had a link here at. To achieve a proxy, which is another proxy that is fascinating. My favorite proxy ever. You can do a load balancing using net a layer for load balancing to be specific. Here's how you do it. You can if you load band if if you can effectively make your gateway as the load balancer, here's how you do it. It's a victory, the complete opposite of this path forward. The client will send a request to a bogus IP address. How does layer four load balancing work so in India, specifically in Asia proxy? I think that's the first time I've seen it in a proxy genius. So here's how we do it. Client will make a request to a bogus IP address. A 100 or 100 or 100. 100, whatever. Right. As long as it's outside, it's subnet, right? And. This IP address. Go and add an entry in your route and you're on your net that says, ho, ho, ho, ho, ho ho is actually this physical IP, IP address on this on this machine, you know, and then it is also this physical IP address on this machine, and it's also this. So if your clients answer packets to Ho, ho, ho, ho, ho, it will ask the question is ho, ho! My subnet? Definitely not. Where do I send it? If we don't know where to send it, we send it to the gateway. That gateway will receive that packet. But your router is marked your router. You wrote your router from scratch. Here, write it. Understand everything. So understand that. Oh, I know this is not actually something people want to visit. This is a virtual visit. This is a virtual IP address. Effectively, that's what's called ho, ho, ho, ho, ho. All right. How did I know? There's actually this machine or this machine or this machine. Let's look at the first one and realize the destination I buried it in instead of this garbage. 100, 100 hundred. To the actual physical machine. And send that request and then complete the connection. And you can make the connection as stateful as as per the TCP rules, you know. So if you if you send another request, another connection request, it will go to the router and it will pick another one based on the round robin algorithm. So let's go to this server. Oh, this let's go to this server. So now you can literally pick any IP address that doesn't have to exist in the real world. But if your router in this case is a proxy, which is in net mode, it is effectively a software that you are because of what's out there are those are just software. And those guys came in and they had their genius idea to use that as a load balancing. And it worked. It worked. Isn't that cool, guys? I absolutely love this. So all you have to do is just rather intercept the back end that replaces the service IP address, or that's what it's called service IP of virtual libraries with the destination IO the decision server. Right. And then this is basically what layer four reverse proxy is. The limitation of this is obviously limited because now you really need to configure your fleet of clients to be gateways to your load balancer, which is almost very hard to do. Right. If you think about it, somebody IPV four is limited. It's only 4 billion. Right. IP addresses. You need to translate from private to public. So that's that's why we net obviously and port forwarding load balancing, you know, and the limitation of the IPV four kind of makes us need net. That's not that's why Nat is really powerful, guys. Let's jump to the next section.


### TCP Connection States vtt

Another very important state when it comes to TCP. Connection. States. States will prove political must have states, right? By definition, because we have the idea of a connection. Right. Then the connection must go into a state diagram. Right? As always, that action is now closed. The connection is now established, the connection is now open. The connection is now waited. The connection is now, I don't know, finalized, whatever there is state that goes through the connection. And this is what we want to talk about some of at least some of these states and how critical they are. Let's jump into it. So TCP is therefore for protocol. By definition, it needs to maintain state and you better get these states right? Right. Both client and server need to maintain all sorts of states, right? Window size sequences, the state of the connection know the the length of the so you when you maintain all sorts of state windows are the sequence the state of a connection and the connection goes through many, many states. Let's hide again. Here's a diagram explaining the four way handshake to closes that connection. This time I added the states from left to right here. So the connection is start established, assuming it establishes the connection or starting technically closed. Right. And you establish it. But I'm not I don't want to go through that. I want to talk about this particular for states. The moment you want one, a client want to close the connection, it will send a fin. We talked about this. This is, of course, the fourth four way handshake. It will send different and it will go to a circle fin. Wait one. It has to wait. Right. And immediately to a moment the server sees the fin, it will go to a second clause weight and will send acknowledgement. Right. The second state and the server. Right. It will send an acknowledgement. I will go to a state called clause weight. The client would receive the client who initiated the fen. Right will go into ethical fin way too because they received an acknowledgement. Now we will incrementing in a state here. Meanwhile, the server will send the last that will send the fin its own fin to close the connection and will go to end to a last acknowledgement state. Right. And then the moment this client receives that find, its job is done. It goes into as they call time weight and is since the final request or the final segment. And that segment is the acknowledgement of the fin. The thing is, we don't know because that's the last one will never wait. The server will never respond to anything else. So technically we don't know if the EC is received or not. This is called the two general problem, right to general problem. You know, the two fighting generals where if I send an EC, how do I know if the actions received? You don't know. You have to wait for an approximately 4 minutes. And this limit is defined by the error of CRC, but the maximum segment length out there, which is 2 minutes times two. Right. So let's wait for the whole 2 minutes until we make sure that the segments are all the segments are expired that are in flight. And this is the safest way and after which. That 4 minutes is done. We're going to close the connection. That's why when you do a nets state. Right. Nets that sorry you can. You're going to see a lot of connections and time. Wait. And yes, those take some memory because you will have to have your file discussed. They're open effectively, right? And they take a finite amount of memory effectively. And those source ports cannot be reused. They're being used here. You cannot really use them for anything else you have to wait for. The connection is closed and you cannot kill the connection immediately because data can't be send during this time and accidentally you will have an attack called the session resumption attack, which is a kind of a dangerous attack. Right. You don't want to do that. So that's why you leave the sockets open. But one trick whoever requested the fin will end up in this state. So if the client requests that the phone or the server request to defend, they will end up into the time wait state. All right. So if this is in annex, for example, on a chair, proxy or envoy and it close the connection, it will end up in timeout state so so as a back injury. June, what can you do here if you think about it? Well, I don't want my server to be flooded with this time waits right. And I really want to close some of these connections. You can if you own the front end and the back end, you can design a protocol such that you send a request, a normal request that is just normal data that tells the client to close is in a state. So you send a request and the client will initiate the field for you, and as a result you will clean yourself up a new close. Right? But the time wait will be declined decline because they don't care about client. You're going to have a lot of clients and you are you are one server. So that's one trick I found interesting where you can have the sockets time waited on the client side right and how do you do that if you control both the server and the client which is most full stack you know implementations don't do you can do this right you can do this trick and instead of actually you as a server closing these connections, initiating the first no do do the opposite, you know, send out a normal request, you know, a push notification to the client. That translates to effectively a clause of a connection, right? So if you receive this data, that means clause yourself. Right. And since the client will be initiating it the time where it will be in the client and you don't care about the client here. The most important thing is the sober here, right? Right. That's where all a lot of it's. And that's I find this very useful. All right. And with that, let's jump into the next lecture, guys.


### TCP Pros and Cons vtt

All right. The pros and the cons, the power and the drawback of this. And there's always a pros and cons for everything. ECB is no exception. The prose of TCP is exactly the cons of UDP. If you think about it, plus a little bit more, you're guaranteed delivery. If you send packets DCP going to control the transmission and it will guarantee that you get it. No one can send data without prior knowledge, so no you no monkey business where you spoof IP addresses and try to be put in something else. No. You get a well, you get a you got to establish a connection first to do that. Flow control and congestion controllers are two important features which we discussed in length about them. You know, very critical concepts. Order packets. No corruption or no or app level work. No. If you want to order packets. If you don't have the order package, then you have to order that. The application level I give you using UDP, you don't have to do this right with DCP. Now we're going to talk about how ordering back is actually a cons as well. A bad thing about ECP or certain cases at least be. Yeah, we've got to talk about that. So there's going to be a whole lecture about it. But for the general purpose, it's ordering packages is good. You know, if you think about secure can be easily spoofed. You know, I I'm going to say because well you still need a good authentication, you know. Well, you cannot just easily spoof because you cannot just send random data knows that. So you cannot easily dos with DCP as easy as UDP. But uh, how do you know that you are talking to who you think you are? You don't. It's an IP address. It doesn't mean anything. It doesn't have knowledge about the person. Right. That's why you need an authentication of protocols that just tells what has a certificate which is signed by a third party that proves that this guy is who say they are. Most mutual deals, you know, client and server. Cons. What's bad about DCP? Nothing. DCP is perfect, guys. Okay. Large header, overhead, competitive. You seen this 20 byte up to 60 bytes, you know, yuk. 60, 20 bytes. What a shame. That's a lot of header, man. So definitely takes more bandwidth than you would be if you think about it. It's stateful, so it does consume memory compared to UDP. Here's something that I have to keep running on my server and this time with that we talked about right there, you're going to sit there and take a lot of memory, but that again, therefore a good reason and the time waits thing the states, unfortunately, we need them to protect against these attacks right there a player that's considered high latency for certain workloads know slow start congestion and EC so considered high latency for certain workloads slow start. There is congestion control, there's access. So there is a lot of stuff that is going on. So we start very small and then eventually we grow, right? There's so much limitations, right? So the latency are hit very early on. Yeah, sure. We can go very high medical action, but after what? Right after, after I talk ahead. Latency is an acknowledgement. I have to wait for acknowledgements. Right. Sometimes I don't have to. Depends like all of this to go to the more complex and complexity hurts and it does too much at a lower level. What does that mean? You see quick was invented because of a limitation in DCP. And we're going to talk about it now in a minute. So single connection to send assume this you have a single connection, a single DC big action and you want to send multiple streams of data, multiple DB requests, want to go to the same connection to the same server through one connection. How do you do it? Right. How would you do it? You would send in multiple requests. But guess what? I get request my 15 two hour segment. Right. Let's say one get request. You want to send four get requests, right. And each fits nicely into one segment for simplicity. So it's just 1.5. Right. And you send four segments. All right. And let's make this big a little bit, though. Is it for segments for get requests right in the same connection? So segment one, two, three, four, and then order. Sure, no problem. But here's one problem. Segment one, which is request, one segment, which is request to segment three, which is request three. Segment four, which is a request for four. Get request, right. And let's say segment one didn't arrive. Which is request one transfer to request one. The application stack didn't arrive by request three and saw a request to request three. Request for they all arrived. The segments arrive, but guess what? Two These are completely agnostic. Request HDB request. You know, requested to be request has nothing to do with each other as stateless. Stateless. So no request to request. Request for arrived. But now I have to wait. Why? Because stinking segment one didn't arrive. Why? Because we have to order. Because. One, two, three, four. You know how to order. You get two or three, four. So I can do anything. There is something called selective acknowledgment, but I don't want to talk about it here. So a system doesn't exist. So two or three? Four didn't arrive, but one didn't arrive. So guess what? I'm not going to do anything I can't acknowledge. So they came out. So I starved for requests. I started technically three requests just because a stinking one didn't arrive. And that's called the head of line blocking. It's trying to do too much. This is trying to do too much. Or maybe we try to overuse the DCP. I think that, yeah, we as a DGP try to use the DCP in in in a in a way that is not intended to. Right. We want to paralyze requests, but no can do that. So what happens here is just you blocked. So you have to rescind the four stinking get requests again until they arrive in order. Yikes. And that's the main problem with this appeal which led to the invention of the quick a.t b3i believe should be three was invented first, right, and then quick was invented and stripped the htp stuff, you know, the headers and all that stuff and they made it quick out of it. Right. So yeah, quick was invented because of this particular method, the ordering and TCP was this TCP head of line booking. Right. So yeah, stream one has nothing to do with stream to or request. One has nothing to do of course to. But yet both stream one stream to Baghdad must arrive in order to do that. So that's a big limitation when it comes to the ECP in this particular scenario, you know, no, no. Everybody and only everybody uses the same TV connection to send requests. Right. At the end of the day. Another common thing I've noticed people doing is this. This is the flip of this, right? I know those people in the database back in engineering community use a single TCP connection between the back end and the database, and they would expose a web tier, you know, request to receive a DB request and. The multiplex multiplex. All these are, of course, into a single DCP condition. Bad idea. Why? Because if you sending multiple query sequel queries in a single, single DCP connection, these queries will be translated into two segments. Right. Sure. Segment one will be query one. Segment two will be query three. Segment four. Three will be query three. If they are arrive in order, they will be delivered in order. Okay. Go execute sequel one Go to Go. Sequel to Go Excuse Sequel three. But what happens if Sequel three finishes first? What happened? You got the socket wall, right? The sequel. Three result. No databases will guarantee order. Right. How? You know? How do you know that? Oh, because the sequel. One, two, 3 hours arrived. Let's let's wait. No, you cannot guarantee that if you don't guarantee. Sure. But if you can't, then you. You're done. Sequel three was the fastest, so it will respond first. Now the client. How does it know which responses for which query? It doesn't. Because it's just a stream of data. There is nothing that tags anything here unless the protocol allows it. You know, if you include the query ID in the response, sure, you can do that. But how many databases do that? Right. If you were sure, by all means do it, but don't do it. That's why you use pooling when it comes to databases. I know I'm going all laboratories, but I believe this is all related. Right. It is related. Right. These are connections, databases. That's why you're here for IEEE. On to bridge that gap, guys, between what are we using on a daily basis and what we're studying in the TCP land? These are people down again. Discipline is not a good idea for VPNs just because if because we use DCB on a daily basis, if you use VPN and you establish a connection between yourself and the VPN as a TCP connection, and then took the TCP segment and encrypted that. Then you will be using a TCP inside a TCP, so you'll be doubling the congestion windows. You have double the congestion windows, you have double the receiver window, you have double the flow control, you have double the congestion window control and you have double the transport retransmission. God knows what will happen. TCP you will literally melt down. It will melt down, it will retransmit, but then it waited and then the inner TCP blocked, but the outer one didn't. And then acknowledgement of the second one received, but the inner one did. You have no idea how bad this this can get. And there is a video by a computer or file. Actually, we will explaining this if you want to watch that. And guys, this concludes our TCP section. The next thing we going to talk about is that we're now going to make a little bit of a demo, uh, spin up some, a TCP server play with a little bit. I'm going to get you to do some practical. You're probably bored of these slides, you're probably bored of me talking. So we're going to do some practical stuff, you know, listening to a server do and, you know, TCP stuff, you know, do some TCP work a little bit, though. We're going to do the final section. Note the final section. I'm going to do the next section, which is. One of the most fun sections there. Let's jump into it.


### TCP Server with Javascript using NodeJS vtt

All right, guys. So we talked about TCP. You talked about how TCP server works. It's very interesting, you know, understanding of these fundamentals. But how about we actually build a TCP server now and I'm going to use Node.js for that JavaScript. And as usual, we're going to use a high level language and a low level language. I know CE is also called high level language, but let's be honest, it's not it's not a high level language anymore, right? It's very low level. It's close to the metal, you know. So how about we actually go ahead and create a TCP server this time? So I'm going to go ahead and create open your folder, go to my project's networking. Networking course. I'm going to create a new TCP server slash JavaScript here. Let's go ahead and do that. So I'm going to do here is create a new file that's called Indexed or MGUS, and we're going to do import net. We're going to need a library as usual net from net. This library is available by default, so we don't really need to install it, I don't believe so. We're going to do here is to create a server, right? Just like we did right. With UDP, we're going to create a server. Right. And this server will take a kind of a lesson or function, you know, and a one on a successful is going to call this function. And this is basically the end line function that's called the socket. And it's going to give us a beautiful socket the moment we get here. That means that VCP handshake has been successful. You will only get this function called when the TCP handshake is fully successful. And that means that it has been. It has been cold, you know, the Cincinnati and all that jazz. Right. So I'm going to do a let's do a console message or console log in the message itself in the in the services. Okay. TCP handshake successful with with whom exactly. I guess I can do that other, you know, template on button. Sure. Let's do different things so we can do the socket. Dope was the address actually. It's called the remote address. Right. And then we're going to do this, then we're going to do a socket dot remote port. Right. This is the client port. This is the source port effectively. But and we can choose to write something to the client back. This is how you write data to the socket and I'm going to write. I don't know. Hello, client, this is me. The server now sending a hello message. Hello client to the so this is this will fit into a TCP segment right. Speaking technically speaking here and we'll send it out if Nigel algorithm is in effect and I have acknowledgement this won't be sent until as a full segment is basically completed. So I'm going to create an event here where when we receive data from the clime, I want you to call this Bobby. I'm I know. Just like, okay. It was a received data that I still receive, right? I'm not sure we're going to data to string. I'm going to compare to to estimate because it's a byte array. I believe that's pretty much it. I'm going to add a breakpoint here. Break them out. We forgot the most important thing. What did we forget, guys? We didn't listen to up. All right. So I'm going to call it 8800 here. And again, we always listen to on a specific I just make it a habit to listen on an address that never listen to all especially if you push this code. Oh yeah. It can become really problematic, as they say. And let's initialize my project. I don't think I need to install in net, so I'm going to go ahead and run that through without installing. Yeah. That's what I thought. All right, now, what do we use in order to connect to this? Let's go to the terminal. This is our UDP stuff. We don't really need it. We can use the nets, cats, the same command, but we don't need that -- because default is TB mode. I'm going to connect you to 001 and then port wide 8800 and immediately the Nets cat will establish the Send Sinek ec and I am here and I am saying hello. I printed his anything even tells me the source port 64409 as very close to the end. Huh? And then right back to the client. Hello, client. We got a message from the client and now we can just chat. I just hit enter and that is effectively a command. So that single wide. How sad is this, if you think about it? That single inter fits into a single segment, which would be with a 40 byte header from 20 from the TCP and 20 from the IP is just sad if you think about it. Just a waste. But well, we like to waste stuff here. So here, let's say Edmond on tag. Did we get it done to data? Yep. We get it as a bi array so every bite and then finally operated enter and then obviously this is a boom and we can play with this weekend establish multiple TCP connections with the same server. Hey, let's try this as actually new tab. Can I do this? So if I do it in c12 7.0.0.1 880 8800. That is another DC v connection. Now you have two sockets running right to physical. So now we're on the session layer, right speaking. We just received so we have an ATC connection from 64409. Now that is a progression from 644 28. So that's what we have today. And, and this guy can send stuff right now apparently and enter actually in init cat. It triggers the send command here and we can still the second the first DCP action is also alive. You know, and basically this is what I want to talk about. How about we jump into it and do another lecture? We will talk about a little bit of low level details in the C writing. Is C little bit intimidating because I'm not comfortable with all the language, see? So I took this code from GitHub and I'm going to cut it out of everybody who wrote it. But how about we jump into and hope you enjoy this again. The code will be available for you guys. Thank you so much.


### TCP Server with C vtt

All right, guys, just like we did with the UDP server, we did UDP with C, we did UDP with JavaScript. You seen how simple it is in JavaScript versus C, it just makes you appreciate what with this stuff, what is happening behind the scene, all the memory allocation, you have to do everything. You have to look at Instagram and really the work that did the server, the back into doing is a lot, you know, the appreciate every single memory allocation and this way we can see the both sides. We did the JavaScript version of the ECP server, now we do the C again, we took this from Nicole, so let's go ahead and look through this. So these are all the headers we need identical to the UDP server and this is the hard coded port 8801. We're going to need a socket file descriptor as usual. Going to need a server address and a new address. Right. This is me, my server. This is the destination server going to create a socket. Exactly the same thing. And remember with the UDP we did a socket socket object, another means IPV four and socket stream. That is different than the diagram because I need TCP now. All right. So that socket file descriptor is still a number when you put it. This is actually just a number. But if you look at the operating system table, the socket for the script at table is actually have more entries there. And this is what I want you to think about. The more socket file descriptors, the more memory you're going to need. I said the memory and I said the of that family, the server address, the port, the order identical everything. I didn't vote the 80 year. The only difference really is the as the this thing really so identical. We bite the socket we got the server address, we'll get the server address, the the size of it, and then we're going to print that. Hey, I successfully bind to that. And the only difference really now binding and listening is kind of different. Binding. That means any packet that is received, go ahead and. And and deliver the data grams or the segments to me. But binding that you really need in TCB, you need to bind and you'll have to listen. You have to listen for incoming connection requests because this is different. Right? And you have to also not only listen, you have to accept connections. This is where the it's a game changer when you understand all this stuff. Right. You're going to listen on the socket file descriptor for incoming connection request. And the moment you get a connection request, you have to accept. One thing to do here, guys, is listen is what performs the TCP handshake? And this is done by the TCP stack lower than us. And if you know, there's a there's a number parameter until the backlog. In this case, it's five. And this number is what is this? And this number is the queue, the maximum number of number of connections that can sit in your queue without getting accepted. Now, when you accept the connection, it comes to your application and now you can consume, you know, your queue is minus one, right? So this way there is a limit. If you accused useful and you didn't accept these connections fast enough. A problem proxies face. By the way, how can I accept connections as fast as possible? As how will you do threading and multithreading too, just to accept as fast as possible and make the queue obviously bigger. They won't accept these connections fast enough. You queuing before and if your queue is full, this what? The connections won't be handshake. So if someone tried to establish a connection for you and your application queue is full, they won't get it's an act back. Very interesting thing. This is compared to Node.js, right. Which does everything for us just makes it makes us appreciate this work in Node.js. If you remember, it was literally I didn't do anything I just at a function and then, hey, it's doing it by default. It accepts all the time. Here you have to call a function to accept a connection request. And only when you do accept that you're going to get a socket, an actual socket that you can start using to send this information. And so that is kind of the difference between TCP and would be. So when we accept the connection we're going to get, then you address that from the remote address which has the source port and a distant source point in the source address. Mm hmm. And then we're going to get a new socket, and that's it. You can use this new socket to send data to that. Right? So this program immediately prints, uh, it copies a string. Hello to the buffer, you know, because this is how we need to do it. And see, you have to copy the string to the buffer and then send that buffer object, you know, and they have to tell how much, how much data you are sending as well and that you're specifying which socket to send to. And the operating system will say, okay, this socket belongs to this file descriptor and it connection is accepted. Yeah, it is full at full full connection, you know, and that and after y after all we'll get it just immediately send and close the connections. It's very simple program. This program only accepts one disabling connection and immediately quits. So if you want to accept a lot, if you want to build a lot of sockets, right, you have to build an array of new sockets of this new sockets. And then you have to knew and you have to every time you have to loop and then accept. And every time you accept, you have to add it to this list of array and then you play with these sockets effectively. We didn't have to do that in JavaScript because JavaScript had the concept of closures and it beautifully treated. It created two object for us and each object is is in a different scope. So the first connection was in the first one and the second version was the second one. And every time we send, we triggered the right object here and see, sorry you had to do it yourself. So go ahead and run this program. And JCC may not see. And then eight out. We're binding and we're listening. And again, you get only one connection. So let's do it. Let's do it right. It is. You don't want. I believe then it's on you to be. It's you to be. So immediately we could the connection who they send us. Hello. And we're done. Immediately closing the connection. So this is a very simple program, but I thought it would be good to explain what exactly happening. And I think I believe always makes us appreciate this very this. You have no idea what people do to make this performant that accept no accepting is the most important concept here in TCP servers or proxies they so some the way they do it is if you do a loop and you do accept sure that works. But there is a limit, right? Because now your main threads now is looping and accepting and you don't have time to do the work. So what they what people do is they do the they make the threads except they made the main thread except the connection. And they create a thread, they create a new thread and send that socket to their thread to do the work. And because they cannot do everything and want to thread at the end of the day, right. So people invented a lot of stuff. And again, you were talking about 20 even more than that, right? Work of people just doing stuff and innovating. And we're looking at a simple C program here, but just makes you appreciate. I thought it would be a good idea to share. Hope you enjoyed this, guys. See you in the next one.


### Capturing TCP Segments with TCPDUMP vtt

Now that we wrapped up the TCP section, which is one of the most important protocols really in existence, how about we actually look out how it looks like in the wire? And as usual, I'm going to use the TCP dump. Tool that it would be using throughout the course, which is a very, very popular tool to inspect and capture really any kind of packets, you know. And because the TCP sits inside the IP packet, we're going to see the IP packet and its content and TCP dumps does a good job showing us the insides of that and tries to parse it out for us. How about get started http? At least the vanilla one sets on top of TCP. It uses TCP. So if I'm going to use if I'm going to browse for a web page. For a given domain, I'm going to establish a TCP connection between myself and then domain IP address right after. Obviously I do it then hence I can get the IP address and that's what we're going to capture. And after the TCP IP handshake happens, the HTTP protocols takes over and sends specific segments, you know, using the TCP segment protocol format. And those will send the methods, get slash, you know, post anything like that to the observer and the observer will reply back with the content given to that path. So I'm going to use the example dot com for the example here. So let's get start. So as usual, going to do TSE bump dash nw that means I'll want the numbers. Right, show me that. But show me the IP addresses. Show me the protocol numbers. Don't show me hey, don't try to show me HTTP or DNS this or the host names. Just show me the numbers. I like numbers. No verbose mode please. No I e in zero the interface again you can look this up in ie of config. You can use iPhone first to find what's your interface so you don't have to listen to everything. You just listen to the interface you're actually capturing on because again, you can have wi fi ethernet, you can have loop peg loop back with a B, right? And what we're going to do here is I'm going to capture I want to know the IP address of example dot com. So what is that ping example dot com. It is this. Right. So I'm going to copy that guy. And. Do source this and. Any IP packets where the source IP is this or any IP packet where the destination is this. And I could I could add something like that and port equals 80 because I'm going to use the unencrypted HTTP protocol in this case so I can use that to to add an additional filter. And that should work, too. Let's test it out. So now we're going to be capturing TCP, right? Specifically on Port 80 and anything going to this IP and coming from this IP. Right. So let's go ahead and refresh and look at this beauty. Let's take a look. TCP guys. So that is the first IP packet we received. Let's highlight all of it. Where does it end? It ends right here. So the highlighted part is the first IP packet and. Clearly this is the one we sent, right? Right. This is what we're sending. So the time stamp IP differentiated service is zero. Time to left, 64 ID zero offset zero flag do not fragment. So now something is kicking in here is very interesting. For TCP packets, that's the first time we actually see the don't fragment. So do not fragment this IP packet. So this is either a something that Safari did, which is my browser, which is is designed to maybe send IP packets that are not fragmented for, you know, for consistency reasons and avoid the problems that we talked about in this course. Or maybe this is the operating system setting it by default to enable multi path into you discovery. What are we going to talk about in the future, you know, which is the way we detect the MTU of the network that we're in. So it's don't fragment. That means, hey, do not split my IP packets. If you detected an end to you that is smaller than than the one where the one we sent basically. So then the next thing, keep it highlighted. The next thing is protocol. The protocol. That's the bit in the TCP. And actually this is the bit in the IP. Packet header and that's six, the value is six and that's DXB. We seen that UTB was 17, TCP six, I believe ICMP was one and length is the final one, which is 64. That indicates the one that is the total length of that IP packet. And remember, guys, do you know what is the first thing we send when we want to establish a TCP connection? That's right. The CIN. Right. That's the CIN segment. It's why n so let's continue. This is the my source IP and since it's DCB, they're going to be a source port and this is my source port, right? 51502. It's random randomly generated by my operating system. And then this is the destination I'm going to this IP address as an example. Where are we going are going port 80. How did I know was port three. I never wrote anything that says 80 here. Well that's the default right? If you don't say anything in the browser using HTTP protocol, they assume it's 80. If you want, specify a port, you add a colon and. Put the board there and then Colin ends basically the IP section, if you will. That's not entirely true because we actually mentioned the port TCP dump kind of mix and match here. But now we start the actual TCP IP stuff. No, the segment, the flags, these are different flags than this. This is the flags for the TCP header. Now this is the segment header and sw what is SW stand for. That's the cen. That's right. So now we're sending a syn checksum if you remember from the course we talked about the checksum in the TCP segment, that's to ensure that things are correct and not broken. And that's the sequence number. Look at how big is this? I think that's obviously starts with a sequence and that's the window size, right. We talked about window scaling, right. So that's the window size six, five, five, three, five. Again, that's not entirely all of that because there is something called the window scaling, which is a multiplication. You can this is a factor of six or 2 to 4, six, and then multiply by that. So you can go up to a gig with this thing, which is good stuff. Then the options, this is another section of the of the segments itself and the option says, hey, my maximum segment size is actually 1460. And how did we find out? That's because. From my M2 and my wife I am to you in this particular case is one 500 -20 for the IP, -20 for the headers for the DHCP and -24 the headers for the IP that gives you on 460 in op window scaling six. Nope, nope, nothing. And then this is the time stamps. That's a whole extension, right. That, that uniquely identifies the segment just in case your connection was so long and your sequences ran out of the 32 bit number, which is the 4 billion you basically sent 4 billion worth of segments. Then you're going to reset. Right. How do you avoid this wrap around is by having a timestamp. So you're going to see this timestamp all over the place. Right. And this is, I believe that a reply from the Times that these are two related to each other and this is the selective acknowledgment. Right? So I support selective acknowledgment. We're telling the server that we're supporting selective acknowledgment in case you want to acknowledge gaps and ranges. I support that. And the length is zero. So this is this length is different than this length, you guys. Right. The length. The upper length is the IP packet length and that's 64. That segment length is zero. There is no data for the segment. Literally, we didn't send anything and we talked about how TCP was it called Fast Open. You can send data with the content effectively, right? Which is pretty cool. And that in this case with the Syn, you can send some data, but that's a wasted effectively that's a wasted 64 bytes send it just to receive by the server. So that's the first IP packet which has this SYN, right. So we received something from the server. Let's take a look again length zero. It's just so sad. You know, think sending valuable you can send so valuable information with this right. And if we as backend engineers just thought about that a little bit and says, okay, how much waste is really there? There is so much. If you look through the actual network, there is so much wasted. And that's what we need to kind of appreciate here and try to do better effectively. So this is the snack basically, right? I'm not going to go through the IP packets because we know all this stuff. The length is 60 and the server is sending something. So the source is the IP address of the server. The source I port is 80, the destination is me 51502. Obviously guys, this never made it to the server. This private IP address never made it to the server. The server is seen have seen my router's public IP address, but then my router changed that. So it just appears as if it's sent to me as a as a private IP. Right. So if that is being done on my server and this is sin ach dot means ach, I believe that's what I deduced here. Checksum sequence, another sequence, another ach. So now we're asking, you know, there's an acknowledgment number, the window size, the maximum we can get, but then also have the selected acknowledgement and we have a window scaling of actually larger nine compared to the six that we send server telling us, hey, I can handle more. So this is the window size that at the buffer size of the server and this is the window size as me as a client, right? And the length is zero. So nothing said the final IP packet to finish that to finish out the the handshake is this basically doing sin ach sin snack and then attacking the final one. Right. It's interesting. I don't know if you noticed, but the the second packet came in from the server. It didn't have the donut fragment. So me going to the server, my, my IP packets were flagged to do not fragment but coming from the server. Right. It did not have that flag. It's just interesting. That means the server decided not they don't care if the fragment or not and copy from a problematic right if you're using like a protocol like quick they the both servers have to agree to do not fragment in this particular case but that's that's not that's going to not going to be a problem in this case let's continue. So obviously this is the IP is the honor fragment length 52. That's what we send back 52 and then LAN zero. So notice that we didn't send any meaningful information in the handshake. It's just, well, there is data being sent, but it's not user data if you think about it. Right. It's just length zero. It's just so sad, you know, that we're wasting this stuff, right? That's what the TCP Fast Open takes advantage of that tries to send data with that and Cook does the same thing. Not quick as a protocol does that right. As I am a handshaking, I'm going to send data, I'm going to send the TLS handshake stuff, I'm going to cram as much data as possible. So then so that's us, right? Sending to the server, acknowledging heck we got it and that's the dot because just it's just an act that is no send anymore. And then we get an IP, another IP packet and this one is a large one you guys. Because what has the response from the server. No, that's actually not the response. Remember here, we're just about to send the actual get request. Now. That's going to be large because now I'm sending a request to the server and this is the application sending stuff, because this was just the handshake. Now we're actually doing business right again, little known fragment protocol. Look at the length. It's larger, right? 412. So my it's still not the maximum segment size, which is good. My request was small, but if you have cookies and stuff like that, this can really shoot up into multiple segments easily. Right? And this is where the slow start algorithm kicks in and all the, you know, the congestion control and all that stuff. Right. Then 192168254144. Same source board destination. Now we're doing a push right p right. The flag now doing sequences, acknowledgment, one options. Nothing interested here, but this is it. The actual segment size is 360, right? But the total is 412. This tells me that we have an override of over 40, but it's actually more than that, right? Because of the other options that we keep adding for the headers. But the length is like 30, 60. And look at this HTTP length, 360. I like that. So this is something that TCP actually does. Sorry. TCP dump does. Yeah. So get slash http one one. So that's what the browser sent is sends a host header example dot com. This is the start of the protocol. Right. Get the method slash is the path. Get http one. One is the protocol host. The first header example is come to the second header upgrade in secure request one accept the third header to accept all this stuff. Right. And user agent Mozilla Macintosh Intel Web Kit. This is basically for backward compatibility, you know, because although I am using Safari, you should only see a user agent of Safari. But for backward compatibility reason we have to send everything because all the web servers will not were looking previously on only certain things. Right? Otherwise they will break. Right? So that's why we send everything except language and GZIP Right. I accept this algorithms to compress, to compress the HTTP content, right. Because the Steve is large, right. And that connection keep alive. And then the server. What does the server do? The server actually sends us. Wow. Look at that. That's the server. The server census. An IP packets that is empty. Right. I take that back. They send us a segment that is empty and the IP back is around 52 bytes. And that's just an act. We're just acknowledging that we received your massive segment. Right. Well, I'm just acknowledging that that acknowledgement. Right. Is delivered to me while the process and the web server is actually executing the request, because that was the request. And only when we receive that and it's delivered to the web server, which is example com, they will start processing that get request and then they will actually send me the content and look at this huge payload and that's where that reason hopefully you guys can see the start in the end. But this is a this is it. Right. So that's the whole content. And IP and let's stop TCP. The IP packet is entirely a kilobyte. That whole thing is one kind of and again, this is an and nice website. Imagine I go to, I don't know, Twitter or some I don't know, a website that is Facebook. This is going to be huge. Right. And this makes you kind of appreciate what you guys do. You know, on the back end, it's like anything you send is goes here, right? It's like Jose Da, of course, but sometimes we don't feel it. We cram Java scripts and spaces and stuff like that and add CSS and mass and or bootstrap and a lot of junk, you know, and all of this is, you know, taking it from our performance, if you think about it, let's continue. 903 It this is the word the server sent. It sends to us again the same destination, port push, checksum, sequence, window size one, three, three, the scale. And so this is they're doing some rescaling here for the congestion window option no problem times length 1022 you know so that's the actual length of the HTP, you know, count which is the segment. The segment is 1022. The actual the entire IP packet is 1074. But the actual actual useful content, the content length right is 648. How many links do we have? This is as an application developer. You guys might familiar with this because you've seen this. This is what you actually send back. That is the size of the HTML content, right, that we send back, which for some reason I don't see it here. Hmm. I think I should add another flag to actually display the content because it's not here because that's the. Yeah. Okay. It says geez, episodes compressed. Right. All that stuff, all the headers, there is an E tag obviously for caching purposes, the expiration day, blah, blah, blah blah, all that stuff and then. We do this? There is another IP packet. What is that? Says Dom fragment coming from Mina. I'm now acknowledging the server as say Hey server, I got you, buddy. I got you. I got your response. That's it. Right. And then the rest of the stuff I believe is, well, we'll find out what it is. Let's see. So there's another IP packet right here. Many jobs are here and. That. That IP packet. What does it do? It's a fin f, which means hey, V-neck. Who's who's initiated the fence. Fence means we actually close the connection. Right. So that's my browser probably. I see who's who's in ask me that's my machine. My machine says hey close the connection. Usually browsers don't close the connection directly. Let's see. When did that happen? It took us like what, 200? 100 microseconds? No, I take that back. Look at that. It was the connection was closed after. 11 seconds of inactivity. Because remember, I was talking after we received a request and these came in later. They came in 11 seconds later. Wait a minute. What kind of math? I do this. 30 seconds. 31 seconds. So exactly 31 seconds. You can say 30 seconds. So that's like a very specific number Safaree has. No, I love this stuff, you guys. You know, how much can you tell about the reverse engineering and application by just looking at what the output is? Look at this. We just discovered that the time out for Safari as a browser is 30 seconds. If nobody sent the browser of the client didn't sue and didn't send or didn't browser, it didn't do anything for 30 seconds. The browser will close the connection because that's the 30 seconds, because assume this is ten, it's very close to ten, right? Almost, almost like look, this is a microsecond, right? It's very close. So it's very close. If you if you do if you minus this, there will be exactly ten, 30 seconds. I love this stuff, you guys. Yeah, I know. I'm a little bit excited about this stuff. I know, but yeah. 52 I had sitting Hey Finn Finn ate up Finn and I'm friend I'm offended I'm offended I'm finished Close it length that we continue what do we do? Who's this? Who is this? This is the server saying okay. Fennec. Fennec. Fennec. And then the server responded. Okay, I agree with your fan. Right. Again, these are empty stuff segments. And then the final thing, the server or the client actually will say, okay, ach, I ask your fan and that's where we're going to be put in a TCP wait state and that connection will be closed. And yeah. That was TCP, you guys. Pretty cool stuff if you ask me. I like that stuff. I like looking deep into all this stuff and I appreciate this stuff. So by the way, you can, by the way, export the TCP dump into a file and have fun with that storage store. Let let's just do a dash W and then get to B, dump it into a file. Obviously, you need to do a dash or to read that file. You cannot just use it or text editor to do that. All right, guys. See you in the next lecture. I hope you enjoy this stuff and make sure to read the course and if you if you're enjoying it. Appreciate you guys. Thank you.


## Overview of Popular Networking Protocols


### Networking Protocols Introduction vtt

Welcome, guys. This is a brand new section on my networking course and I called it overview of Popular Networking Protocols. Now that you are in a place in the course that you already know that what an IP protocol is, what a TCP protocol is, what a UDP understand that anything, any protocol out there must be built on top of these three protocols unless it's very low level, then it's built on a layer on a lower layer protocol than that. But most of our protocol that you work with out there on a daily basis, HTTP built on top of TCP which is built on top of the IP CDB to build on top of TCP which is built on top of IP DNS built on top of UDP which is built on top of IP. What else? As such, built on top, directly on top of TCP, which is built on top of IP. And there are many, many other examples as I as I can think about it. Quick SDB three, SDB three is built on top of quick with a brand new protocol that is built on top of what UDP. Exactly. So UDP is built on top of IP. So many other examples that comes to my mind. If I if I can remember a lot of TLS, TLS is built on top of TCP, although it is kind of a protocol independent. It is by default built on top of TCP. So that includes HTTPS, which is best built on top of TLS that uses TCP. So TLS is basically the encryption protocol that allows to encrypt to communication. So what I'm going to do in this section is give a brief overview of each of these protocols and why why brief now? Because each of these protocols that I talked about deserve to be honest hours of content, you know, many, many lectures, tens of lectures, because they are so in-depth and there is so much history. And as this protocol evolve and I won't do their justice if I started just, you know, talking about them, you know, like in detail here, this is not the venue for it. I think they deserve to be their own courses. Then in the future maybe I'll start working on that. But for that it's they do deserve an honorable mention to be mentioned in this course at least, and given like an overview, maybe five, 10 minutes, very high level summary. I won't go into details. I'll pick one protocol and you're going to see lectures filled as you progress through the course. Whenever I have time, I'm going to add more lectures and based on your, you know, requests. Really. So how about we jump into this section, guys? Enjoy.


### DNS vtt

DNS domain name system. This is one of the only protocol that is almost always exercised as we progress through our day working with computers. You know, every time you type in a URL. The first thing that we do is query the DNS, our DNS resolvers and find out what is the IP address that is matching that domain so that we can turn around and use TCP IP in order to establish TCP IP handshake to that IP address. Because the Internet protocol works with IP addresses. The internal protocol has their own addressing system, which is we talked about that entire section. The IP protocol, you know, has its own addressing. The problem with that, as you might have known, is nobody really remembers numbers and IP addresses. So this is one of the reasons why DNS is involved. How about we jump into it and discuss how DNS works? What is DNS? Why does it there? How does it work? And then go through an example actually, you know, get our hands dirty and query some DNS records. Let's jump into it. Why do we need DNS? Right. This is one reason that like, makes sense, right? People can't remember IP. These are numbers and people are very hard you know, they find very difficult to remember IP address like it's like imagine having to remember the IP addresses for every website that you have to remember. You might you might get lucky and remember, but IP addresses change all the time. And what happened if I want to do a load balancing and I and I have seven servers and I have seven IP addresses, do I use it? You expect users to remember all these seven IP addresses for each server, for each domain? It just doesn't make sense, right? So something had to be done. So people invented this concept of a domain, which is a string that he type in and it has a subdomain and the domain self and then a top level domain. So w w w dot Jose Nassau dot com. And it's very critical these three sections, the reason that these were broken into three sections is because it's a good idea to actually add it so I can explain it. So this is a subdomain that actual domain and then the top level domain more or less everything is like this. You can see another subdomain here which which gives you the same exact identical thing essentially. But these layers if well, they're very critical to the building of the DNS as well. So this is the top level domain dot com dot org, Darby's dot whatever, dot engineering. This is the actual domain and this is like subdomains. So people remember this easily so people can just type in here and also dot com boom, go to my website. Right. Also, the beauty of this is like it's just a text really that points to one IP or key B pointing to multiple IPS. Not only that, you can add more data to these queries. It's like, give me your mail IP address, give me your website, mail address, give me any information about yourself, like a text information, give me your service record. What port should I connect to you? So many valuable information can be retrieved from the DNS, not just the IP address, right? So this additional layer of abstraction is actually fantastic. You might say why this is so I remember. Not really because IP addresses you guys change, right? Can change. And if the IP address change, how can you expect that the user keeps track of all these IP addresses? So having a layer of abstraction like this where I hey, it's always going to be the same, but the IP that points to it can change and we're going to take care of that in the back backend to flip and swizzle these IP addresses to give you any IP address. The beauty of this is you can do something like the load balancing, right? So that if I. Aquarius and also Tor.com. I can give five IP addresses and it's up to me to round robin through all of these IP addresses. All of them point to my website. That's pretty cool, right? This way you can connect to the first one, right? And then have another client connection to the second one. A third one and the fourth one. This way you can balance between multiple servers. Pretty neat. This is what this concept of load balancing is really powerful when it comes to DNS and Netflix heavily relying on that, at least in their first architecture. When they first built Netflix, right. They rely on this concept of DNS to give you multiple IP addresses and client low client side load balancer. This is not server side. This is the client chooses what server to connect to. Pretty cool idea. Another concept is the IP addresses. As we said, it's always changes and if it changes then we don't really have to worry about it. We're connecting to the same server all the time. Another cool server concept is the geo DNS concept. Cloudflare. See any CDNs fastly. Cloudflare. If you choose to host your website on Fastly or Cloudflare, they live in the entire world. They have servers in India, they have servers in Australia, they have servers in the US, they have server in Bahrain, they have servers in Russia. So their content lives there, they will they will replicate your content everywhere. So now if you go to your website, let's say Hossein Nasr dot com. And you are a client from India. The DNS resolution in India will give you an IP address that is in India of Hussain also dot com. This way you are immediately connected to the closest server. You might say why care so close the server? What does that mean? Well, the distance really matters because it means lower latency, you know, because that means the establishment of the TCP IP handshake, the segment that says TCP, uh, you know, send syn ack ack and then the TLS and all these, you know, the latencies, every segment has to be acknowledged. And then we have to run through the congestion algorithm and all that stuff. This is way much better if my server is closer to me. It's way better if if the latency is 5 milliseconds compared to if my server is I am in India and my server is in California and the latency is 50 or 60 or 70 milliseconds, it adds up. So if I can be served an IP address that is next to me based on the DNS, that is genius, right? And that's that's part of the DNS. The DNS does that automatically. Cloudflare or whoever Europe is cloud the DNS recursive or resolver does that in a in a very intelligent manner. They know you obviously your IP address and as a result, they serve you the closest IP address. You know, they have IP addresses of that for that domain everywhere. But they serve you exactly what you want. Obviously, some work needs to be done there. I'm going to just have beautiful now because there are going to be a lot of. Graphics here. All right. So every time we introduced a new addressing system, you guys, we need a mapping. And that's a problem, kind of. We've seen it with AAP, right? Address resolution protocol. I talked about that at the beginning of the course where we have the MAC address. That's the first thing that was invented. We never thought that we're going to be doing routing. We always thought that everything was going to be close to each other. Every computer is next to each other. And we invented the Mac, which is the the media access control address, which is global. It's unique. No two MAC addresses can have the same Mac, right? No, no. Next can have the same Mac. Let's use let's always use that. The problem is that we can't because we need to root and Mac. Our Mac Macs are not suitable through networks. That's why we invented IP. But guess what? We still need the Mac. So if I have the IP and I don't have the Mac, I'm I'm screwed in a local network. So we need to do an address resolution protocol. Hey, I know your IP address, but I need your Mac. So we ask the network and it gives us the Mac and. And we talked about that in details. Now it's the identical problem. You have the name because every customer out there knows Facebook.com, but almost no one knows the actual IP address of Facebook. All right. So same problem. We have the name. You're going to need the IP. You need a mapping. DNS is the mapping. We use the DNS to map from the name to the IP and then we turn around if we are on the local network from the IP to the Mac. Right. Always goes back to the Mac at the local area, obviously. It is built on top of UDP. Really cool protocol. It shows up right from here. Now everything that we talk about is either built on top of UDP is either built on top of TCP. Right. And sometimes rarely it's directly built on top of IP, like voice over IP. It's built into directly on top of IP. Right. Or ICMP. It's built directly on top of IP. Right. So that's what I want. That's why this course is very important to me, because once you understand that everything is UDP or TCP, everything becomes so clear. It's like something that gets an understanding that started to break down and a huge progress can be made. Now it's on Port 53, so this is a reserved port for DNS. It's well known that Port 53 is DNS. Does it have to be? No. You can spin up a DNS server that listens to pretty much any port you want, but the fact that we have fifth port, that means we have to have like we are in layer four effectively, right. Which is the UDP protocol and it supports many records, you know, not just the IP, the A protocol is what gives you the IP, but the C name gives you an alias, a canonical name. Right. So my for example, in my courses, I always have an alias. So a network host NASSCOM points to nullify servers which then directs you to Udemy with with the coupon applied database. Then also the com is actually a C name. Right. So nothing if I connect to my authoritative server, we're going to talk about that. So when you go to network dot Hussain also dot com, the authoritative name server which is Google in this my case will get you back the alias another hostname that actually lives in a nutshell. If I forgot what it is, it was like something something dot net dot com. And there's one where my website hosted and on that website I have a direction that directs you back to Udemy so you can get the My Udemy courses. I have all of this configured with C names database dot hosain also dot com is a C name a network that has an answer to com python dot Thursday and also dot com and the same thing text m for the mail text for text information anything you can put literally anything here but it's I believe it's used for something else as well. So Google.com points to an IP address and then we can do our resolutions and we can connect to the server once we have the IP address. So here is how DNS works, you guys. So you're going to see it's not as simple as quitting a database and getting back a name, an IP address, because that's how we we would have built it, right, as a as engineers, right. Because yeah, this is a networking course, but all of us here are engineers. Like I'm a I'm a software engineer, so I build software and this is how we're going to build it. And. Query. Build an index. But can you imagine how big that will happen? Like is going to be a huge database, a huge table with so many records. So you get a partitioning. That's that's the technique we I talk about in my database course. Right. You're going to partition the database so that you have smaller and smaller data sets that you can search. And this is exactly how DNS work. It starts with the root that is will divide servers, right? And then it gives you the top level domain servers, which are the dot com servers, the dot org servers, that dot in generic servers. And each one of these, obviously, they are duplicated and replicated. Right? Each one of these top level domains have entries of the authoritative name servers as well, which those authoritarian servers have the individual IP addresses that you actually want. So the resolver is effectively asking many questions to get that final answer. So let's go through the layers here. The DNS resolver, which is what is configured when you connect to any network, it will give you a default DNS resolver. Usually this is your router, but you can override that 1111 is a popular resolver. Eight, eight, eight, eight is a popular resolver. Eight, eight, eight, eight is Google and 1111 is Cloudflare, a very popular resolvers. You know, they have caches everywhere. Right. And those resolvers, most of the time have the answers, but sometimes they don't. So if they don't, which is what we interested here, what do they do in order to get the IP address of the server that is requested? They go to the root server and the root server will give them not the answer. No, there was never will give them someone who will know the answer, which is the top level domain. And that's why you need the dot com. Right. If you give me this just Jose and also the root server will not know anything. It's like asking the root server give me the dot Jose Nassau top, top level domain. That's what you you're asking about, right? Effectively. All right. So so once we get the top level domain, then we ask the top level domain server of the dot com or the dot org or the dot engineering or the dot IO. Then get me the authoritative name server that might have the answer effectively. And this is your registrar. This is where you registered your domain effectively. You can. Spin up your own authoritative AMP server, and a lot of attackers actually do that and do very nasty things with this. If they if they spin up their own authoritarian server. Anyone who does a DNS asking for their domain, they will land. The client will land. It's almost like a honeypot. They will come to them, you know, and attackers do all sorts of nasty things to sneak in data and stuff like that. We talked about it in my YouTube channel. So many, so many attacks happen like that. So actually once you have the voting server gives you the IP address, then you connect to the actual server and connect. Let's see. Actually, let's see this in action. I want to go to Google.com. This is the actual IP address of Google dotcom. I do not know it. As a client I ask a question. What is the IP that is of google dot com the resolver, you know, what does that mean? Let's go back. Let's let's expand that the resolver here. How did I know my resolver first? When you connect to a network, you will know your DNS resolver immediately. There is a bit that tells you here is your DNS server, one primary or secondary, it will connect to the primary. And this is usually not a domain. This is actually a hardcoded IP. It has to be. If it's not an IP address, then you're going to get into a loop, right? It has to be an IP address. Like that's why most DNS resolvers are not actually hostnames. There has to be domains otherwise. Okay, how do I look you up? Right. What's your IP address? So now you get the resolver and then you send a UDP packet and we're not going to get into the actual details of how does it look like? We're going to show I'm going to show a diagram, but I'm not going to go into details because, again, this this deserves its own course, to be honest. The DNS is so much that is so much there and I can't possibly cover all that. So now send our UDP packet to the destination is the 53 port. Right. Obviously, and the destination is the IP address of the resolver. And then there is also a check. Do I have a cached entry? Did anyone asked for the IP address of Google before and I did all this work before? Nope, I did not. Let me do it. Please. So blocks the client is blocked. So this is a synchronous call. That is synchronous call. So if you're a frontend or backend engineer, you know what this means, right? It means you're blocked, you're waiting. The resolver now turns around and and checks. OC What do you want. Google.com. Okay, I need so that means I need the dot com drop top level domain. Who does know of any top level domain of a dot com there are. This. Now we're interested in just the dot com. There are thousands of servers that host the dot coms, right? Who knows? A router server might know. And these are well known hardcoded route servers. It's very well known. And if it is over, know them. You can connect to them today if you want. All right. So the resolver asks the question, Hey, root, one of the root servers that are managed, I believe there are 13 or 12 out there, 13 actual individual root. But they are replicated, you know, everywhere, obviously. What is the dot com server? That's the second question here. Give me a dot com server. Give me any dot com server. That means give me a top level domain that hosts dot com entries. And okay, here's one TLD one. This is a dot com server. So now number three, the resolver will turn it out and connect to the top level domain one, which is, again, these are all IP addresses. There's no hostnames here. All right. So now that always we're going to get into our loop, right? Because recursive recursive right now, we get that top level domain here. Go and ask another question. Hey, top level domain one. I know you are a dot com server, so you must know this question. Where is the authoritative name server of google dot com? See, I did not ask the IP address of google. Com. Give me someone who will know why. Because if you started putting the IP addresses of everything here, then imagine top level demands are replicated everywhere. So you're going to duplicate this IP address everywhere for every TLD, for every dot com server that exists. And that is a lot of work, right? So no, we don't do this. And instead we save one IP or maybe multiple IP address of something called the authoritative name servers. Servers that absolutely no. The answer for Google dot com and they are dedicated just for google.com. So now you get a sense authoritative name server one and this is the answer number five. And then you you connect to that server as no connection in UDP, you just send that request. What is the IP address of google.com? The IP address is levs right here in this server. So this. Now you minimize the search space again we are talking database here. I have spade now. Now. And instead of searching in a huge area, we minimize the search space to only one server in this particular case google.com. The authorities name server since it's only responsible for google dot com. Their database is so tiny. Right. Maybe it will be responsible for managing multiple domains, but how many? Right. It's not going to be that big. So Google does come. Oh, sure. Boop. Here's the IP address of google.com. So did you see seven trips? Right. This is called the question and UDP and DNS. Sorry, a question. And this is called an answer or a query or a response. You might say hosain. UDP is stateless. How do I know that this response belongs to this request? That's a fantastic question. There is a header in the DNS in the UDP packet in the data section, the DNS add something to a query ID or the transaction ID. So you're going to generate a transaction ID and this transaction will be it will be replicated. It will be used through all over this. So if you ask a query and you give a query ID, this server will probably with the query ID that you just reply. So that means if the resolver is asking many questions in parallel, it knows which response belong to which request. Right. This is how you do mapping effectively. And now finally, number eight, we gives you the client and then number nine, establish a TCP connection handshake with Google.com, that particular IP address. That is a lot of work for Dennis. That's why. Dennis in at least Norges, if you noticed, it's done asynchronously. Because of all this, there is cost to do this and you do do this all the time. If you do an Axios request, if you do a fetch request by default, you're going to do a DNS. The underlining libraries. Does the DNS do that? The DNS for you? Right. And as a result, this can be expensive. So Node.js is specifically used something called Lib. You've got the library to do asynchronous calls for all the DNS queries because of what we're seeing right here, because that could be blocking. Right. And we don't want to be blocked in this particular case. Here's how the DNS packet looks like. Again, we're familiar with this part. That's the IP, right? And I took this from the use nex paper here. Here's our references and this is also the RFC of DNS. If you want to know more about this, this IP, we talked in details about all of that stuff, fragmentation or the enemy of DNS, by the way, IP fragmentation. We talked about that. Right. And we're going to talk about it in the in the next section, actually talk about IP fragmentations and how it's nasty. Actually, this is the UDP header destination port 53, source port, whatever is going to be random and checksum and the length and this is actually the data portion of the UDP and that's the DNS header. That transaction idea that we talked about have to be copied with every request and response. The operation codes, there are flags which tells you this is authoritative, this is not this. This is the number of questions, number of answers, you know, the number of answers, the number of name servers, the number of additional responses, because this is not the entire header, this is just the header. There is a data portion that is very large for the DNS and this is what can get variable can get really, really large the DNS effectively the size of the DNS because this is because you can ask multiple questions. Right. And and you can get multiple answers as we talked about. Right. You can have multiple IP addresses. And this specifies like what do you want exactly? You want the A record, you want the C name, blah, blah, blah. Here are some notes about Dennis. Why do we have so many layers? We talked about that kind of thing. Why do we have a top level demand and then authoritative? Because this is so expensive. This is a brilliant distributed design. It's a decentralized in a way. You can argue about the decentralization. We can show myself now and you can see my hand faces here. Yeah, yeah, yeah. You can see me. Yeah. So you can argue about the decentralization of DNS because we've seen Microsoft, Google, Facebook actually going down during the pandemic back in 2020 because of DNS. If you say this is decentralized, then this kind of it's like how if it's decentralized, then how is it going down? It's decentralized. Well, it is decentralized when it comes to the route, when it comes to the top level domain. But it's completely centralized in the authoritative name server because at the end of the day, the authoritative name server has the answer. If the authoritative name servers are down, which what happened to Microsoft back in the days to two years ago, if those are down, then you can't get a DNS answer because only those only Microsoft knows the answer eventually because they host the authoritative name server in their data center. Those are down. Everything shut down, basically. So you can argue about DNS being centralized, decentralized, actually not really true unless you decentralize your authoritative name servers. So we talked about so many layers because of we want it to be decentralized as much as possible and we want it to be distributed. We want to partition the the namespace so that we don't really get slow queries. If you talk my database, of course I talk about this in details. You know, if you want to work with a billion row table, the best way to work with a billion row table is to avoid working with a billion row table. That is the advice for every database engineering. You know, avoid working with billion rows. If you want to search for one record in a billion rows, avoid work searching 1 billion rows. That's how indexing work. That's why we have a partitioning. That's why we do sharding. That's why we break things down as much as possible into smaller and smaller and smaller and more data files. That's why we have file systems of your file groups. They are falling out of favors, but still you can still do a lot of other stuff. DNS is not encrypted by default. UDP you send it to UDP request. The DNS is plaintext. Everybody can see it. Everybody can change it. Right. As a result. Not everybody can change that. I can't take that back. I think that back your ISP. But is a router, right. So all IP packets go through your ISP. We talked about that. Right. And because IP packets carry UDP packets and because UDP carried DNS and because the NSA is not encrypted, your ISP sees every single domain you visit. They do not see the content of the website you fetch because most websites are encrypted with GBS, but they absolutely see the UDP uncorrupted DNS. And if you go to port 53 especially so they know everything. So they can log, they can block you if they want. Very easily. People start to solve this problem with Dotti and Deutsch and other older technology like DNS SEQ. So DNS over TLS and DNS over http s and I'm flipping back and forth which one is best? You know, DNS over GBS means we're going to use port four, four, three and we're going to send DNS queries as normal HTTP requests. This means network engineers and firewall admins. They cannot no longer differentiate between DNS queries. Right? Anymore because we always used to look for Port 53 to know 53 them is Dennis. You cannot do that anymore if you interrupted with Dodge. That's why a lot of people favor especially network engineers are pushing hard on dot specific port just for encrypted DNS over directly over TLS right so that means I believe that would be either D TLS, which is the datagram TLS right or TCP. So that means still going to be UDP, but it's going to be encrypted, but there will be assigned port specific port just for that. So there is a lot of talk here and there is no time to explain all of that stuff, to be honest. But it's very it is still a research topic. People are still researching this. This is not implemented fully. Firefox, I believe, has implemented it. Then they took it out. So many servers are supported. Cloudflare started to support the edge. But yeah, it's it's like I would say maybe 20% implemented in the web right now. And you can if you want to, you can enable them. There are many, many, many attacks against DNS and this is the entire course. Just talk about DNS, to be honest. So that is DNS hijacking, you know, which is basically injecting yourself as a. As a fake authoritative name server. Right. As a result. And and in this case, if you injected yourself as a fake top level domain or as a fake name server, you can get you can start getting requests effectively and responds with fake responses to malicious servers. Right. And this is one of the worst DNS cache poisoning where an attacker will just because they're using the transaction ID, the query ID. You can see that idea, right? You can you can immediately see that query idea. So you can guess you can response with the query ID because you know it. Right? Uh, so in this particular case, if the resolver is talking to the top level domain or the the authoritative server, you can inject and start just spamming UDP package with destination port that the server had and. Transaction ID. Random transaction IDs and then a fixed IP address that is basically your attack, your attackers malicious hosted server. So if you got a hit, so if you got the port, right, right. Again, you must say port is 53. No, in the word the source port is random. Remember, guys, the client becomes a resolver and they will generate a random port, which you don't know necessarily. Right. So in this case, if you start responding with transaction IDs and you manage to beat the server, then you can poison the cache of the resolver. Very nasty attack. Very hard to do, though, so. How about we go into an example, you guys, let's actually use GNSS lookup, which is a very powerful tool. There's another tool called Digg that allows you to do this. Let's just let's just work with this stuff. Let's jump into it. So how about we get started? So in this lookup stands for name server lookup and it's available on all all machines, all operating systems. If I'm not mistaken, I'm using a mac, so you should have that as well. If you're using Linux or Windows, it's right there and it takes two parameters. First, it takes actually three takes a bunch of options and take it takes the domain that you want to look up the IP address of. Right. So you ask, hey, what is the IP address? Was it not going to give you that IP address back? That's that's what it does. Right. But also takes another third parameter, which is which server do you want to query? Very powerful and very beautiful. Third parameter here we're going to use in a minute. So if I do nice lookup was and also lookup these are the four IP addresses that are returned all the A records that are returned from my DNS query. And if you notice this was the server, the DNS server that was used, this is my router, my router is a DNS server and I used Cloudflare configure Cloudflare for that. So my, my router is a resolver in this case and all my DNS queries goes to my router first. So these are the answers I got, I got for IP address because I'm running on top of Google. My website is running on top of Google. So they're doing good load balancing there. But you know, it's not only that, you know. This is not the only thing you can ask from the server. You can ask for all sorts of domains, like you can ask for options and say, Hey, I want the text records. These are notes, if you will, you know, that that are stored in the records itself. If I do say so dot com. You can see that these are the notes that are stored. I didn't I didn't do that. This is Google did that for me. You know, they stored some sort of a key, the verification. And this is a proof on how what is that? This is actually my site. You can ask for all sorts of other records as well. Like a few as ask a that's the default, right? That's what you get. So these are some of the records that you can return. But, you know, what is this? None an authoritative answer. This means that this IP address was returned from a cached resolver. As I explained in my previous video, that means this is not really the actual name server answer. So that's why it's called non authoritative. Well, what if I don't trust this answer? No. I don't want to really be secure. Well, you can do this. And let's look up dash type equal ness and you can ask when you do the query, you can ask for the name servers for the authoritative name servers to be returned. And these are my authoritative name server DNS for I have like five D and authoritative name server I, my server is running on top. My registrar is Enum, which went down a few months back. So now here's what it comes power for. Let's copy one of those puppis and I can now do a nice lookup. I do also dot com. But then I add another parameter. That third parameter is which resolver you want to query. And in this case, I want to query this particular one. You can put any DNS server here and I will answer you. But because this is now the authoritative, you see that I don't get that text anymore that says, Hey, this is not authoritative. And you see that my cursor, my local router or cursor was bypassed. I have bypassed everything and went directly to the source. So that is as authoritative as it can get. Very powerful stuff. I can also do Husain also dot com. And I can ask a query from any particular resolver. I can ask Google. Eight, eight, eight, eight. Give me an answer for that. So this way that query, the UDP packet will go to eight, eight, eight, eight. In order to answer that query. So you get that back the answer. Obviously it is not authoritative. That's why you get that message. In the future, we going to talk about, you know, a more detailed tool that's called DIC Domain Information Group or does almost the same job with more details, you know, just queries. Any records and show you pretty much everything if you want. So this is the same record, same query against using deg. You know so we can do dig saying also dot com and I ask for I don't know text record. So you can get the text records this way. You can ask for Amex records, mail documents, the mail, the email servers. You can ask for SVC any service request. There isn't any, but there is an also a it's the root service. Very useful tools for DNS querying. I'm going to see you in the next one. You guys say awesome. Goodbye.


### TLS vtt

Transport layer, security or TLS is one of the basic, fundamental way to encrypt communication between any two party. We have to build a standard for encryption because we can't send just IP packets with data that is plain, you know, text that everybody in the crowd can read. So we needed to encrypt it and you can encrypt it anywhere you want, right? But we need a standard. The standards are good. Yeah. So this was invented. So this is the topic of today's lecture. And obviously I can't go into much details because TLS is is really rich. There is so much history when it comes to TLS and I think it deserves its own course to be on. So maybe I'll do it from the future, but it's worth maybe 30, 30 minutes discussion and going through that, maybe less. Let's find out. So in this luck, so we're going to discuss a TLS through a HTTP while you can build TLS on. Literally any protocol, anything that comes from layer seven down to the TLS layer can be encrypted. And it's arguable that TLS which layer sits on, I believe the best way to if you want to shove it into an OCI model, it can be layer five because it's stateful, because it has state left and right and right with file descriptors of the TCP handshake. Right? So while DCB is layer four, the session variables and the window sizes and that we talked about lives in session five in the session layer which is layer five. Right. And that's kind of fits there because it has some sort of a session. Right, a session that is agreed upon. So that's if you want to fit it again, this is not written on a stone like, oh, it has to be layer five. It's all, you know, intellectual, if you will. Then we'll go from. I'm going to talk about what Vanilla HTP looks like without encryption. Now I'm going to talk about HTTP s, right, which is basically HTTP over TLS. If you want to break it down, then we're going to talk about TLS 1.2 handshake, how it actually works on 1.2. You might say the same about the one version one version one one. What happened to those? Those are unsecure and no longer used. They are no longer recommended. So I thought I thought maybe let's just talk about what is actually popular and then I'm going to talk about Diffie Hellman, which is a special key exchange algorithm that actually found a lot of flaws at 1.2, 1.2 was improved to support the fee element, but it still supports some other, if you will, unsecure or not really. Yeah. To the top of the chain forward secrecy kind of a thing. But this actually is one of the best algorithms to exchange keys. And then we're going to talk about how to also monitor is currently the best candidate for everything you know Walt there's 1.2 is still used actually for backward compatibility, things like that. So how HTTP works, here's a server listening on pause 80. It is a client and this indicates the TCP handshake that we talked about. So this is where we open a connection. We opened a TCP connection. So we have established the sense and accent, right? And then we have an open connection and here's where we actually close it. So before we close the connection, we want to send a request. And this request looks like this, right? Hey, get slash and http one one or two. Right, over, over or the version is then you put the headers and then the headers on the body. If you have a body get usually don't have bodies. So send that that gets that moves into an TCP segment and that or more than one TCP segment based on the MTU, obviously. And then we will move down the stack and then move that into an IP packet. And we talked about all of that. The segment is received here, the request is understood at the application layer and the application will just respond back with the headers, more response headers, index.html, whatever it is, right content effectively the body and this becomes one or more segments and it depends on the algorithm of the TCP whether we should send all these segments at once or one by one based on how the congestion control is at that level. Usually when we start it's slow, right? That's why we talked about we're going to talk about slow start. I don't know where this lecture is going to be. Right. So that's why it depends like where is going to set. I think we already talked. We're going to talk about that slow start later. But we did mention it definitely. So that's it. So TBS, what do we do? Right. Whatever we explain before this is unencrypted. So anyone in the middle, right? Like a router, like an ISP where all the IP packets will go through. We'll see this request. Not nice, right? We need to encrypt them. So the same thing. We open a connection. And we do a handshake first. And the goal of this handshake is to share a symmetric key share encryption key. The same key should exist on both the client and the server. And once we have these keys, we're going to use them to encrypt our get request effectively. And this is the critical part here. How can we get the same key as the symmetric encryption algorithm, right? Where at the same key is encrypted, the same key decrypts, and nobody in the middle will have this should have this key. Nobody should. So it will be encrypted here. Nobody in the middle will understand what it is. The server will get that encrypted request and that will decrypt it with the key and then we send it back. The question is, how do we actually exchange that key? Is the. Is the key. No pun intended. We encrypt with symmetric key algorithm. So my question is like, well, I'm saying when we talk some security courses back in uni, but what does does it have to be like at the same key? Do I have to encrypt with the same key? Not really. There are symmetric key algorithms and there is asymmetric key algorithm, but symmetric key algorithms are way much faster than asymmetric because symmetric key is usually use XOR and it uses the key to x, all the content which is extremely fast because you're exploring blocks of content based on whether your symmetric algorithm is block based or not. And then you're going to get an encrypted content. And if nobody has the key, nobody can actually decrypt it, right? And that's the goal of this. So it's very fast. Asymmetric is where you have a key. That encrypts. And another key that actually decrypts, right. You cannot use the same key that encrypts to decrypt all. You cannot use the key that decrypts to encrypt. Does that make sense? And this is also popular, but it's very slow because it always relies on exponential stuff. So anyway, anyway, we have exponential is going to be slower to compute. Right? And you need more CPU, more processing, more energy. Yeah. So that's why we always use symmetric key. So if anybody asks. But the problem with symmetric key is we're going to share the key because you cannot just send the key across the board because anyone can grab it. Right. And that's the really the gist of the situation is how to exchange the key without getting caught. Right. And also the goal of the DLC is also to authenticate the server and do other stuff as well. Like, hey, I want to make sure that you are who you say, who you are and the server to do that. They send you back a certificate and the certificate has always rely on this p API, the public key infrastructure. It's like asymmetric key algorithms where you can you have the public key make sure that it's actually me that signed the certificate and it will go back to check if the certificate authority is actually who signed this key and they will verify the certificate authority goes back up to the root. And this is not our topic, but that's just a general overview of authentication extensions during the TLS client. Hello, which is the key exchange. We talk we have so many extensions packed in this client ALO. We have SNI server name and indication we can't talk about. That is going to take a long time but rather not so pre pre shared key. So where hey I already establish a connection prior right with the server. So here is the pre shared key. So I'm going to encrypt my encryption. My my, my, my communication from from this first step there is no round back and forth. So this is called zero RTT. So there is no roundtrips at all. So extension is very it's another thing as well. All right. Let's talk about the last 1.2 again, guys, I'm going to avoid the math here. First of all, I'm not that great in math. So I kind of I think I think it's really irrelevant for us network engineers, but I'll mention it for those interested, obviously. So then at one point too, you open a connection, you close it. This is against the HTTPS connection. But I want you to pay attention to here. We're going to use RSA, one of the most popular asymmetric key encryption algorithm. If you are in RSA, you have a public key and you have a private key, right? A private key is not never shared, but the public key is can be shared effectively. And that's where the certificate actually contain the public key of the server. So with that in mind. What the what the server will do after opening the connection, the TCP connection immediately. The first thing we can get is to do a TLS client. Hello. And in the last client. Hello. We're going to request an encryption. Request. So, hey, I want to I want to encrypt. That's it. Says, Hey, I want to encrypt. And I'm going to use let's use RSA as a key exchange algorithm and let's use a as as a symmetric key algorithm or Cha-Cha as a symmetric key because there are two algorithms, one for to exchange the key, as we talked about, and one for the actual encryption, which is supposed to be symmetric, which is faster. So now the server will reply back is like, okay, here's, here's my cert and here's my public key client generate for me a pre master. Or you can think of it as just generate your symmetric key and send it to me. Like I was saying, you cannot just send it across the wall. Well, you can. We're going to show how. And so what the client will do is we'll generate this yellow key, right, which is the golden key, which is the symmetric key, which we want the client and the server to have. Right. So the client should have it here. Maybe we can fix that. So the client will generate the symmetric key and then we'll take this service public key, encrypt that symmetric key. It's actually not the actual symmetric key. It's a pre master, right? It's the input to generate the symmetric key but might as well it's this is just explanation purposes. So we're going to take that and then encrypt it and then send it across the wire. So anyone in the middle, if they get this, they will not understand anything. They will. L Sure they're going to see an encrypted symmetric key, what is supposed to be because we we have a label for this TLS handshake that's called change cipher -- and finish. I'm done here. So there's one roundtrip and then the second one is where we finish here. But we get to if somebody snipped here and got this key, they can't decrypt it because they don't have the corresponding private keys. So the server will get this and they will use the private key to decrypt that content. And as a result, how did you like that information? Huh? That's awesome. The the private key will be used to decrypt that content and then they the server will get the private key as a result, the symmetric key. So now both of them have to have it and nobody else in the middle can actually get it. And then this was a Helga, I got it done and then the encryption can happen using this symmetric key, right? Again, we don't want to use the public key because we could do this right? We could. We could. We could kill this thing, this yellow key, this golden key. And we to let's just use all of the public key for every content that's encrypted. That's the client, the encrypted with the public key. And then this the server will decrypted with the private key that is so slow, especially for a large pool, you're sending out a large 16 K JavaScript file that's going to be so slow. You don't want to do that, right? Yeah, we do it for Tiny Data. That's why I said you get away with it. It's a touch. I believe it uses private public key encryption, but. But it can because we're sending commands. Right. And this is not like sending huge data right now. Let's see what transferring files that's can't get slower. But sure, but yeah, the encryption can happen now. No problem. It was the problem with this right before we go to the phenomenon. Well, the problem with this is. Something called a. Forward secrecy or perfect forward secrecy. And let's assume I'm an attacker that happened to be in the middle. I am an ISP, right? I like to use the ISP because ISP all the IP packets passes through the ISP. Right. So ISP sees everything. Yes. Encrypted, but the ISP right. Can record everything. Let's say I'm recording all this configuration, I'm saving it to disk and I have a terabytes worth of content communication between all clients and the server. I have all the communication recorded for the past year to say Who cares? Sure, you can have it, but you can't see anything. You can. You can. You can run all the quantum computer against it to break it, but you can't. It's going to take you years to break it. Millions of years. Hmm. What if I get the private key of the server? How? You can't just get it right. You're an ISP, you don't have access to the server. It happened once. There was something called Heartbleed Openness Bug, which is the this is the library that people use to do all this mumbo jumbo, this TLS and encryption because you can't write this code for yourself, you can try it. It's so complex, you know, but people use libraries and that library had a bug and that bug was. While able people were able to extract where the private key was loaded into the server memory and people were able to use certain crash open SSL and leak part of the memory from the server and part of the memory. There's going to be a lot of junk, but part of that junk is gold. What is that gold? It's a private key. So people managed to get the private key of servers. Right. I just realized that I'm covering the page. I'm not really covering anything. That's good. Yeah. So people was able to get the privacy. At the moment, the ISP can get the key. What can they do? I'm not saying that I speech are bad because I'm just giving an example of you. Do you understand? Because of this perfect example, someone in Starbucks happened to hijack the router, right? And all the packets will go through that outer, right. All the package that goes to the Internet will go to that outer. Another example. But yeah, once we have that private key, I'll do a loop and exchange every single, you know, decrypt all this thing. Because now I have the private key I'm going to look for for each change cipher call, go decrypt it, get the cemetery key, and then decrypt the communication. Because even if the symmetric key is different right for each session. I don't care. The private key is the same. So I will get the symmetric key for each session and I'll decrypt it. That was really bad. So people try to avoid as much as possible using RSA as a key exchange algorithm because of this, because it lacks forward secrecy, because in the future someone might go and go back to a recorded conversation and if the private key is leaked, that could be a problem. And that's basically why people don't use the RSA. Now, people also try to use make sure the private key is as short as possible. That means whatever whoever generates the certificate should be as short as three months. Or sometimes Cloudflare gives like two weeks to get more security. So even if someone did hack it, they only have two weeks worth of things to steal. But yeah. So that's it. So made diffi helmet, which is a better key exchange algorithm. Right. And I started giving the math portion of here, but the key here. For Diffi Hellman is to generate a symmetric key. Right. You need two private keys. You generate a private number. A key is a number. Computers deal with number. Right? So we generate two private numbers and one can be public. If you combine the three together, you get the golden key, this private key. Has to be private. This private key has to be private. This can be public. So what you do is like you make one party, the client generates this private key, let's call it X, and the server generates the other private key, which is why. Right. And they keep it next to each other. Right. But the public key, which is a public number, it's actually two numbers that can be all that can be shared. That's fine. And that's the that's the key here. Now, if you happen once, all the three are together, you get the key. All right, that's. That's not really enough. Look. Put it into perspective. Here's another development thing here. If you combine the private key, one of the private key, the blue key with the public key, this is an unbreakable thing. Nobody can break it. So you can and this can be public. You can share this fine. And nobody can get the private key if you combine them together. It's like it's like you merge something and that's it. You cannot merge it. Same thing with this key. And that might get a little confusing. But here's the math here. We can explain it. So the public key is G, right? Right. The public number. So you you basically raise G to the power X again, exponentials are slow. That's why development is can be slow for large data. So G. It's recommended to be a prime and has to be a prime has to be right. And then due to the power x, you you raise it to the power of that private key and then you do a modular in and it has to be a large number and it has to be a prime to get an actual value of it. This is the unbreakable thing. Once you get this, you cannot get X out of it. You cannot do a log. You know, you can do all the math you want, you can do a log G to the power g, but you cannot because you did a modular on it. It's gone, right. It's very hard to break this without spending a lot of compute. Compute. This is the same thing here. G to the y modular end can be broken, but it is the key. If you merge all the three, then you get the golden. How power of the math. Take this thing due to the power x module in. If you raise the whole thing to the power y, let's assume you have y you you are the server. The client can send you this portion. Nobody can sniff it, they can't sniff it. They cannot extract anything to the power. Y and because of math. G to the power X to the power y is equal g to the bar x, y. Right. And this is actually the symmetric key, right? It's the input to the symmetry key algorithm that generates the symmetric key. Same thing g to the power roy modular. N All raised the power. X You put that and then you multiply x and y and that's the key. That's, that's the math behind it if you're interested. TLS 1.3 again, etc. 1.3 but at 1.2 has an option to use Diffie Element or even better elliptic curve development, which is an even better algorithm than development write development developments that secure can be broken with certain. It depends on the key size as well. So now what do we do? We combine the keys, as we said, right? The client generates the private key, the blue key, and the generates also the public key, which is the the G and the N, right. It combines them with which the raises to the power you take G to a power x and then modulo n, and then you send that nobody can they can take this. Sure they can know that g and even that is not enough for them. Right. They can they can read this, but they cannot understand they cannot get the blue right again once they get the X that that could get the X that can decrypt it, but they can't. So the server will receive this part. Okay. And then. Well the survey will do is will generate its own private part of the diffi element, which is the Y as we talked about. Right. And then take this and combine it with the three. What do you get which basically take this and raise it to the power of Y and you get a beautiful symmetric key. Now we got the symmetry key. What does the client need to generate its own symmetric key? It needs this, right. But you cannot just send that plane, right. What you can do instead is you take this and combine it with the red key, which you have the server got from here. Now this is combined. Nobody can break it. So that's g to the bottom of y modulo n and then you send that the server will get this, the client will get this, and then the client will take this part and raise it to the part of what? To X? And that basically will give you the private key. No, the symmetric is sorry. And then you basically do the exchange, you encrypt it and the server can decrypt with the same key. And we're done. And that is the algorithm again. Right? So g to the power x module in the power of y that is done by the server because the server has the Y right and this is unbreakable g to double Y to double in power x is done by the client because the client has the x. Got it. Got it. Again, I'm covering part of it, but it's just the server. It's not not a big deal. And now I think I can I can disappear. All right. So let's summarize. We talked about the vanilla TV. Right how it works. We talked about TPS. We talk about TLS 1.2, how it is actually two roundtrips. Right. Because the first answer is just to agree on what what are we going to use? Hey, what key exchange you got it going to use and what symmetric key are going to use to address 1.3? Just remove that says hey, it's always better to use the Fidelman right or elliptic of the development. And in the first handshake and in the first handshake immediately send these parameters. Don't even don't negotiate. Just just send your parts of the exchange and the server will immediately know that. Right. And so that's the power of development here. You can do it in a single handshake. There is no exchange. The first one was like just like, you know, just negotiation that is just removed, right? So to us, 1.3 is way faster than at 1.2, right? Because there is no external round trip that we have to do. And we know like any roundtrip that TLS 1.3 and 1.2 is content to the TCP layer. Right. And we talked about that. So segments, you better really know how large this things are. Right. And and the payload that comes back from the server, by the way, it has a certificate. This is really large, right? There is a specific ref. See a request for comments to actually compress the certificate which is not today is not compressed. So this is a lot of segments send back. Yeah, I draw one arrow, but there are so many segments here. Right. And we talked about all of that stuff as well. And it's one, right. But it can also be zero with zero RTT. The client can send the the everything including the pre pre shared something that is agreed upon. They can immediately encrypt in the first handshake, right. If they want to, if they have communicated before. So they can agree on some some pre master -- and the server can look it up and use that to to get the key immediately. So it's a little bit stateful but yeah, I guys hope you enjoyed this TLS lecture and let's have any questions you might have, but in the section question Q&amp;A, I try to answer all of these questions and I pick some of these questions to present as lectures which are very, very valuable. See you in the next one. Stay awesome.


## Network Performance


### What is this section vtt

All right, guys, let me talk about this section a little bit. Networking, consent for effective back in applications. Finally, we reach the bridge that we want to cross between the network engineering fundamental responsible concept that we talked about for a few sections now and now the actual practical, you know, linkage by which we end things that we encounter on a daily basis. Performance, latency, effectiveness, all of these things we're going to understand by taking these real life examples, which has some some resemblance of the networking and the back end engineering concept. So while we did mention most of these concepts across the lectures, you know, this is where I try as much as possible to cement these ideas and kind of link them to practical back end applications and front end application. Because those is going to show up because guess what, TCP or UDP are the communication or two sizes, not just one, right? So there are configurations that you can do in the client side that kind of at the back end and vice versa. So this is a very critical section I'm going to talk about in detail, talk about all these concept algorithms that affect and this section is going to continue to grow. These are a few lectures right now, but we're going to continue to grow and add and add more to it. How about we jump into it? Let's jump into it.


### MSS vs MTU vs PMTUD vtt

While we talked about MSA as the maximum segment size, the maximum transmission unit, we really need to talk about another concept, which is the path to you as well. Right. This lecture will I can explain this and kind of cement any ideas you might have, any gaps that you might had. Hopefully this lecture is going to glue everything together. How large the packet can get is the answer here. The maximum thickness size must fit in a single aim to you, but can that grow any larger? So TCP Layer four unit is segment. We don't know this and we've been repeating this across the course, right. The segment slides it into an IP packet, which is layer three. The IP packet now has the segment plus its headers. The IP packet slides into a layer to frame. Right. And that's your Ethernet or Wi-Fi Ethernet, you know, back a frame. And then the frame has a fixed size based on your networking configuration. That's the limitation of the network interface. You know that network card, how much can this handle now? And the frame itself has a size and the size of the frame determines the size of the segment. Actually, we talked about this, right? The size of the frame. Derives everything above it. It's the most critical thing ever, and it's disappearing in 4 seconds. With the hardware empty, you know, I'm I'm the lowest level here. Maximum transmission unit is the size of the frame. We're a layer two here. And you can go. This is my actually my network. And I now see the empty you know, you can you can set that in your router. You can also set them to you if you want to run, but your ISP will only accept certain empty you at the end of the day. But you can change that if you want. Play with that, right? Hopefully you don't you don't terminate your internet during the process. So it is it is a network interface property and it defaults to 1500. Now that's that's why we talked about the Internet and our Internet is just 1500, you know, so don't bother sending anything larger than 1500 in the Internet. You know, you might say, Hussein, I never actually when I write, when I do a curl or a Python request that I know just fetch, I never or Axios, I never actually sit these thing. Yeah, you're right. Because it is decided by the protocol or the API below you write. All of this thing at the end of the day is just writing the segment, right? And writing the segments eventually writes the IP package, which is all code, right at the end of the day. But we also used you guys on using libraries and myself included, right? We barely know anything that is happening behind the scenes, unfortunately. Right. So it's really healthy to go and from time to time to kind of explore like what is going on beneath me? Can I can I actually write an app? Yeah, you can write the SI app that writes an IP packet. That's pretty much it. I don't think you can do that in Node.js. Right. Especially with this idea at all. But yeah, some networks have dump of frames that can go up to 9000 bytes. And the the question is, are there networks with larger arm to use? And we talked about this. Right. Or does Amazon use more than 9000 to use internally? I would imagine, yeah. I wouldn't be I would be surprised because the larger if it's internal and you are talking to each other and you own all the hardware and everything, why not? But I don't know what's the limitation if you go for that and that obviously you are going to need more processing. So you need really beefy network interfaces that handles that, but. The larger the frame. Then the larger the frame. Scott the pace, the larger the frame. Guys. Uh, that lower the latency again because you're going to shove more content in your frame and then you going to send it once and once it reaches. You don't have to send multiple frames anymore, right. Because you see innocent one frame or one large IP packet. Yomi has really delicious, you know, large IP jumble like a hot dog or something. You know, I don't I called Dumbo is like in this big IP packet and that goes you if it reaches in one piece, right. Assuming that and that's one limitation, you don't know if the IP back at that large IP back want arrive uncorrupted if you have hard you know a good transmission and good network with low you know A2 animation then yeah why not. But if if it's the network is unstable, then going larger is probably a bad idea because in this case any bit that is flipped, that IP packet is invalid. Right. And lot the larger it gets anything that get wrong, the whole thing you have to send again. So you get you can argue that the larger the bucket, the worse it can get. Again, this cloud provider probably did the research and and it probably decided on the optimal empty use sizes. And we're here just obviously discussing I never walked in a cloud provider before, so I don't know. But this is just fascinating to me. And they always if if any of you guys are actually working on Amazon or the cloud provider and, you know, these kind of deals and you can share. Sure. They'll be awesome. All right. So I'd be back. Adam, to you. The IBM to you usually equals the hardware them to you. Usually we say it usually and I, I mentioned IBM to you like think, oh, this, this is the IBM Beckett size. If you think about it right. It's usually equal to the hardware you which is the low level in network interface and deal one IP package should and I add this code should fit in a single frame unless you have fragmentation. Right. And then you have fragmentation. All bets are off. Large IP packets will be a fragment and do multiple frames and these frames have to be assembled. And then the frame itself has to have bits that says, Hey, please go and assemble this into a single IP packet, right? And if someone ran into a cover that in some in one or pieces of a news that I did where someone uses that fragmentation to exploit that uses exactly that right to trick the fragmentation. Uh, I don't, I don't quite remember. I'm going to reference the, the article here that I was, if I remember, I'll add a link here to one of the attacks actually that uses fragmentation to do that. And pretty nasty stuff here. But the fragmentation can be really bad here. Maximum segment size, it is determined based on them to you. Obviously the larger you can get is them to you minus obviously the headers and you're going to get no one for 60. Obviously, that's the. Golden standard size. He had one for 60. So now if you think about it, now you send data, right when you want to send actual data for your application. Think about this number. This number is the goal. The number? If you can send something in 140, 60, write one for 60, then that will fit into a single. Segment. All right. Single segment is better than two segments, right? If you can cram everything into one kilobyte, if you can compress locally in the in the client, then and send it in one K, that's good. You know, because you can send one segment instead of three, right? Because three while you can, you can't guarantee that sometimes the three segments can go together concurrently and reach at the same time based on the flow on congestion control. You can guarantee definitely a single segment will definitely be pushed immediately if it's failed, obviously, and you don't have to go to them like Nigel algorithm that we're going to talk about in a minute. So if you're sending 146 60 byte, exactly, that will fit nicely into a single image. So just keep that in mind right now. Hopefully. What are you thinking now as you as you're reading this and as as you're listening to this lecture and the course, what you will be doing is trying to link the work you do with this. And now hopefully after this course, everything will change for you because now you will appreciate the stack that you're using. Right. And you will have more curiosity when it comes to using APIs. It's like, okay, if I send certain data, like how much segments are my sending? Because you want to send as minimum number of segment and I packed full, you know, and only useful information because remember he's sending an HDB request and that transfer to multiple segments. Those segments have to hover over an order and they have to be assembled to be a request and only if they arrive all of them together, then they will be delivered to the application as a request. Right. If any segment that is missing, you can't do that. And that as obviously it fits nicely into a single frame. I took this picture from Cisco. Let me hide myself a little bit. So I thought this picture from Cisco is actually pretty good. This explains what we're trying to do nicely. Right. So this is the whole entire frame, right? And this is the 14 header rainbow and the Ethernet. And this is the 20 bytes. IP header, again, can go up to 60. This is the TCP header. 20 can go up to 60. And all of these are here. This is the payload. And this is the hardware, not you, right? That's the maximum transmission unit, right? This is the frame. And this is the maximum transmission unit and the IP packet. The IP transmission unit is all this includes the IP, but the IP segment is was is everything but these headers. That's how much you actually get in the TCP MSI site, only this particular payload. So kudos to the Cisco for doing this beautiful dagger I found. It's fascinating. I want to share it. Another critical concept here, guys, and this is one of the most important thank you to return to sender so empty use our network properties right so they are different in every interface. So if you have a network like this and you have 9000 of you here, you have 1500, you have five, 12 for some reason. I don't know why, but just an example here to break things and you have 1500 here. How do you know? What's your. So you can determine the maximum segment size? You have no idea. So you have to. To taste the network in order to find them. Do you meet path into you? Discovery. That particle discovers the lowest into you and that is what gets used. So how does it work? MTA is the network interface property. Each host can have different value as we talked about. You really need to use this more or less them to you. We talked about this. Why you cannot use a larger one. Because if you use the larger one, then you will have to fragment. Right? But then can you help determine the MM2 in the network path? That's how it that's how it works. So that's what it does. How it works is going to be in a minute. Clients and an IP packet with its empty you with a flag. That's how it works. So the client will send its on them to you to say I'm going to. I don't know what's them to you. This is my home to you. This is my maximum segment size. Here is an IP packet of 9000 bytes. Right. Go and don't fragment. Remember this flag. I showed you in the IP options. That is the flag. The there don't fragment like one of the flags in the IP says, hey, no matter what. Do not fragment this IP packet. It tells you that if you encounter an empty, you let lower than yourself. Write lower than the IP packet. Fail. Tell me. Fail and tell me. Fail is one thing. But tell me is the most important. Tell me. How do you tell me? ICMP shows up again, I think is one of the most coolest protocol. I really love it. So what happens here the host that their MTU is smaller will have to fragmented by can't because of the death flag. I don't fragment. So the host sent back and ICMP message fragmentation needed. That's the message it sends. That's the one of the types of the ICBM messages will say, hey. Sorry this particular guy encountered this particular ICMP, this path according to you. And it it it requires a fragmentation. But I couldn't because you told me not to. So I need to tell you that the fragmentation needed. Now, what I don't know is whether the actual empty will be included in the packet or not. Probably not. So the client will have to kind of lower that. And that's part of the algorithm. You can go read on this algorithm to read to know more about how it actually works, but that's effectively how it works to determine them to you. Once I know them, to you, I know my segment size. Once I know my maximum seconds size, I can use that to send my data effectively. This is to avoid fragmentation, as we talked about. Summary. Summary MTA is the maximum transmission unit on the device. Talked about that and assesses the maximum segment size on layer four RAM. And if you can fit more data into a single segment. You lower latency by design, right? So it is in your advantage to try to fit everything into a single segment, if you can. Right. Because this segment is better than more segments if you get. No, but obviously is very hard to do in today's day and age. Right. Because we are so bloated in the application. So we send all sorts of garbage data that we really don't need. But yeah, it's just something that makes you to appreciate what we have today. It lowers the overhead from headers and processing, right. Because, you know, instead of processing seven segments, if you can process one that's 20 times seven headers that you don't, you have to wait a minute, 40 because we have the IP and then a layer four DCP, right? So we have 40 bytes minimum because it can go more than that. But then you can discover the network lowest into you with ICMP, we talked about that and flow control. Congestion control still allows sending multiple segments without access. So this is just a note that I know that ad I guess you're having everything into one segment is nice, but yeah, we have flow control. I could just add good. Or that allows us to send multiple segment. But you still have to process these headers. Everything adds up, guys. Everything adds up. A few more headers, a little bit more latency. All right. With that, how about we jump into the next one?


### Nagle's Algorithm's Effect on Performance vtt

I believe a lot of people ran into that particular algorithm and not in a good way. Usually in a in a negative way. If you ever seen a delay in your client side app that you couldn't explain. Right. Network is good. Everything is good. But the server is not receiving my packets. Just there is a random delays that is really none deterministic. This guy. This guy is the reason Nigel's don't grow them or Nigel's algorithm. I don't know how to pronounce it right now. It doesn't go to them. Let's talk about it. Very important. Have you ever seen a delay in your back in application and find it application? This might be it. This might be it. Stuck about it. Nigel, don't go there in the till the days it goes back to the overhead of TCP, you know, sending the segments with 20 bytes and then the IP here with 20 by. So that's 40 bytes. You know, you're sending 40 bytes. You better fill this mscs with content. You better fill the segment with content, right? In the Telnet days where you type in your space and that sends a single byte and then you type in, turn that sender, then go by sending a single byte, right? Oh. Sending a single byte with a 40 byte overhead is really, you know, it kills the bandwidth. You know, you just like what a waste. And this is like way, way, way back, you know, it's just like, okay, we really don't waste. So an agent came up with the idea. It's like, wait a minute, how about how do we don't how will we wait to fill the segment fully to its maximum segment size before we send it? Right. So let the user type in, type in type and open less, whatever, enter, send any commands they want. Right? But then we're going to only send them back the segment when it's filled up with content. Right. Obviously the algorithm a little bit more complicated than that. But this is this is the idea. Let's wait. Right. Let's only send it one when the next one when the segment is full. Right. And they'll wait and they'll wait. Is what you experience in the client side as a delay and as a performance penalty, effectively? And we love performance. We don't like things that kills our performance effectively. Carol, I don't know if you know this utility call. Very, very critical. Very important. They disabled this algorithm. As you know what, our trials is being slow for no reason. There's no point. If I have data, please send a don't wait. Don't buffer on the client side operating system. Don't try to be smart send the data immediately that's that's the goal here so it's read and then one day sending a single by missing in a segment is a waste can combine a small segment and send them into one that's the goal here let's combine multiple segments like multiple data into one segment, fill the segment and then send it. The client can wait for a full MSDS before sending the segment. There's no wasted 40 byte header that we talked about, right? There's this extra header that is added to manage that stuff. He we don't want to waste that. Let's disappear for a minute here. How? Right. Here's an example. So let's assume the message is one four 1460. Right. That's the default, right? An A1 to send 500 bytes to be right. So what happens here is a writes the 500 a byte as an API says, hey, send me this 500 byte. It doesn't know anything. I just tell the operating system, hey, since 500 bytes to be. Right or as doesn't send it right. This is how an ideal algorithm works. Awaits. It weighs in the buffer. So since 500 is listed in 1460 clients way to fill the segment and then decide to send 960. What a coincidence. Exactly. It perfectly aligns. Do one for 60. Right, guys, I just did that. You know, it's an example at then to the right, you get my point. Right? We'll go to what examples? Don't say so now 960 the segments filled and then we send them back. So there is a delay right here. We waited. How long do we waited? I don't know. It all depends, to be honest. And this delay, by the way, guys, only happens when there is some data that needs to be acknowledgment. Right. It doesn't happen when you don't have anything in the to be acknowledged. Right. If there is if I'm sending this and there I don't have anything to acknowledge it immediate as in the 1200 will get sent and so nature algorithm is not you know that you know extreme it gives you a lenient right away if there is no act data. Yeah let's go ahead and send it. Well, this will be clear in a minute. So is the problem with Nigel's are gone, right? Sending large data causes delay. So let's say I want to send 5000 bytes on one 4060 MBS. Right, 146 0 a.m. SS. Right. That's what I'm going to do. So that equivalent, if you do the math, three full segments of one 460, but you're going to have 620 remaining bytes because that doesn't full fits the entire mess. Right. Guess what? You're going to send the segment because it's full. You're going to send the segment because it's full. You're going to send the signal because it's full and because you're waiting for an acknowledgement. You have one small segment of 620. That will not be sense. Sad. That's the delay you're going to acknowledge. And guess what? The delay you're going to see the delay. That was a little bit wrong. That was should be until here. Really, only until here was the reserve acknowledgement. Okay. We got an acknowledgement. No more act to stuff to to ask. Let's send it. And now you send the packet, the remaining six to any packet. So there is a delay. So if you're sending a large packets, all of a sudden it will ask what is going on? Why, why? There is a 40 millisecond or 50 millisecond or whatever the latency really if you the more latency you have between Amby, the larger the delay because you're going to wait for the act to come back in order to send that stinking segment. Or you can order or fill that segment with data which you don't have obviously. Right. So you can, you can you can trick either you have to solution, either disable the natural algorithm or fill your segments nicely to their message, which is very hard to do, obviously. Right. So the fourth, fourth not full segment are only sent when ack is received. We don't know about this. So that's that's the problem with Nigel algorithm. Very, very problematic thing. And disabling Nigel. I go to most clients today that I heard of disable nigel algorithm. Nobody wants latency, nobody wants performance again. If this is not clear, this is a client side only change. You know, you disable nigel algorithm in the client side because that client side is the one who said, oh, by client I mean whoever sends the data because the server also can act like a client and the server also sends data. So it has the same it's the same bullet effectively. Right. So so I know a lot of people say, hey, I either get the performance, then then there's small bandwidth. I don't care about the bandwidth. Right? So what if I send 620 bytes and not follow messages? Send it. This is called TCP, you know, delay. If you're low level in the C land, it's called TCP, you know, delay. Right. And Carol, actually in 2016, this is the commit. I found the commit they did that they disabled. Nigel. I'll go to them in 2016 completely and curl in official curl today if you have a call. Nigel is always disabled by default because the creator sorted through this, actually. Bum, bum, bum, bum, bum, bum, bum, bum, bum, bum. This is a badger, which is the Daniel Thunberg. After a few wasted hours hunting down the reason of slowness during a TLS handshake, the is the encryption. The handshake that causes the encryption after the ECP handshake. That turns out to be because TCP no delay not being set because TCP is not delayed. Not being said, I think we have enough motivation to toggle the default for this option. We now enable TCP no delay by default and allow applications to switch it up. So you can, if you want, you switch it out. So you enable TCP no delay that that tells your clients, I do not delay the packets do not delay the segments. Send immediately sitting TCP. No delay disables Nigel's algorithm. This also makes the CCP no delay and no delay unnecessary, but no TCP, no delay can be used to disable it. So they flipped it. So they had the option for the longest time, optionally, to disable Nigel's algorithm. But now they made it the default. Whether you said or not, we're going to disable it by default and you can enable it if you want. Enabled Nigella to go to them vis a vis having not been on the language, which is now a very confusing to read. Pretty cool, huh? This is Nigel algorithm, guys. So, yeah, Nigel had to go to them. A lot of people disable them. So what is Nigella giving them? Summariser. Nigella I gave them was designed to weight. When you are sender, whether a server or client. You wait for a full packet worth of MSDS. That's why you really need to understand what I MSRs means. I to understand this. Right. Let's wait for a full right while while we have acknowledgement why we have data that is not acknowledged only that only them. If I don't have any data in transit, send immediately. That's okay. But if I sent a sentence and since the data and now I have, I want to send three bytes, right then you have to wait for this segment to be filled. Yikes. And when you do that, obviously that weight. Kelsey on the sender side. So this is client and really server, right? Imagine imagine the the server trying to respond back with the I don't know, the result of a sequel query. Right. It will send send sentence and then there's one byte left, right. And then we tried to send it, but the server can't push it. Tries to push book, can't push the whole stack just because they don't acknowledge the result. Disable I guess enable TCP not delay. Write this case and yeah. See what see. And definitely I believe that will increase the performance. So that's a very critical backend configuration that you can do. And from that as well. Right, if you want your product to be more performant and yeah, add an additional no cost, I rather not wait that latency hit. Take the head latency just to wait for an acknowledgement in order to send my half a segment. No, sorry. I rather disabled android. Go them. On to the next one.


### Delayed Acknowledgment Effect on Performance vtt

All right. Here is Nigel's algorithm cousin delayed acknowledgement. Those two were developed at the same time. Back in whatever the 1970s or eighties, actually, they were developed at the same time, and they will implement at the same time. And boy, if both are enabled the same time, this is a disaster. Let's get into it. Less packets are good, but performance is better. These are my thoughts, by the way. I make this up so that let me know if they are cringe. All right, let's disappear again. This is one of the slides that I couldn't cram in screen on. So delay technology, algorithm waste. It is waste directed all segments right away. Maybe it's very wasteful to acknowledge a segment, get a segment, acknowledge the president, acknowledge. Let's wait a little bit. Let's wait to receive a lot of segment and then acknowledge one. Right. Because here's an example. Right. I sent ACE and segment one, two and three, four. Right. And then five. If I waited, I can lose them all at once. I can send one beautiful back, one back to rule them all. This is the ah Lord of the Rings now. Okay, so one actor, all of them all to acknowledge them all. Isn't that great? So why waste bandwidth and and garbage packets that are empty? When I can delay the acknowledgement. The problem with delayed acknowledgment like this beautiful thing. All right. Let's combine this with major algorithm A, let's see what will happen. Cause delays in some clients that may lead to time out and retransmission because now you're delaying. Some clients will think I did my ah my packets. Did my packet arrive or not. I don't know let me retransmit. So you're you're doing empty retransmission on nothing really there is noticeable performance degradation with both of them are going to explain that a man and that is combined with major algorithm you can lead up to 400 millisecond. That is brutal, man. Each body's just waiting on each other. Let's. Let's get into this example that I rolled up, guys. Here's here's the same example from before 5000 byte. Both. Both, Wolf, send these three segments. Right. Here's the thing. The fourth segment is just 620. So I need to send it, but I can't because Nigel right now just says, wait, wait, man. And be. Why y I'm waiting. I'm waiting for an acknowledgement for these before I can send your half a segment. Right. So. So, B, in this case, guess what? Delayed acknowledgement. I'll go to them on the server side. So we are delaying. We're waiting. So this guy is waiting. This guy's waiting. And nobody's just talking to each other anymore. So this is waiting and this is waiting. And this gap can increase up to 400 millisecond in some cases. You know, and all of a sudden, you know what? I think I waited enough and this wait is I don't I don't know if it's defined or not, but if this goes along, is this okay? I give up. Let's acknowledge it's been a while. It's been a second already acknowledged this. And the ones we acknowledge these three guys as obviously act three. So I could acknowledge all these guys and then we can obviously send my segment only then. So look at this delay. So there is a delay right here and here to this law that I'm still you and then we can send. Yikes. That is absolutely brutal, if you think about it. Really brutal. So yeah. Disabling, disabling, delayed acknowledgement. Go them. You can do it by setting the TCP quick ach. So there is a delay. No delay. And there is quick ach quick ach option. Disabling delay technologies can be done with the TCP quick act. So this is a low level socket options they are called. So. Some apps like Node.js exposes some of this stuff, right? Node.js is working on exposing certain things like their algorithms, but Python, I believe also. But if you have if you have a CE client like low level CE, glad you have access to all this stuff. You know, that's one one advantage of writing low level code, right? I guess segments will be acknowledged quicker when you disable delayed acknowledgement. So it's a it's really a lot of people think really hard about this and experiment and taste, you know, to to see the difference about this combined with acknowledgement, you have to try and and taste all these things and see what is the impact on your back end and your front end in this case. Right. But just understand, especially this combination of Nigel and DeLay deaths is just a disaster. You know, it can really kill your performance. So if you ever experience a performance now, you know why you're this is lower than you. You know, it's this way. You're at the higher level and this is a low in the network stack. But now, you know, on to the next lecture.


### Cost of Connection Establishment vtt

The cost of connections, understanding the cost of connections. When you establish a DC action, there is a cost of the cost for everything. And this is my 17 years of experience. This is if I learned anything, anything I am going to do. There is a cost to it and only if I understand the cost. I feel comfortable using that think right. If I don't understand the cost, I feel anxiety is like, okay, what's going on here? Am I really am I good using this thing or not? And the cost of connection is one of them. Let's jump into it. Connection establishment is closely. So the TCP three way handshake. What do we know about the TCP freeway? Can't talk to you unless you handshake me. I need to talk. Handshake to you? No. I need to. Reach out and you need to reach out back. Sinek, send Sinek back and then I can send data. So that's cost, that's latency. The whole cost really is latency here if you think about it. Yeah, if I'm close to each other, I don't care about the TCB handshake. Like a virus is like one less than a millisecond if the devices are next to each other. But if it's across the globe, if I'm talking to someone in China and I am in California, there is physical distance that the network has to go through and that is sold rather than all the titles that need to happen and all the inspection. My packet will be abused by the time it reaches China. By the time my packet reaches the destination, it will be it will be dead. The packet won't be done. Right. So there is latency is what kills us here. No. The further apart the peers, the slower it's it is to send segments out. Of course slow start keeps the connection from reaching its potential right away. I talked about the slow start algorithm them and edited it in a minute and then and then so it goes slowly until it funny is slow start actually fast because the exponential you know because it adds one segment after each an acknowledgement. So if you don't have delayed acknowledgment enabled right then your slow start is actually faster because you're going to get a lot of access, you know, which is pretty cool. So slow start will kick up a little quickly, but it's a cost regardless. We have to do a slow start, right? We cannot send a lot of data. We cannot send our data right away. And then congestion control and flow control limits that further. There's so much stuff delayed and delayed in acknowledgment and Nigella got them can further slow down things. Destroying the connection is also expensive. This is costed. Everything we do. What do we do with our lives, you guys? Meat Connection Polling. So Connection polling is something I talk about all the time in my courses and my YouTube gal most implementation database back in now. Now we're linking vendors to actual databases and reverse proxies. Right. This is this is now the link that I was talking about. You know, just now we're talking about TCP IP and UDP and IP packets and headers. Now we're what? Now we are in the trench, you know, connection pooling. Most implementation of the database beckons and reverse proxy use pooling, actually. So what is a reverse proxy? Reverse proxy is a is a server that a. That talks to a bunch of back and fleet of machines without you knowing. You talk to one reverse proxy. As your final destination, and that talks to a back end or one or more back end. So by design, the reverse proxy have to talk to many backends. So it's in its interest to establish this connection and warm them. In advance because of the cost that we talk about, because if you send a request synchronously establish the reverse blocks, you will synchronously establish connections upon the reception of the request. Then there is an additional cost that you need to manage, plus the the slow start and all this. A Why would you make the client suffer? Start the reverse proxy. Hit the fleet in the back and warm all the connections. Start 30 connections and prepare, you know, for this stuff. So that's one idea of the connection pulling. Same thing with the databases or databases. You would have a database server and you have the back end and you will create a pool of database connections. With that user effectively connected to the database. And then clients that hit you as a back end web app will pick one connection from the pool, reserve it or nobody touch it because of the problem we talked about earlier with the sequel statements, you don't know which responses for ones request, you know, reserve it or nobody else can use it. Send your sequel query, get the response back. Now you know that only you can respond back in this machine, this connection, and now get back the response and then releases back to the pool. So now you don't have any leaking on anything like that. So you can effectively have these connections warm and already reaching its potential slow, start wise, congestion control wise and all this stuff, you know, how beautiful is this? If you think about it, I'm so excited. I'm sorry. I just love this stuff. So establish a bunch of these pictures in the back end to keep them running. Good idea. Good idea. Good idea. You know, Donna Summer connection on request, right? Because that will just slow things down. Right? Don't do asynchronously. Do asynchronously, if you will. Any request that that's not really asynchronous. I take that back. Any request that comes to the back end. Use an already opened connection. Beautiful. An open connection that is hot. By hot, I mean it's already warm enough such that data has been sent there. Poor schlub first client will suffer a slow snooze. Let me let me say this without the tongue twisting the poor slob client that first makes a request of the reverse proxy. And their first party will hit that freshly open connection to the bag and will suffer through a slow start and will suffer through congestion controller. Congestion avoidance algorithm. Right. But as you send more data, more data, more data, the connection will be warmed and the congestion window will be really fat and big. And that's what we want. We want big windows. We want big windows. That's why we want. Any recourse that comes with the back end or any connection to talking about that. This way your connection will be warm and slow. Start would have been already kick. Then don't close that connection unless you absolutely don't need to. Absolutely don't need it now don't lose the connection unless you absolutely do not need it. Keep that action open. What do you care? Memory. Its abundance. Keep it running in the back end. Right. Don't close that connection because if you close it, then you have to open it. If you anticipate some, you know, uh, traffic, keep it running. If you really think that you absolutely don't need it, be a way to close it. Right. Another concept that we front engineer and back engineer uses eager versus lazy loading. So how does that affect us as a networking when it comes to networking, eager versus lazy loading? These are two paradigms that we use all the time, which comes to back end and front end engineering. These are techniques that there is no wrong, there is no right. Each technique is used based on certain use cases. And I, I used both approaches and I cannot tell you when to use one because it all depends on you use case, right? So depending on what paradigm you use, you can actually save on resources if you care about saving resources, if you want to scale. Right. When it comes to scaling, so you would use eager loading. What is ego loading means? Hey, I started my applications. Let's load everything up. Start everything up. Start up. Start of everything. Run. Open. 700 connections. Leave it running. I want everything to be eager, you know. So in this case, the start of a slow ride. Because now you're doing all this work right on starhub. But requests that comes in will immediately get served. Right? Because they have this stuff hot. So some apps. Send actually warm up data, which is numb. I don't really like that, but I've seen some of does that some absence warm up data to kick in the slow start algorithm but really careful about the bandwidth when it comes to scalability here. Right. So what these apps do in the back and specifically, I was like, okay, it will start the back end and then we'll start that ECB connection. But it will send dummy data to the back end. Right. And we configured the back end to discard this data effectively. And since it's the back end and the reverse proxy is so close to each other such that it doesn't really matter because it's a local network. Right. I can do that, but I'll send warm dummy data just to kick in the slow start all the way up to the congestion avoidance so that I have a large congestion window. Some people do that I've seen, although I are like it, I feel I feel less let let things happen naturally. You know, you just loo why waste the resources when you don't really need just to get that extra performance. I don't know. I don't know. It already depends. So this is the ego loading effectively. Let's load everything. Let's warm everything up and make it hot and ready. Lazy loading is only load things on on demand in this case. So startup is immediate. Startup is not doing anything. And that's something you might want. You don't want your startup. Yeah. If you're doing eager loading and you want to test AB Oh my god, that will be really slow because the startup will be really slow. You're doing all these resources, you're consuming all this work every time you run the AB is doing all this warming up on the backend. Oh don't do that. Lazy loading immediate, but the request and the first request will suffer. The first pull shrub that makes the request will suffer. Unfortunately, startup is fast, but request will support as initially. All right, that's it then. So connection establishment is expensive. So that is basically what you need to understand. Connection establishes is costly. We can do certain things about it. We can do more things in the next lecture, we're going to talk about it. But this is some of the things that we can do to. Eventually amortized the cost of the connection establishment. On to the next lecture.


### TCP Fast Open vtt

Be fast open. Wait. I can send data during the handshake? Yes, very. You can. That's actually pretty neat. So we talked about we talked about the TCP handshake. Right. Is like sense and EC and ACH. It's kind of sad that I am sending this this data, then this this packets and they are empty. There's nothing in them. It's just a single bit. If you look at the TCP header, you know, the the the layer Florida, the TCB, they went through just a single bit and nothing with the bunch of sequences and that's it. That's so sad. I could send data, the handshake. But guess what? We don't trust you because I need to trust you in order to send data. So people came up with this idea, what if I send you a pre-authorization authentication? I talked to you once. Here's a proof that I did talk to you before. This is specifically cool. If you want to send if you want to establish multiple TB connection from the same horse to the same destination effectively. Right, which is very common. Right. TCP IP is very common like the browser when you first open it htp one one specifically it it just fires up six DC connections just like that because it wants to paralyze, but it can't because of other problems that we talked about with with TCP and yeah ahead of line blocking and all sides have had a blind block and so there's a problem. So what it does is like it opens took DC back action and tries to. Demand triplex in this case. The request on this DCP connections. Right. Just like a multipart DCP in this case. Right. So what does the DCP fast open do? The handshake is slow. We know it. The handshake slow. I already know the server. I have a stubborn connection to prior. Can we use a predetermined token to send data immediately during the handshake? Can I do that, sir? Meet TCP fast open. All this fast open or TFO? Easy to follow. TCP fast open. Let's take an example. Let's hide here because it's not really. So the client and server establish this connection one. Right. And they agreed sort of all and sends an encrypted cookie back to the client. And they're kind stores that t if oh cookie for later usage. They client want to create another connection from the same server from the component to the same server and it's just the different port that's what the change right in this case the client center sin. Data center data. Actual data beautiful segment has a data in the thin right. How long does it take to fall cookie in the recipe option? So the three options that we talked about, you know, that that so that we're adding a cost to the header is a year, guys, right. So it's more than 20 by today or in this case because I'm adding a different type of or cookie dough for this device, vast open cookie. The server authenticates the cookie and sends back the response. Actually, if there is a response in this case, it include the response and the snack, which is part of the handshake and the final leak. Isn't that cool? Right. So the best use case for this. What comes? So what comes immediately after TCP handshake? The DNS, the one that establishes the encryption. So most of our data is actually TLS, if you think about it. So if I can send this steals data with the first send, I would have saved Emily what a few milliseconds. Not if you actually a lot. Right. Instead of waiting all these roundtrips right you can send data and every really you can really save a lot of bandwidth a millisecond latency wise. Right. So Tier four was actually enabled by default in Linux, 3.13 was enabled back when. Right. And you can enable a TCP fast opening call by doing crucial dash dash TCP fast open if you want to and call will try to. When it tries to establish a connection to the server, it will send data in the syn effectively. So it goes without saying you will get TCP slow. Start with TCP fast open. Right. So this is a statement that I pasted that I said and Twitter at one point, actually, Daniel was the first one to say that the creator of Kernel, it's like I was, I was confused like a few years ago was like, okay was that a TCP fast opens lost hour? So he explained it to me and then I said so. So a slow start has nothing to do with fast open. He said, Yeah, you can get slow start even with DCB Fast Open. And I found that statement very contradicting. And I always tweeted from now that people get confused like over, Yeah, I believe now you understand this. Of course you can fast open that connection, but you get a get slow start because this has nothing to do with this. Right. So let's start is the congestion algorithm. You can take advantage of this feature to send early data, as we talked about.


### Listening Server vtt

Here's another lecture that I thought it will be very fascinating to add in. Here's another lecture. I thought it very, very useful to add in this section. Right. Listening server. We listen all the time, right? When we create a server, whether in Python or Node.js on C, C++, C sharp. We listen on an interface and listen on import RAM. But we really understand what we're listening to. So that lecture kind of demystifies that, if you will. Let's jump into it. So listening. What is listening? You create a server by listening on a port, on a specific IP address. You have to specify an IP address to listen to. And you might say why I'm listening on my machine. And my machine should have only one IP address. No, no, no, no, no, no. Your machine will have multiple IP addresses. It already have multiple libraries as just the low loopback, which is the local host is an IP address. Another IP address is 0000, which is literally tells that tell the host that I want to listen to everything. And the third one, which is the actual network IP address, which is whatever it is that connects you to the Internet. Right. Could be wi fi. You could have a plug in Ethernet. That's another IP address. You could have Docker installed and that's a bridged network adapter. So you can have many IP addresses. So when you listen, you better know what you're listening on because, boy, this is dangerous stuff. Dangerous stuff. Let me explain to you one bug that I read that costs really cause a lot of disaster. There's an admin interface and one tool that I'm not going to mention, and those guys that built the admin tool listened on 000 this 1000 that. And they just shoved the tool, you know? And this tool was deployed on a public server and the admin doesn't have any password or credential. Because you listen on all sports that include you're listening on a public IP address. That means that port is open for business. Anyone from the wild, wild, dangerous, scary internet can connect to your admin API and cause harm. So when you have an admin interface that no admin API or admin page that should nobody ever has business connected to from the internet. Look, listen on the loop back host or listen on a private IP address that you know you can only access from the private. Because guess what if you listen on 000 is going to listen on the public. Private. Private here. Private here is going to listen on everything. And it will accept any connections from anywhere because servers are dumb. Let's continue on. Your machine might have multiple interfaces with multiple libraries. We're talk about that. You can read one lesson. What does let's explain all this stuff. We're going to go through an example in a minute. I made a video a while back and I want to reshare it here. I think it's a good idea to share it here. So we're going to listen on 1 to 7 .0.0.1. Right. And on port 88, what does that mean? It means listen on the local host IPV four interface on Port 88, IPV four, this this is the IPV four local host Ray. That then if you go to SDP, call in 127001 call in 88. That will establish a connection to this guy. Right. But if you go to the IP address, 192168, it won't because you only listen to the local host. If you go to try to go to the IPV six version, it wants to connect because you're listening to the IPV four version. And if you listen on this string local host, it's all over the place because it depends what local host is actually pointing to. A local host is just there DNS entry, right? Local host can point to this guy or this guy, by the way, call and call. And one is the IPV six version of a local host. If you want to listen, call in. CALLER 188. This listens on the local host IPV six interface imported data. That means if I connect to you, that immediately connects to that IPV six, right? That's on that. That will only happen when you do that. So when you try to connect to 1270011 happen because there is no one listening here. Same thing here. 19.68.122. You can only access from this. You cannot access from localhost. You cannot access from here. Maybe you might get away with localhost. I think actually, I'm not quite sure. I think we need to test that. If you listen on that, will localhost be allowed? Probably, I guess. And here's the most dangerous 10000. I believe it's all the default if you don't specify. It's also lesson 000. I don't know is the default that's just dangerous. You know, this is what all interfaces. So if this is the code that you have, wherever you run, it is going to listen on any interface that it finds there. Hey, oh, this is an iPad. Let's listen, listen, let's listen on that. Very dangerous. So you can only have one process in that host listening on a IP port pair because that's the error you always get, right. If you have two processes, one process tries to listen to 80, 80, and another process tries to listen to the same port. Remember, you get this letter right through our port already in use. However, there is always an exception for everything. Rain and this is called the socket option. Port reuse. Right. The operating system. At some point in his life, Diane introduced the ability to listen on the same port by different processes. And what the operating system will do is load balance that segments, not segment the actual connections to the right process. So it will do actually a load balancing, which is fascinating. Most always all proxies use this concept now. I saw that because it will give you huge yet a bandwidth bandwidth in a baton that is the wrong way. It will give you a huge, you know, throughput. That's the word I'm looking for. So there's a conversion that allows more than one process to listen on the same port. This is the socket option port reuse of resistance bands. The segments among the processors does not entirely true. Not all segments will be balanced to all processors, right? It will. It will hash. And based on that hash, it will go through that particular process, you know. So in this case, the hash will be the four pairs that we talked about and that will go to a specific, you know, application. No. So it's it's effectively, let's say this. So this is going so app x oh. Let's. Let's remove my face so we can see it. Right. So app x here is listening on board 8080 and our boy is also listening on port 88. So this guy is coming from five, five by five. This guy's got five, seven, one, one, two, and they're both going to 88 on the same host. What they'll system will do is that OC ten 002 88. Sure, I have that, but where are you coming from? One five, five, five. Okay, let's hash that. Okay. Hash that goes to app X. That same thing at 80 717712. IPA one. Different hash. Go to the other one, right to the other app, although they are going to the same port. The operating system will take care of the load balancing of this. There are bugs that were reported in Linux where the if the if the process stops right, then the hash table changes. This causes real bad. This causes real bad problems when it comes on the table. The hash table changes. We know about the hashes right at the table. If the hash table changes, you have to rehash and remap things that can get really nasty. Right on to the next lecture, guys. Today I'm going to talk about the HTTP server, listening and Node.js and then just take it apart. What is it doing actually? Right. How about we actually just jump into it, guys? I'm going to create a brand new JavaScript file. Called Listen to Jess and we'll just go ahead and create a new variable here. We're going to require the FTP server and we'll create an DTP server object by calling it DTP to create server and nothing more. Just just that. And we're used to really just calling listen and providing import, but. I want to. You know, explore what other options we have here and what do they mean, really? And when we actually provide a pour, we're going to listen on that board. Obviously, you're going to tell us that. But I don't know if many of you know that even the poor there is a question mark next to it, which means it's actually optional. So technically this works. And the first question that you might have is Hussein, or if it's going to listen, where is it going to listen to which board? Well, everything is an event almost in the logs. So you can effectively have an on listening and like a video studio could actually autocomplete for you and it tells you hey an on listening call this function right. This is very similar to when you provide the callback in the listen argument. But what we're going to do here is effectively, hey, when, when I am listening, we can just literally just print, hey, listening on port. And here is the beauty. You can get the port from the piece of our new. We need to be really careful here because this will only be called when the Listen event is successful. And basically, if you don't provide a poor or you provided zero as a value for the port. So the zero is not a valid port. It will pick a random pour and listen to it. And here's how you get the port, that random port that has been picked for you and you can do a STB server. I just found out this really recently, to be honest, while I was exploring and making the other video you can do to Deep Server, that address and address is a function, right synchronous function that you can call and it's going to give you a lot of useful information. A rhythm is a port. And if I do that now. And that's just that's just the sense this is a really use useless app. So let's add an actual event here on request. If someone make any requests to us, I wonder if you can do an end and write something. I'm not quite sure. Let's try that. Let's just do. Okay, okay. Okay. Let's just do a run. All right? It's good. And run. You can see that we were picked the port. Five, seven, five, six, five. That means if I go to if I go to a browser type in just deep localhost you can see at our server is beautifully running and you can do tricks like that in your app says like on a localhost then you can put the port here and what's his name which was to decode will actually give you almost a link when you can do command click and it will open for you. Right. It will all different definitely port pick a different port. This is this is useful specifically if you if you don't know that environment you're running in and you have no idea what ports are going to be reserved. Right. And other apps or listening to you can use this approach, but this is not really the fascinating part to me. What what is really interesting here is there is another parameter here is like on I'm going to type in this here like what are we listening to is the other question that is actually the most dangerous question here. Sometimes there is an address, right? Right. So say now you're listening on your machine. Of course you can also in your address, you know your address. Not really. It's not as clean cut and dry. I really wanted to I wanted to show you this. Most of you some of you might know what this your colon. Colon is the short hand writing for the IPV six right here. Calling Colon is equal to 000. And IPV four, this is the IPV six and I believe which this this is an invalid. It's called, I think, an unspecified, unspecified address, which means that really it's an invalid address. You cannot use this physically on the network, but software can. And usually what it means, it means every interface. So like if my if I had a Wi-Fi and I have a 5G network and I have another public LAN Ethernet, my code by default listens to all of them. And you really need to be very critical and very, very careful with this. So technically I am running it through a local host. But so if I opened this again and I opened a new one and I did Hosain Mac Dot Local, this is my DNS which technically points to my IP address. Right. That also works. My IP address is this. That works. So I was able to go through a local host. I was able to go through my DNS hostname and I was able to go through my IPv4 and I'm pretty sure if I have an IPV six, I can do that too. I just couldn't figure out how to actually paste the IPV six here and then have it work from the browser. So that means that if I have my phone and I'm going to show you right now, if I type in on my phone here, I was saying, Mac, the local call and five, seven, five. 89. That works from my phone on the same network, you might say, seen and duh. So yeah, we've shown that it works on my phone and works everywhere. But just just the defaults. There's listening in everywhere. And I've seen some apps incorrectly do that. And the danger of this is if your app is supposed to be like an administrator of only that is only should be accessible from local host and not any other interfaces. You really should only access from the same machine. Then you only want to listen on the loopback or certain IP addresses, right? If you accidentally deployed your node g-sap. And that goes for any application. Really, this is almost always the default if you accidentally deploy this on the cloud, on an and on some on something that has a public IP address, then anyone scanning the Internet for this open port, they will be able to access to you. Why do you think we have all these data leaks that is happening with Elasticsearch and MongoDB? Because people just listen on all interfaces, even though they don't necessarily want to do that. You only want to listen to the interfaces that you absolutely are only needing to connect to. Like in a development environment, if you're the only one here, you don't want anyone in, in, in the vicinity in the same network at be able to access your stuff, the app that you just throw out because it could be dangerous. You might say I don't care and that's fine. Just understanding this is actually I think I think it's important, right? If I say it again, zero for the port and you can specify the hostname. So they are really not only interested in 1 to 7 .0.0.1, I only want to be listening on the loopback and not any loop, but I want to listen on the IPV four loopback which is one day 7001. That means let's run again. Boom, this works because the localhost and this also works. But this right. What's my IP address again? 192168.254. ten. Doesn't work when your client, which is the browser, tried to establish a TCP connection to this IP address. The operating systems passed that scene. Try to parse this into this interface and this is what is like a nobody is listening on this port, on this particular IP address. Right. That's why that connection was refused. You didn't get any snack as a result. And sometimes this is a good thing if you don't want anyone, especially like I don't know if you are like in a public Wi-Fi, right. Building those the apps you are near. Listen, you only want to listen on this. Things that you absolutely need. Yeah. Just another another thing. And then I'll add that there is some other useful information which is here tells you what family right ear which is the family of the IP address does. Look, IPV four. We know that 127 is IPV forum. If you want to listen to the like, let's say you have an IPV six configuration. I believe it's this, right? This is the local host version of IPV six. I might be wrong. We're going to look it up, if that's wrong. There you go. That's actually correct. 001. And now I have no clue, to be honest, how to actually browse this right here. Is it just pasting this? I really I don't think this will work exactly. So it doesn't work. I don't know how to navigate the IPV six. Let's, let's, let's actually look it up. Thank you. Internet. It's a square bracket. Do you have to add a square bracket and then you put your IP v6 and then you can connect to it. So that's how you connect to an IPV six, right? A See if actually 127.0.01 works and this is what I expected. 127.0.01 doesn't work because well, you only listen to the IPV six, you clearly listen to only the IPV six version. All right. So I wonder what local host will do, to be honest, would listen to both IPV six and IPV four. It sounds like a defaulted to IPV four by him. Must find out wrong and wrong port probably is not convenient to generate to run a port every time. Huh. Now just realize that it's convenient to listen to the same. But it's just something new, guys. All right. Okay, now IPv4 is listening, but IPV six is not. It's. It's interesting to kind of pick up all these small things and. All right. Do some more fancy stuff. What I'm going to do is I am going to listen to a random port, but I'm going to listen to the loop back IPv4 and then I'm going to create a brand new server. I'm going to call it IPV six now called a six year based, based based waste waste based app. And here I am going to listen to that one. So now I'm going to get two different ports, right? And I'm listening on two different interfaces. All right, let's do that. So to figure out the six here. Now I'm going to just like listening server two. I'm going to server six. Call it here on server four, which means, look, IPV four, what was going to do now? What happens? We're experiment because we're playing look at that. We got obviously two different ports there after the other. But just let us fix this, do this. Then we know how to connect to IPV six now. So let's do that. So now this is boom. Let's open that up and then let's boom, let's open that up. So technically, well, tomorrow working. But both them are completely different servers. So now instead of say, Hey, okay, you are hitting the IPV six server and this guy, okay, you are hitting IPV four server. Look at all this stuff that we're seeing here. Refresh. And now I'm going to do this. Boom. And then boom. Look at that IPV four server or IPV six. Now I'm going to ask you a question. If I did randomize this thing. Eight, eight, eight, eight. 88, 88. What do you think is going to happen? Who can make a guess? Am I going to get an error? Or not. Take your guess. No errors, guys. Of course. Why? Because the uniqueness of the port is address port pair. Okay, it's not just the port. Right. You're listening on the local host. So technically, you can also listen on eight, eight, eight, eight on the IPv4 public public private IP address, 192. That's that gives you another interface if you want to scroll someone. You can definitely do that. So let's see what happened, though. Boom. Open that guy. Up to six. That up. Oh, localhost. Localhost is pointing. Look at that. Look, I was actually pointing to V6. So we actually need to do bom bom one, right? We have to do 127.0.0.1 because the local host, the DNS is effectively pointing by default to the IPv6. You know, that's another thing you need to understand. Local host is just a DNS label. Right. I don't know if that's actually a correct thing to say, but that label point has to be a point to the IP address by default. Sounds like it's pointing to the IPV six. Right. So technically, to be more accurate, this thing has to be 1270.0.1 copies. Good stuff. Open that up. Look at that. It's a completely different server. Let's close all this taps. So now, if I actually forced an error as far as I know by doing this, this is a no no. Because now you're listening to this guy and you're listening in the same interface, right? If you're a fresh. Process exited with code one and we have no idea why. So now this is when we actually need to somehow capture the errors, which we didn't, by the way. That's why when you need, that's not on at all effectively. And I love Visual Studio code. Look at that. The people behind this Visual Studio code are heroes. So we're going to do a console. Lord God is going to do it. And what am I doing? Error. Error. And I have no idea what I think. This is just a string, but it might have been an object. And we do, boy. And then I go capital p wrong undo still figuring out them although I converted everything to them here. Still trying to learn them. Learn and just like force myself to, you know, I'm getting used to it, but I only know very few commands that, you know, get me up and running, but I'm no expert now. Let's go ahead and you run this thing and we're not getting any air or why hurt you are non e server for listening so we listened successfully on this guy. What? Oh. Oh. I'm going to do an error. I forgot the. How did I forget this? But even though we should have gotten something right. That's odd. They got. We got an error or error error. And listen address enus address. Already in use. By address they mean the pair of IP port. Right. And you can get the same error if you like. Stop this. If you do like 0000, which means all interfaces. And if you do the same thing here, you're going to get the same error. And probably it's better to do an actual at or higher so we can see it and read see it and read log of that. Although 000 means all interfaces, right? But it also means that you already listened on this particular IP address, which means all interfaces. It means. Sorry, I can't do that ever. And already someone else is listening. So that guys, I think. I think I'm going to stop here and let's. Let's get back to just do this, have this undo, let's do this to be back to the IPV six and this guy to be to be IP if you fall under refresh. Yeah. The stop here I think, I think it's fun to kind of understand these basic fundamentals and this just from one command, you know, there is so much we can scratch the surface and learn. And I don't know, I personally find it fascinating. I don't know if you do, but I just like to talk about this low level stuff I'm going to see in the next one. You guys, the awesome goodbye.


### TCP Head of line blocking vtt

All right. Here's one topic we talked about, which is the TCP head of line blocking, guys. Let's jump into it. So TCP orders the packets in the order they are sent. Right. The segments are not acknowledge or deliver to the app until they are in order. Right. So if I sent one, two, three, four and packet one is not received for some reason, two or three four took a different path through the routers and path, while one took a was unlucky and took through an author that was congested and was dropped. Right. And in this case, two, three, four was arrived, but one didn't. The application is not identified. Fight of new data because this is not a complete set. This is a grade. We want the A client to rescind the packet one. Well actually the client will send all of them right. In this case. Right. Unless selective acknowledge them, which is something I'm going to talk about in the future, hopefully where it says, okay, I received two, two, four, but I didn't receive one. So the client will know to send only one. But let's assume it's not there. So it will send all of them. So this is great. But what if I have multiple client using the same connection? What does that mean? Multiple client using the same connection. Well, we have a use case called DB one, actually. Where we do that. Here's an example she may request. May use the same question to send multiple requests, actually. Right. If I let's say request one is actually segment one to do right to say the request is really large. So we need to segment. So it's actually what more than two K 2000 bytes. Yeah. The request to a segment three or four. So let's say segment two and three and four arrive, but one is lost. So logically speaking, if you think about it from the application, you as a as a front end injury in this particular case, this is a request to this is a request one request two is three and four. Request one is one and two. So if request two technically arrived successfully, right. Hey, Fred Ford is right there. Deliver it, please, to my app so I can respond. Because these are stateless. Nope. The whole thing is blocked. One is is not arrived. The whole thing is blocked. One segment is missing the entire fleet of records. Imagine ten requests. It was intended course and the first segment is dead. Your ten requests will not be delivered. You will have to rescind all of them. That is a huge problem. Initially be on one and specifically to be to actually with streams is to be to use DCP. And that's one major problems because we use the same TCP connection, right, to send multiple streams of data. Right. And each stream technically is independent from the other stream. Right. This is a stream of in think of the request as a stream here. They are completely independent. But because DCP is just a blob of streaming data, it doesn't know that it doesn't have that context, unfortunately. So as a result, you end up blocking and this is called the TCP IP head of line blocking. And this is one of the major problems that SDB too has, which was solved with STB three and quite effectively. So obviously this introduced a huge latency in apps, right? When you do STV two, right, especially if you send a lot of requests, you you're going to feel it if there are some packets that are, you know, dropped or felt congestion or, or any other reason. You mean like innovative it and that's basically it, guys, right? This is the head of line blocking problem. Obviously, I'm going to talk about Quakecon in a different video probably, but this is it.


### The importance of Proxy and Reverse Proxies vtt

So guys, as you started watching the course and asking questions. One common question that I start to notice is people asking, what's the difference between a proxy and a reverse proxy? While this is purely a networking course, I really thought that it would be nice to actually explain proxy and an inverse proxy because this is the bridge to the back end, because if you're a back injury, you have to understand the difference between a proxy and reverse proxy. So this lecture exactly does that. I'm going to explain what is what is the meaning of proxy? And all the slides that I have here is a little bit of a high level, but I'll talk through the low level details when it comes to not talking. So I'll try to bridge that gap as much as possible, and then I'm going to explain the usefulness of the proxy. Why are we using it? Why do we use a proxy? Finally, I'll talk about whether it is a reverse proxy, and then in that case, what is it used for that we're going to mention obviously API gateways, load balancers. They all either this or this, you know, site car containers, you know, things like envoy line Kurdi, they all either fit as a proxy or reverse proxy. And that's the state we want to get in. We won't be able as much as possible to have a few basic fundamentals and every fluff that is being uttered out there in the software engineer and the network engineering world, we want to be able to resolve those two these basic fundamentals, because any word that you hear means something, you know. And obviously the sad part is marketing that no, to unnecessarily blow things up, you'll hear a lot of things that kind of confusing at times. And so that's what this goal of the courses to confuse if possible, let's jump into it. Alright, so let's just explain the difference between a proxy versus a reverse proxy. And I had these slides already laying around so I just used them, I changed them to a theme, right? So if there is like a little bit animation that is distracting, I apologize. This is this is slides that I have already laying out on that I built a few years back. So what is a proxy? So the definition of a proxy is it's a server that makes request on your behalf. By that definition, it means that you as a client want to go to a certain destination. That's your final goal. You want, for example, to go to Google.com, but if you use a proxy, the proxy will go there for you. That's what it means at the high level. Obviously, this is a networking course. So we're going to explain what does that mean? Okay. So. If I want to go to a Google dotcom, what happens here is I know my final destination, Google dotcom. But I also know in that client machine that I have a proxy configured. And you can look at that if to check if you have a proxy configured and if you have a proxy configured, what happens here from a layer four perspective, your TCP connection is being established not with Google, but with a proxy first, so you can establish a tipping action between you and the proxy. And the content of layer seven will go to Google dot com. Obviously after you establish the CCP connection at layer four here, right, you're going to send the get request and the get request will clearly say Google dot com, go, go, take me to Google dot com. Right. So the proxy would receive that get get slash google.com and it will turn around and establish a brand new TCP connection between itself and google dot com. So Google dot com knows the IP address of the proxy. It never sees you right? As a you as an IP address, you buy the content as you transmitted. It is completely rewritten and written to the new connection here. So any application seven right data is sent as is some proxies and in the case of HTTP, they add their own headers, things like ex forwarded from and stuff like that. And if those headers are added right, the original client can be known from layer seven, but from layer four, Google only sees the proxy. Does that make sense? So I say, what is the purpose here? So, so that that that's the statement here. I just know that and I received a request from my proxy. That's the only thing I know as Google dot com. So in a proxy configuration, right. That client knows the server but the server doesn't know their client. Yes, there are there are exceptions when the proxy adds a header that obviously identifies the client. But that's kind of cheating. Right. So that's what a proxy is. You might say what's what's what's the purpose of this thing? What why are we doing this is kind of useless. Well, one use cases anonymity. Right? It's like I don't want to be identified from an IP address point of view. Right. I don't want anybody to know my IP address. So I want someone to write that request in in front of him instead of me and send it to this destination. Obviously that's not really secure because of the proxy. The moment the proxy adds your IP address, you're done, right? So you need to trust the proxy in a way. Caching is another way. So a lot of proxies, especially the old days, I don't think this is happening now, but if you use it in like an organization, a proxy, that means all HTP request goes to your organization proxy first like this if you want to go to Google, it goes to this thing. The proxy can choose to cache like if if a sees someone went to this static page, it can choose to caches. So if someone else from the same organization using the same proxy hit that server right, it will serve it from the cache. So that's it's it's kind of different, right, if you think about it. And this is slightly different from a CD in. We're going to explain the difference because the CD is actually a reverse proxy. We'll come to that. But this is another use case logging. You know, all this, you know, sidecar containers and service mesh purely rely on the idea of a proxy where you have an application here. Right. And this. This. The site. This software is installed as a proxy next to the application as a sidecar, as they call it. And the application is configured to use this as a proxy then means if you want to connect to service A or service B or service C, all these request goes through the proxy. This is very valuable. So caching CEC is kicks in. We don't really care about anonymity here because of the service service mesh architecture. We just the proxies to talk to each other in a sense so we can log all these. We can see how long a request took. You know, you can measure these, you can monitor, can do all these things because all of this pull into this proxy before it's sent to the actual destination. So the proxy actually takes care of that logging. You might argue that, hey, isn't that kind of slow? Of course there is a cost for everything. And that's what I want you to do. Anything I say here, I want you to challenge it. And that's the beauty of the engineering here. There's nothing that you can take for granted. Anything I say can be wrong, right? Because anything my experience is, is my own experiences and any one out there can be wrong. And it's beautiful to actually think of it this way, because this way you open your mind to critique, right? And as a result, if we critique, we get better, we can build better softwares. So that's that's another thing here. So this this idea of service mesh is primarily using log in, tracing gold as some block sites. So this is definitely used to block websites back in the day at least tried. If you have an organization that uses an issue ATP proxy, it looks at all the sites that you're visiting and it can choose to block says, Hey, hey, hey, can you take me to this site? Yeah. No, sorry. You are not allowed to go to that site. And just the fact that you're not allowed to go to this, then that means that proxy actually sees the sites you're looking at. And this is maybe something that I'm going to talk about, right? Like that TLS and decryption most proxies sometimes decrypt that traffic. I actually I forgot to add one layer which is a debugging, you know, if you ever used a fiddler or man in the middle proxy to monitor your requests like you want an application to see what request is sending, Fiddler is actually a proxy. It is installed on your machine and you're configure it to decrypt that traffic. You can forget it so that all the request goes to it. So we can clearly see all the requests that your application is sending a very, very popular monitoring and debugging tool. So that's another thing. Logging, if you want debugging and it is another job proxy is very useful microservices as we mentioned that already because it kind of digs into logging and caching, which is the idea of service mesh. Yeah. All right. What is this, a reverse proxy? So we said the the proxy in the proxy is the client. Let's go back to the proxy. In the proxy case, the client knows the server ID not. Hey, I want to go to Google or com take me there, you know, but the server doesn't know the client. That's, that's what a proxy adds. Right. In a reverse proxy. It's exactly the reverse. The client. Doesn't know the final true destinations, however. It talks to Google dot com as its final destination. But Google dot com could be a reverse proxy. It turns out it talks to another Google server that you have no idea that you talking to. Right. So you as a client, you just know that reverse proxy. That's your final destination. Right. But Google that turns around and sends that request to an actual back in server. So this is called the backend server. This is called the. Sometimes they call the front end server. Sometimes they call the edge servers. Right. And just like that explosion of beautiful use cases are born. You might say, what, what, what? What kind? Or the kind of things that can be one. Hey, here's one example. Load balancing, right? If I made a request to google dot com, I only talk to the same server. By Google dot com. Could choose to say okay let me send the this request to the server. Google Chen can choose to send the request to the first server. The second request of the third and could ground rob open through them as effectively lowered the balance through them. And even better you google dot com based on the path that you're going. It can take you to different servers. So let's say you're building a microservice, you know, gateway or API gateway stick enable gateway. So if you going to slash. Example post this is the post server. It has a database and everything. So we take you there. If you going to slash read messages, this is the read server. This way you can have completely different servers, completely different databases. This is a read intensive database. This is right in of database. This is like a for example, this is a roast or this is a columnist for analytics. Right. And you can do all this sort of genius stuff just by the beauty of reverse proxy. So a load balancer is a reverse proxy, but not every reverse proxy is a load balancer and a put as proxies just and that is it makes a request to something on the backend that you don't know about. Right. So that's a reverse proxy. Very powerful. So how does it work behind the scenes? You establish a connection between you and the reverse proxy. So the TCP connection is here. Right. So that's the destination IP address. So the reverse proxy knows you. Obviously, you do not know the final destination. You never know the actual server that will serve you. Right. So that's that's basically the difference between a reverse proxy and a proxy. I use this definition for almost six years and I absolutely love it. Everybody here that hears this usually clicks with them. Right. So again. So a proxy. The client knows the server. Final destination. The server. Right. But the server doesn't know their client. In the reverse proxy case. The client doesn't know what the true final destination server effectively. And because the reverse proxy talks to that in the back end. And if you flip this, you can you can see the clearly that this is this is like almost like a reverse proxy, right? So, yeah, so we can establish a CCP connection here and the Google turns around and establish an actual TCP connection to the actual backend. So you can have configuration where, where you are using a proxy and a reverse proxy at the same time. Like here. Yeah, the client knows the final destination, but even that, if you think about it, that final destination could not be it. Right? Because Google itself could talk to an act to something else here. Right. So that could be a reverse proxy. But the client is using a proxy to talk on its behalf. But from layer four perspective, this is my final destination. From layer seven perspective, this is my final destination. And from layer four suspected, this is my father's station. And from Labor seven perspective, this is my final destination. From both cases, this as my final destination, I have no clue what's coming after that. And that's the difference basically to me. And maybe you can look at that this way in a campaign out the real nicely I think. Well are they use cases caching same thing in content delivery network right. I could look at a content delivery network actually like fastly stores the content right here and you talk to CDA and as if it's your final destination, you have no idea what the client is going to talk to you. Right. But that's could be a server in India and you could be a client in India, but you get to connect to that seed in India, the server in India, DCP right. And layer seven as well application and get it serve content but that reverse. You can talk to a server in America, right? A completely transparent formula. So you are served content. So a CD is just a glorified reverse proxy. I'm no bouncing. I can choose to balance your requests from to multiple to multiple servers on the back. Ingress like we talked about this API gateway. Uh, configurations like any API gateway authentication happens in this application and this reverse proxy can out of deployment where you can do things like, yeah, you can test, right? It's like, Hey, I wanted to test this new feature so maybe you can explain this way. So let's say you want to test a new feature, so you want, you will, you will deploy it on one server, but the rest of the server all have the old version. And you can add a reverse proxy rule here that says, okay, 10% of their request goes to this new server while 90% goes to this. You can do that. It's really simple to code, right? Any request you can. Four for every ten request take. One of them. I understand that one of the requests for everything requested right as they handled it for everyone, their request. Ten of them should go to this server and 90 should go to this server, and ten of them will experience the new feature. Then 90% will experience the old behavior. Obviously, we have to build our application in a stateless manner. Right. So that doesn't break because that request might go here. The second request might go here. Right. So you need to obviously code around. It's not that simple. And obviously micro services. Yeah. So these are some of the use cases of the reverse proxy, a very, very powerful concept. So these are some most of the common questions that I receive all the time about this. Two things can the proxy in reverse proxy used in the same time? Yeah, but you would never know it, right? You as a client will know you have a proxy because you have to configure the proxy in your configuration by the reverse proxy. You don't know, right? Your server you might head actually could be a reverse proxy that talks to something else like in Gen X, right? And Gen X is a reverse proxy, Ajay. A proxy is a reverse proxy. Can I use a proxy instead of a VPN for anonymity and not a good idea because some proxies actually terminate DNS and looks at your content. VPNs do not. VPN operate at IP level, so any IP packet, they encrypt that and they don't know or care what is in that IP packet. Proxy operator layer four and above. Obviously. So they need a protocol. As a result, the proxy needs to know the protocol that you're using. So if using that's why there is an DTP proxy, TCP proxy, you know, stocks proxy, all sorts of proxies as well. Is proxy just for sleepy traffic? Not really. There are many types of proxies, but the most popular is the TTP proxy. And there is there is. If there is a mood when you use a ITBS proxy where you can effectively use a tunnel mode, where the client can ask the proxy to open a connection for it, maybe you can explain this way better. So a tunnel more than a deep tunnel managed to be connect to the method. Their client can ask the proxies. Hey, connect me. It will send you a packet. Initiative type packet has to be a request that says hey connect to google dot com. Right? So the proxy will actually establish a connection and it will do that connection will be a dump pipe and that just by the fact we are doing that, any segment that is sent is immediately written to that packet. So the proxy in that configuration does cannot see the content. It's like it's a tunnel all the way in to the end. So in this case, you can you can do the TLS connection in an end to end fashion, right? So the proxy doesn't actually that's just a bonus content, if you will. All right, guys, that's it for me today. I hope you enjoyed this lecture and the next election. I'm going to talk about the different different types of proxies in reverse proxy like layer seven and therefore a little bit more in detail. Looking forward for that. See in the next one, goodbye.


### Load Balancing at Layer 4 vs Layer 7 vtt

So we talked about the difference between a proxy and a reverse proxy when it comes to the networking aspect of this. What is exactly happening when you use a proxy? What connections are being made and who's talking to whom effectively? This is very critical to understand the back engineer, especially for you using this on a daily basis. Right. But then another important concept is also the layer four and the Layer seven proxies or reverse proxies on load balances. So what I did, I built up this lecture specifically to differentiate the differences between layer four load balancers and Layer seven load balancers. And this is specifically a back end concept, if you will, but it's tightened in with networking. So I couldn't really explain this lecture if one did not understand the basic principle of networking, which you should buy now the end of the course, really. So how about we actually jump into this fantastic course was one of the my favorite actually. Let's jump into it. Alright, so layer four versus layer seven load balancer and you can replace load balancers with the reverse proxies and it will be identical effectively. A load balancer is a reverse proxy is just one case of a reverse proxy that happens to have logic to balance the request between multiple back ends on the back end, while a reverse proxies just doesn't have to balance it. It makes a request on your behalf. Right. It just talks to a back end, but it doesn't have balancing logic necessarily. Right. So every load balancer is a reverse proxy, but not every reverse proxy is a load balancer. And obviously a proxy has nothing to do with any of this stuff. That concept of just a proxy, get that out of the way. The fundamental component of back in networking, the layer four on layer seven load balancer we've been talking about back in June, understanding these two pieces very critically. So this is the agenda of this particular lecture we're going to talk about what is the difference between layer four, layer seven? Well, you already know, but it doesn't hurt, right, to mention it. We're going to talk about what is a load balancer. Going to mention the load for layer four load balancer. Do the pros and cons going to talk about the pros and cons of this layer for load balancing? And we're going to talk about layer seven load balancing effectively and also talk about the pros and cons. Let's jump into it. This is our beautiful office. I model the physical, the data link, the networking, the transport layer, the session presentation application, what we're really interested in as back end engineers as layer seven and layer four, we most of the time we play of this layer. Some of us play this layer, or we want to manage file descriptors, play with sessions, see how many segments this connection has received, see how many bytes this connection had, stuff like that. This information obviously is stored stateful in layer five because it's the session layer, right? But that's pretty much as we always play in these two layers. So what does that mean when it comes to actual load balancing? So here's what a load balancer is is also known as a fault tolerant system. You know, I want to build a system that is fault tolerant such that I'll make a request as a client to this load balancer. It it can talk to one backend or several and I don't really care how many back ends. That's the beauty of the reverse proxy, right. That a proxy you talk to this back end and that's that it takes care of talking to many, many backends on the back end. So a layer for load balancer, if you put in some IP addresses here is it starts up right and establishes some TCP connections on the backend. That's when it starts up. Just just warms up. It opens multiple connections, doesn't have to be one, could be ten, you know, per back end and then it just keeps them warm. This is idea of it's called warming up. You know, the participe connection is just keeping things hot so that we don't have to suffer from since anak ach on request. Right. We have the connections warm and just against the network segments. So now if a user establishes a connection to my load layer for load balancer, what will happen here is that connection will have a state in the load layer for load balancer and it will target to. One and only one connection to the back end. And that's the contract here, because a Layer four only deals with ports, IP addresses, and the data is just segments that I, I shouldn't really touch or try to pass. That means when the client connects to layer followed my answer there will be chooses one server and all segments that the client since to me all of them has to go to one connection not only just one server, one connection. And as long as this you're sending me data on this connection, you have to always send me to this, right? Because it's a stateful thing, a layer four as stateful. We talked about this, right? So if you send me data, I can just send one segment here, one segment here. The sequences will be out of sync and then everything will be that right. The data will be corrupted. You can just do that. So if you send me a segment on here, take the data and then rewrite it onto that connection. Right? So these are two different directions, right, if you think about it. And when you write a segment here, you rewrite it effectively on the back and on that connection. So it's two different sequences. Everything is different here. So that's one mode or layer for load balancer. At least there is another mode that it's called the Nat mode, which really makes everything into a single TCP connection. And then we talked about that and previous lectures as well. But we're interested in the common cases, right? In the next case, the client actually is the load balancer is the gateway of the client. And any connection you make, the load balancer effectively acts like almost like a router, right. In this case and just changes the the destination IP address to the back end. All right. Let's throw some examples here. So now what? Even if I send an IP back, Adria, I'm got a destiny. 244.1.2. That's the load balancer. So this is a reverse proxy. That's my final destination, right? I send to the packet and the data is taken here and then literally rewritten into a brand YouTube connection on the backend. Right. So 44.1.3. In this case, this is my destination and the source becomes me as the load balancer. Right. And the client is completely unaware of that when that one, when the backend receives that segment delivered to the app maybe goes to an able socket, you know, GRB see anything, right? What happens here is the backend responds back, but now the destination becomes the load balancer. The source becomes this that the load balancer takes this bucket. It knows that anything I receive from this connection as a layer follow basin, I have to send it back to this particular can. Actually, it knows there is a table that keeps and it keeps this mapping and so it has to respond back to the actual client and it puts the destination as the client and the response content and then sends it up. And this, this becomes a sticky thing all the time. As long as the connection you're sending, the same connection, it's always going to the same back in the second example where we're sending an actual HTP request here, like I'm saying, going to send and ADB get request here number one. Right, I'm the new slash one. I'm getting some API here. And let's say that these are actually maybe multiple segments. So I'm going to send two segments here. Right? So segment one of the segment to the load for layer four, BANSAL receives those segments and then maybe chooses to acknowledge them. But once it's acknowledge it, it just forward those segments directly to the, to the back end that it chooses right for that connection and that's it and it moves on. So if the client send another segment, it just write it back. So all of this goes to the same connection all the time as I receive segment. I don't look. What's the distinction? Oh, you want me to write this? Read, write, read, write. I don't read. I don't try to do anything. There's no buffering. There's nothing I read and write instead of the situation. Maybe some smart layer forward basin might need to do some buffering to take advantage of larger empty. You may be here. All right. Like if they're to use here one 1500 and then you hit zero 9000. Maybe it's advantages for the lawyer for load balancer to read, read, read, read and then write smaller segments, if you will. Right. So that's one one case that you can do that. But that's back to the performance, baby. We always really try to squeeze as much performance as possible and try to challenge everything. All right, so I sent another request to V2 on the same connection. Doesn't matter. Five, six, five, six goes always to the same back in. As long as you're sending me to the same connection. Right. And the second segment seven goes there. So just like. As you say. Right, right, right. Immediately go there. So now what happened? If I open a new connection, I send the request, get three right in this case. Well, since it's a new connection, the load balancing logic will be triggered. So on your connection, I have to choose a back end that second back and will be chosen based on the load balancing algorithm. It could be around or be at least, at least connected. Like who? Whoever is that least overwhelmed will be picked and that connections will be established. So in this case, I pick this, maybe I receive one segment, right? That segment I see the second segment. Right, this segment third. Right. It so just did the do did it. I don't even know what's in this. This guy does not know it. HDTV. It knows it's DCP at segment and that's it. It does not need to be. It doesn't know it's encrypted, doesn't know anything. Right. If this was a protocol buffers, it was gRPC, if it was web sockets, if it was my sequel connection. Postscript It does not matter because I treat this as pure TCP connection. And that's it. Is just segments to me. And that's the beauty of Layer four. Let me answer this lack of knowledge, this naivete, if you will, is critical and beautiful because it makes mixed layer for load balancer supports really any protocol you want because hey, is just data is just segments. So what's the pros and cons of this? The pros. Let's talk about the pros. It's a simple load balancing because it's literally doesn't do with the data. It doesn't really read the protocol, doesn't really understand what Layer seven content is. This a layer four support IP addresses? That's it. You know, and connections. It's so it's really efficient. There's no data lockup. It doesn't look at any data just like that. Bought IP addresses and sometimes it buffers and sometimes a bit depends on them to you. But most of the time just takes the segment writers to the vacuum. It's more secure because we're going to talk about this layer seven. Load balancing actually needs to read the content. And in order to read, to cache or to do pass or API gateway, it needs to actually decrypt the content. Right. Yeah. It has to decrypt the content to load bands that some people might not be comfortable with that there are bands that is a third party, right, but layer for don't have to decrypt because it's almost end to end in this case right now. Almost, of course. So it works with any protocol. That's beautiful, right? Because. Because it doesn't look at the content. It's agnostic, say, hey, you send me segments. I'm saying I'm sending it back to the server. It can work with one TCP connection in a NAT configuration where if, if, if your client immediately connects to the load balancer as its gateway, which is not very common. A It will the whole thing will be a one TCP connection to the back end immediately. All what the router will do or in this case load balancer is just rewrite the destination IP address to be the back end. Just just change the segment. Write the segment IP address, though. In this case, the IP packet IP address, the destination to be the destination IP address or maybe the port in this case as well to change the port. So it becomes really using pure Nat and we talked about this and then that's configuration, right? Collins hears about the bad thing about this. There is no smaller bands like because you're not looking at the data. You can make smart decisions. Right. So so maybe let's say you're sending a specific request. There are. We know there is. This is heavy consuming workload. Right. I know this particular request. If it goes to this path, slash, I don't know, load or slash, analyze. Right. As an API, this is consuming. I wanted to to move this to a specific back end. There are beefy, right? You can do this with land for load better because you don't know anything, right? You can play tricks. You can configure the layer for load balancer to have multiple IP addresses, not my multiple reports. And if you're connected to this port, that means you want this particular back. If you're connected to this port, this bada back. And so we play with port because port is invisible for the layer four. You can definitely do that and it's not applicable for microservices. We talked about this here. You would need this smarter logic, although you can only obviously use the idea of ports and IP addresses to do that is sticky, right? Bear connection. We talked about this, so all the segments that you send on the connection will always go to one server and only one connection. There is no load balancing per connection. And the reason is. Because I don't know what these segments mean. Right. They're just. If you send a gift request or you send a post request or you send three, get request layer for lot bands that doesn't know anything. Is sees these as a bunch of segments coming in. Right. So as a result, just has to deliver them to the back end. Right. And one back it goes. It can make an assumption. Layer seven smart enough it knows. So it can actually pick and choose which segments are sent. No caching, obviously. Right. Because because there is no because I don't know what's in there. I cannot cache it. You might say, okay, if I send the same segment over and over again, it all bands I can call it. Really? Because what does that segment mean? Right. Yeah, it's the same value. But what does this represent? I have no idea. Right. You can send something that hashes to a certain value, but it could mean something completely different, right? It's a protocol anywhere. So while this was an advantage, there's also a double edged sword. It's also a disadvantage because it can be dangerous. There is one case where a layer for a layer seven. Load balancer can be downgraded to layer for load balancer. If you sent an upgrade protocol initiative either as an upgrade method that upgrades the connection to either of sockets or is dipped to over clear text. Right. And if you do that because of sockets is that is really a special protocol. The easiest way to do is from moving from layer seven. You can just move to layer four to support any protocol. So if I want to support to absorb, let's just move it to layer four protocol and then treated as a Layer four. And as a result, the moment you treat a layer for any logic that you might have in the load balancer to let's say, okay, I want to block certain users, I want to block certain authentication methods, I want to block certain headers. You can't do any of that anymore, right? Because you used to have these rules when you were a lawyer, seven load balancer, but when you were downgraded to a layer for everything was allowed, were allowed. And that's a problem. Right? So that's one kernel of layer after layer seven load balancer, very similar. When you spin up a layer seven load balancer, it just it uses the same concept. The ECP connection to the back ends and just warms up, opens up as many as it can or as configured and then moves on. So now when a client actually connects. To a back end, right to that layer seven back in here. What happens here is, Amy, it becomes a protocol specific. It means like, okay, well, you're connected to me, but are you going to tell me what you are? What are you sending me here? You can just send me garbage. It needs to endorse, stand everything you send it. So any logical request that you send. Logical request. We're going to talk about what mean? What do you mean by a logical request? A logical request here will be buffered and we'll all try to understand it, and only then it will pass it and then make the decision to forward it to the vacuum. All right. So a request here at this request, let's take an example, is a basically the start of GDP slash, slash, right? One, one slash the the path, you know, blah, blah, blah. And then at the end as what the the the version believe there should be one one slash. Yeah, they should be version. Right. And then that begins because they should be and then the headers follows and then at the end of the day we add a bunch of new lines. That's the end of the request. That is when they load bonuses. All right, that's a request. Stop. Right. Let me choose which back into this. Right, this request to. So that request could be one segment, could be two, could be hundreds segments. So it needs to read, read, read, read, read, read, read and buffer and. Read the data, and if it's encrypted, it needs to decrypt the data. What does that mean? It means that to decode the data, you have to have a connection between you and the server and also a secure connection between you and the server. That means if you ever want to host your website, this certificate has to live in the Layer seven load balancer. A lot of people just don't like to do that because your private key has to live in the layer subject as always. How? How can you pretend to be Google or your website? Right. In this case or otherwise, how can you pretend to be your website? This this guy has to pretend to be your website, right? Because that's the final destination to your client. So you have to give it the certificate. Right. So I have to decrypt, has to read. And then as a result, Andras, then let's take an example. So something we send IP packet goes to a back end and then we send back a response and it goes this same thing here. It doesn't change, but let's say I'm again sending a bigger request here slash one and this is a bunch of a bunch of segment. So segment one is still part of the Get one, segment two and segment three. All of this combined will become the HDP, get one slower request, right? So what the LP does is just try to understand it buffers this data to understand the segments and then says, okay, one, two, three, up. There you go. There's a new line. Here's one request. Now let me take these guys and then write them one by one to the actual back end. So all these segments become one unit, one request unit, and all of them has to go to one back end and that's it. Right. So although you send me a request on this connection, three segments goes to this server ballistic. An example. I'm sending another request on this same connection five, six and seven segments. That that's fine. That's a completely distinct request. I read it. I understood why you want it to is stateless so I can pick another server and send five, six, seven to. So I will send I will write the segments five, six, seven that represent the request. And this guy is will will need to pass that SDP request in this case and understand it. This is, by the way, where most of the HDP smuggling attacks happen. You know, if you write about HDP, smuggling is an attack where where if those guys do not agree. Right. If the low band sat on their back and doesn't do not agree on where their requests start and where they request and bad things happen. You know, and this is outside of the realm of this. You can read more about this if you want, outside the scope of this lecture. Otherwise, this lecture was me 3 hours. But yet I can establish a new connection and the same thing. We can. That connection will use that same connection. Right. This is something we can do, by the way, in labor force load bands. Or we can talk about it once. When layer four. Once someone connects and. Reserves. That connection, that back end connection cannot be used for any of the clients. That's, I guess, another disadvantage, if you will, of the layer followed by is becomes a private they call it private connection sometimes. So that's a big disadvantage actually in layer four, right, because you can deplete your back end connections while they are for lot bands that actually nicely balanced through the connections. Right. That's powerful stuff, right? They're pros. So as there is a smaller balancing, there is actual load balancing happening here. We are efficient in using that connections on the back end we can cache because now I'm reading, I'm decrypting, I'm understanding, I can cash here, I can do a smart load balancing. Hey, if you're going to slash pictures, I can take you to this server. If you're going to slash comments, I can take you to this server. If you're going to slash post comment, this is a right heavy workload. Go to this server because it has a a specific database designed for this particular workload. If you want to analyze or do an overlap, you know, here, here is a, a, uh, a server that points to a database that is all specific, you know, such as a Sabiha now or post was got. Maria DB Right. So you can a column base storage, you can do all sorts of smart thing. So that's why it's GraphQL microservices API we're logic like we explain like you can API gateway authentication can happen everything can happen the load balancer as a result you can cache results night what's the cons? Obviously it's expensive, right? Why? Because it's doing more work. It's buffering. It's reading, it's decrypting. So it's more. It's more expensive. It's decrypting the content. So it terminates the address. That's why it's called TLC. Terminator like load balancer is almost called TLC Terminator nine, Layer seven. What are we ever we say layer seven it has to terminate connection it uses to RTC because actually I don't know if this is the cons really. I don't think so. But just there is a way that layer four can use one connection. So that's why I put it there. Is this like a disadvantage? And. But you can you can add that to the prose section here, where actually it's a very efficient use of connection. They are seven. Yeah. It must share that yellow certificate as kind of a commons as well, because some people don't like that. Right. It needs to cover buffer. So there is a performance results. Right. So you need to read re re re that the request understand that it was all the way then so that while the actual back end is waiting meanwhile. Right. So if there is like. The load balancer becomes a bottleneck of this. It's buffering. A lot of request is doing at the same time. It can slow down. And the problem here, this is the major one. It needs to understand the protocol, why there are why there is like requests going all the time, like people actual rescue work with the forecasting features from a proxy, an engine to say hey, please support socket. Oh, guys, can you support you are busy. Oh, guys, can you support. I don't know. With this protocol can usable PostgreSQL. Why? Because of. Because we need to do layer seven load balancing. You cannot do layer seven load balancing if the load balancer doesn't understand the protocol. Right. Because we are. Passing the data we're reading as a result. If we don't understand how to talk to the back end and receive data, we're going to, you know, we're done. That's why load layers have a lot better. They need to understand the protocols layer for in advance. In this case can be used. If you if you don't have basically if your protocol cannot be interpreted. Right. So in summary, so we talked about layer four members as layer seven. Right? We talked about now what the load bands are. It's just basically a glorified reverse proxy that's a smart enough to do actual load balancing. You know, we talk about layer four load balancing project goals, we talk about layer seven load balancing and the pros and cons. There is no right or wrong. There is a case for every load balancer. Guys, hope you enjoy this lecture and I see you on the next one. Enjoy the course.


### Network Access Control to Database Servers vtt

One of the common mistakes that back in engineers and database engineers that lack networking knowledge do is expose their database instances to every possible connection. That's really bad. If you're network engineer, you know this is lesson one. You have to do this. But that's part of the bridge that I keep, you know, yapping about. Right. And this course, this is the bridge of the understanding that, okay, when I spun up a false cross database, when I opened up a MongoDB instance, when I spin up this particular server, a database who can connect to it by default, we don't we never ask the question. I never asked this question a few years back, you know, ten, 80 years ago or now. I never asked this question because I was I was busy writing code. I was busy building application. I was like, I'm not yeah, this is a database. Hey, I can connect. Sure, that's the best thing ever. But the mistake here is, like, just exposing. The port. For anyone to connect is a major big no no. We see articles every day o Elasticsearch database leaked with tens of millions of users. I don't know. Pizza Hut database leaked. What? A MongoDB 700 MongoDB instance as well as file and terabytes of data want it downloaded because the more you can connect to them is you can start guessing the password and yeah, the database will. We'll try to prevent you from connecting, but with a few tries you can get in, so why not prevent it from the first place? And that's what we're going to understand today. We're going to take exemple at the post gris configuration. Definitely one of the best here to understand how you can allow certain connections from certain IP addresses but prevent others. This kind of loops back to the IP section. So if you didn't watch that, go watch the IP section before jumping to that. Let's jump into it. All right. So if you have ever spun up Applescript database, which I spun up a lot in my database course, so check that out if you're interested. But the PostgreSQL comes with a lot of configuration. One of those configuration is this this is it, PG, MBA, and this allows you to configure the unique roles to the connections specifically. So it has a bunch of headers here and I'm really interested in this is this is the networking course. I'm really interested only in the address and that's pretty much it, right? The rest of the stuff is just database specific and we can go through them, but they're not really important for this course. So what we want to do is like go through these and kind of try to understand the different configurations here. How would we jump into it? So the first one, by default, it's a local connection only. And don't confuse this local connection with loopback because it's slightly different. So this is the IPC or UNIX domain configuration where a process can talk to all script process directly without any networking stack. So no TCP is involved. You immediately talk to the process directly if you know the port, right? So you don't even go to the loopback or 12700 is just direct connection. So it's way faster. So if you can get it right and if you're in the same process, if you can guarantee that you are in the same now process in the same machine, and you can talk to the boss because you can do that. But writing an app that can do that call fast can be a little bit challenging to get. Right, right. So but that's by default. So it's allowed if you're running the same. Uh. Host You can write an ad that talks to the process directly, but that's not what we're here for. We're here to talk about the different configuration with the comes to IP addresses. So the second layer is allow another app to talk to the TCP stack to the IP loopback IP address, which is this particular puppet, right? So this kind of hard code, 127.020.1 right slash 32 means hey, I want to apply the mess to all those five things, right? To all their four pairs, which means effectively this exact IP address can connect to me. What this does is like, okay, the 127.0.01, which is the loopback address for the IPv4 slash 32 of them is the subnet mask is 32 bit and means all the bits effectively. So the hub and message everything. So you have to match exactly 127.0.0.1. So of 127.0.0.3 tries to connect that will fail the connection. So it has to be 127.0.0.1 exactly that address to be able to connect so that local specific address. And there is another configuration here that allows us to configure, say if you don't like this view, for some reason you can specify the IP mask. And we talked about this, right? This is equivalent to this. You can specify the IP math. I say you're not comfortable with this classless, you know, approach. You can specify the IP mask and we're going to you to understand that this is very critical to understand the IP math. Now, the local is not really important here, but if your database has no business to be exposed from anyone, not in the same network, not outside of Internet, no one could connect to except the machine that you're currently on where POSCO serves. Then definitely add the online 127.0201 and will allow you to connect this particular connect communication. Now, if you have an IPV six interface, which most of our community configurations have, you can add that call and call in one which represent exactly like this. But this is the IPV six version 128 because we have 128 bit and the IPV six word, you know, and that will allow IPV six connections effectively, right? So if somehow the connection that you made came from IPv4 in this case and you only allow IPV six, it will fail. I don't, I can't think of a reason. Why would you want allow both? Maybe will more security, but whatever. So that's another example here. Local Host This should cover both the local host string and really depends. Local Host The string can resolve to an IPV six which is local host string can resolve to 127001 or can resolve also to call in calling one, which means the IPV six version. So this should cover both in this case, right? And so if you want to be safe, you can just use localhost in this case. And here's one of an interesting thing. So most of the time when you connect to a database, it's almost never right in the same, you know, your application is not in the same machine as the. Database. Right? So you need to allow that user. So what you can do is effectively. All right, I wish they had an example, but you can just type in the address of that machine. Let's say you are on a network and you're in a developer network and you built a post. You spin up a post bias database. Right. And in the configuration, you only want to allow one host and that is your development machine to connect to them. You can hard core the IP address of that machine right here. But we know that sometimes the IP address change, right. Because the if you will, assigning another one sometimes when you boot up, especially if you're are an organization at work. So what you do is you can do this, you can use the subnet concept here, right? If you know your subnet and that your subnet should not really change on a regular basis, it should stay static. Right. And this after configuration was done, if you know your subnet, then you can allow only your local subnet. Let's say you're in the engineer department and there is like a marketing department that is the sales department and everyone will have different subnet. You can check this by doing IP config or if something goes off mac and the next I think and that will give you that the subnet mask if you put that subnet mask right here. So you can put your IP address and slash 24 or use that approach in this case two five, five, two, five, five or it's not going to be 25525525525. It was going to be something different. But we actually show that. So let's actually show this, right. So if I'm I'll go to if config in my Mac I think in one does its IP config. Right. And I have so many interfaces because I have Docker and still there. So much garbage left done right. So what are we going to do here? Look for my IPv4 address, which is this? This is my IPv4 1921682 2250 42144. This is my subnet mask and obviously they try to be fancy and show it in hexadecimal. So this is effectively 255255255.0. Right. So either right you can do slash 24 because like 888, right? So you can just slash 24 by doing 192168. 254 dot the dot zero slash 24. And let's go back to them. Right. You can do this. So if I am if I want to allow my machine in this case, I'm going to say one nine, two and six once again to 250 followed zero slash 24 in my case. So any IP address, right, for this particular situation, 93 two from zero one, two, three, four, four up to 255 is allowed to connect. If you want to allow only one user, we can do this right. Slash to 32. So that's kind of a trick. You can either remove that altogether. I believe you should be able to. Or you can just do this. Less than 82. 32 means I have to match identically. Right. All of it. So you can put your hardcoded IP address. So. So here you would do this in a configuration where you have a developer machine and only that should access the Posco's database and you want to be secure. It's always a good practice to do that. You know. If you don't care, right? If you're if you're in an local network, you can just replace it with actually saying all and everybody will connect. Bad idea. Very, very bad idea. I know in some of my tutorials I did that, but I was using Docker and some local and I destroyed the Docker database. It literally had nothing but never use all obviously. And I always say that and all my examples never, never, never use all I. So you have to specify in a production environment you have you have to think about, okay, who can connect and be pragmatic. It's like I should I allow all these guys to connect? Should I also this and test and verify you can allow specific domains even, right? So you can play with that and then different configurations here. So it is another thing, what if you don't want to allow it's like you want to actually allow everyone except certain people. I don't know why it would do that, but you can do this. You can specify IP address and reject that IP address, slash 32. In this case, it will match all the whole thing. Right. So in this particular case, let's take a quiz, if you will. What does this mean, guys? Man, let's zoom in so you can see this mean allow everyone slash 16 is the first to buy, right? It's bit eight bit. So 1816. Right. I mean, this remains the same. This can be anything else. So this is a large subnet if you think about. Right. So this means let's say you have a subnet 1.0 and a substituted zero, you're allowing everyone effectively an all the on all the companies like my company, we use the ten zero zero approach, but we have so many subnets and we have so many departments. So I really need to be very, uh, you know, digital and I don't know if that's all right. So it's like who can connect to me as a database, right? So you can control this. But I thought this was a very interesting kind of video to make and to show you guys how this is working. Right. It's it's very critical. Any database, the this is like like 101 DB 101. If you spent a database, let only certain host connect to it, right? Because you don't want anyone to connect to it because there was a good many mistakes were okay I'm just testing. Yeah, yeah, yeah. Ole, ole, ole ole ole. And then allow everybody I'm just testing is loyal local my database I don't care but then you start building the app and you forget right and you say, well this is the configurations I used. Okay, let's take it as is. Could you both use limit on a production and expose it to a public IP errors? And since the database you said all it's going to start listening on all interfaces. That's the first bad thing, right? So that's another configuration that we can control as well. It's like, what do you what what do you want to listen to? Let's find that, actually. There you go. I found it. It's called the listen addresses. Right. So the Listen addresses, that's another configuration you have to play with, like with a port, which is five, four, three, two by default that the pulse goes for. But you can specify what addresses you want to listen on. And if I default, let's write the three through this. Specify the TCP IP addresses on which the server is to listen for connections from the client application. The value takes the form of a comma separated list hostnames a numeric so the special and three asterisk correspond to all available interfaces. And obviously you don't want that. We we talked about this right in the lesson lecture 000 the same thing column column means all IPV six addresses, right? So you need to actually specify what you want to specify here. So yeah, you can play with basically the listen addresses and specify exactly what you want to listen to because again, we don't want a malicious connections to be randomly connected to us, right? But even that right Postgres allows us to control who can connect you even if you listen to a public IP address accidentally. So yeah, 000 might be, I don't know, the default here. Right. And you're going to listen to everything, but you can choose who can connect to you. So you can you can you can control both parameters. What post will listen to as a server. Right. And the port. And then finally and finally who can connect you using there should be a configuration that you can specify here so you can see it databases very, very as a summary. Right. And it was a very critical piece of resource. So nobody can just cannot do it. You really need a secure way of blocking connections that that will never be able to connect the right. And I understand yet connecting to a database use W.A.S.P. connection is kind of different than actually connecting with a username and password and excuse me query, but the moment you have a connection, that's step one and step one takes a long time. And step one, you don't want the hacker to reach step one. I block it as a start because the moment they step one, they can start guessing their passwords and username. And boy, people just use random same password all the time. They both go slash postgres, right? Right. So that was like a elected, I thought, very valuable for network engineering for effective back end because it's really something I see all the time, all the new ways, every time all this data was leaked, this database leak. Why? Because of this. So I believe this is a critical and just I want to listen on and just who can connect you. And now you know the IP protocol. You can you you can eat and drink. This stuff is so simple, so elegant. See in the next one. Enjoy.


## Network Routing


### Fundamentals of Network Routing vtt

This video is a crash course about network routing from zero. So routing is a very critical concept in networking, but unfortunately it's not very well explained because there are so many layers underneath routing that we need to understand in order to understand the concept of routing. Basically how routing coming from the internet, you know, how does it happen? How what what does it mean if I send an HTTP request from one machine to another machine, how does it make it all the way to the end? Uh, unfortunately, we either can have very high level understanding about, you know, there is some magic going on. So I send a packet and it makes it all the way there. Or we have very low level understanding to one machine, a single machine to another machine Communication, but we're missing so much in between. And my goal from this video is to fill up this gap as much as possible. So how about we try? All right, let's get started. So routing explained and I put a screenshot of this famous thing that's called the routing table, which will be the last thing we explain in this video. But to get there we need to understand so so much stuff beginning from the concept of data links. What is a data link? See forget about anything you know about networking and see. Just take one simple scenario where you have two machines directly linked to each other, or multiple machines directly linked to each other. I have here a switch. Think of it as a hub. Think of it as just a bunch of cables, you know, wired together. You just spliced literally the the Ethernet cable and just you literally link them together. And these are linked together. Forget about routers, forget about switches, forget about all these concepts and say, how can I just send a message? And we're going to define what a message is between a to see how can I build something like that. Right. How can I build something like that. So this is the original original original requirement networking. And I'm not going to bore you about the history, but I want you to put yourself as the designer of this thing that, hey, I want to build a network. I want to two computers to connect to each other and communicate. So what they built is that. All right, let's have each machine have a unique address. And we're going to call this the media access control. And this address is the way we communicate between machines. Problem solved. Right. We have a unique address. And using this unique address I will know the source and I'll know the destination and I'll just send it. If the machine happened to have the the address, it will accept the message, right? If it will not, it will just drop it as simple as it is. Right. And this is what we have today. We have it. And this is how things work today. At the very low level, what we call the link, the data link. So each device has what we call a link address or a mac address. And we built this and it works perfectly okay. So devices that are directly reachable if you have a device. And if I send something here and I say hey send this to A and set this to B and I am A, that message will go here and it will be replicated. Because how do we send things on the wire. Really it's either electric pulses you know if it's Ethernet or radio, if it's Wi-Fi or light, if it's fiber. Right. Eventually we serialize, if you will, these bytes that we understand into signals. That is the physical layer one, right, if you will. So we send this and eventually assuming the very simple representation where I'm sending it to be right, this machine will get it. And also this machine will get it. But we have logic at the server if you will, that says, hey, if you received this message or let's call it a frame, we're going to introduce new terminologies and abstractions. If you receive it and it's not destined to you, you don't have this Mac address, then just drop it. And that is the logic we have today. So this message will this machine will get it, but it will drop it because it's not be right, but this will accept it and it will just bubble it up to the higher level. Whoever requested this message, be it a process or a thread or something higher up, and we call these layer two frames. Layer one is the physical layer light electric radio. And then layer two is this logical link layer right. So data links a very very important things to understand. Okay so this is silly right. This is silly. Why. The reason this is silly is because well what if I have ten machines connected to each other using this this thing. Right. This wire hub device. I can just send stuff all together, right? I cannot just be sending things, sending a frame to everyone just to. For them to discard it. So we need some intelligence, right? This used to be a called a hub. Hub is a dumb device that just. Just sends most of the stuff and it doesn't have any intelligence. Switch, on the other hand, says. Has some level of intelligence based on the port that you're connected to. So what happens here is now we we introduce some level of intelligence. Right. Let's forget about IP addresses here. I'm not gonna I'm not talking about IP addresses or any of this stuff right here. Just direct link. So what happened here is this machine C and this machine B and this machine A they all connected to the switch on a given port. Right. I know my drawing is not perfect here, but there is a port here that you plugged in this. And there's a port here that you plugged in here. And this was another port that you plugged in here. So the switch knows that port one. There's someone connected to me. Port two, there is someone connected to me. There is a port three. There is someone connected to me. So what I will do is the moment the machine connects to something, it will advertise its presence as. Hey, it's me. Hey, this is me. I'm a. So the switch will record this fact that. Okay, P1 is actually a and P2 port two is B and port three is C. That is such a beautiful uh, if you will, if you took my database course, you'll understand that it's always about elimination when it comes to working with large amount of data. And in networking is not a different thing because you have large amount of machines. And I want to eliminate scanning as much as possible. Right. So what we do for performance, we remember. So now if I, if A sends something to B right. The switch gets it and says well this is going to be I know where B is, so I'm only going to send this. And by send I literally replicate the electric pulse to only this port. I don't need to do anything to port 3 or 4 or 5 or 6 or 7 or 8. And that's the beauty of this, right? So I just send this across the network and it just receives it. So that's the first optimization with data links. Keep this in mind. This is going to happen all across this presentation right. So the switch of course receives it and the switch knows where. Be at it's port two and it will forward the frame to P2. But here's where things break you guys. Things break. What do you have? Let's assume we have thousands of machines. And I just created this example. Right. You have these thousands of machines, thousands and thousands and thousands of machines that are connected to each other. Right? Links does not scale at that level. Why? And the reason is because the concept of Mac address is the problem. You see, Mac addresses are random. So the problem with Mac addresses are they are random and they're random. There is no logic when it comes to them. So how do I find how does A talks to z? Assuming z z z is another Mac address? How does that work? Well, we're going to do the same thing. Z will talk to this switch and it will it will present its knowledge. But that switch doesn't know about this right. So it needs to somehow that knowledge was not propagated all the way. So how does that how does this guy know that Z is over here? There is either you can propagate the knowledge of that. Zee. Zee is actually on switch seven all the way on that network, which can be very expensive. Otherwise you just scan, say, hey, I want to send to oh well where is this? Oh I don't know. Just send it. Right. The switch doesn't know. This switch doesn't have that entry because z z z is never on these direct links. Right. So it will just say all right I don't know. So I'll send it here I'll send it here I'll send it here. Right. It will just send it to everybody because it doesn't know. And this machine will get it. And then it will send it to this, and it will send it to this. And maybe at this particular point, we'll never make it at all. These machines have to be programmed to forward the frames. And you can see that this immediately dies. This concept of links, when it works with a lot of machines, it immediately because we have to scan the entire network to find the one we're looking for. Okay, so what we did is we built a new addressing system and meet the internet protocol. I like to think of an of of an IP address is very similar to an index in databases because it, it has this concept of a network and it's that has the concept of a host, it has a host and a network. And the network part is the, the key for the elimination. So that I know that. Oh, which network you are in. All right. If you're in this network then you can skip all other thousands and thousands of network because you are in this network. Right. While the Mac address we don't have, we are just one big global network when it comes to Mac. And then we're scanning all of this Network. And that's the problem with the Mac address. So we brought the internet protocol and we're going to talk only about IPv4. IPv6 is literally more bytes, but we have four bytes to work with when it comes to the IP address. We have the network and we have the host. The host identifies the actual machine and the network identifies the network. So an example here is I have out of the four bytes which is 32 bits, I want 24 bits for the network and eight bit for the host. And here's the trick here. We always go back to the link. We still need the link addresses. We still need the Mac address. We cannot do anything without those. What we do is all hosts within the same network must use link addresses. All the hosts that are within the same network, you have to communicate with the link address. And that's the trick here if you are in this in my network, if I have two devices within the same network, then you can directly talk to them and directly means this. This is what directly mean. You mean go use the Mac address to communicate? Well, we have our first problem now. Well I have now the IP address and I want to talk to this machine, but I don't have the Mac. Right. I don't have the address the link address. So we need some sort of a translation layer going on. You will find this pattern everywhere in software engineering. I know the domain name, the DNS, but I don't know the IP. So we translate DNS to an IP. I know the IP but I don't know the Mac. So we translate the IP to a mac right. And there might be other higher level translations you'll find you're always translating from one name system to another name system. It's always like that. Okay, so how do I check if an IP is in my network? Because we asked this question before. Right. Okay. I have one IP and this is another IP. How do I know if this IP is in my network. Well we need what we call the subnet mask or the network mask. This tells us that hey this is the mask you need to apply, which is nothing but a bitwise operation that you apply to an IP address. And it will tell you it will just be basically zero out the host part, and it will give you the actual network. An example is 192168.1.4, is in the network of 192.168.1. slash 24. Slash 24 means the first 24 bit is the network or you can use this format. 2552255225520 which is 11111111. And the last byte really is all zeros. So what you do is just you and them together and you get this. So question is this and this. Uh, if you do ten 006 and 255255255, you get this network. This is not the same network. Right. So you can do this check quickly on any IP address to find out if they are in your network or not. But you need the subnet mask which tells you exactly how to find it. How do you know what your network is? Essentially right? That's the trick. Okay, so here's an example. We have ten 002 ten 006. And we have still I'm not introducing a router yet. We'll talk to that ten 006. Ten 002. This and this. So knowing these network, these are in the same network because I know that this is slash 24. Slash 24. So I can talk to to them. But these guys they are not in my network. So I cannot talk to them using link directly. So now that we have an IP address, assuming I am in my the IP, I want to talk to like this guy. I want to talk to this. These two machines want to talk to each other. Well, now you only have the IP address as a form of communication. You no longer need to memorize Mac addresses which are longer, long, even more random. You have IP addresses. So to communicate with IP addresses, well, you still need the Mac because now you need to translate the IP back to a mac address. Well, how do you do that? Meet ARP Address Resolution Protocol, which converts the IP address to a Mac address. How? Nothing magic. Here. We just ask another layer two question. Say, hey, broadcast. If it's actually Fdfdfdfdfdfdff six or more 12 FS, you just send send it say hey, who has this IP address? 192168.115. It will the the switch will get sent. All right. Since this is a broadcast I'm going to send it here. I'm going to send it here I'm going to send it here. Go send it everywhere. You just send it everywhere. And whoever has it will say, oh, that's actually me. So what the the device checks is, well, I received a broadcast message, so I need to read it. So you read them? Because it's a broadcast. Everybody should read it. It literally means for everybody. You get this and you see an IP address. So this IP address, hey, it's actually me. That is my IP address. It's five. So what you do is you reply back and say, hey, by the way, 192168.11.5 is actually me, right? And this is my Mac. And then boom, you reply back. And now you just link. You update some sort of a table. Whoever asked this, which is this guy? Create update some sort of a table says all right, all right. Got it. 19216815 is actually be. Now you just turn around and then send that message the first like like we talked about in the first slide knowing the Mac address you just send a message. Now I want you to think about this as I'm talking. I'm just moving from the bottom up, right. Building slowly. Like think of how a back end application responds to a front end, or how a front end sends a Get request, for example. Now, if you go to any of your Linux machines, if you do IP neighbor show, you can see the actual ARP table which links I say this is the. This IP address is from this. Net network address. Network interface has this Mac address. Here's one note that is very important to understand with ARP. ARP only works on the same subnet. It does not work on a different subnet. Like you can try to ask like hey 192168.14. If it tries to ask for ten 002 Mac, it will not even send the packet because that that's silly. It's just like, hey, this is not in my subnet. I'm not going to ask it. Now this is how ARP has been designed. So the implementation of ARB in the client side. Does that check? And it will never send the ARP request to begin with, right? So now this begs the question how do we communicate with different networks? In this particular case, right. If I can't talk to someone outside of my network directly using Lync, then how do I talk to it? Well, meet the gateway. Meet the gateway. You always need a gateway as a proxy between two networks to proxy you to another network. So if you have any other network, you need this router, this gateway to route your packets to different network. This is exactly how the entire internet works. The entire internet just is an expansion of this. But as just like slowly between different networks, Is just this. Just take it and zoom it out. That is the entire internet. How it's based on. It's just the concept of a gateway. You to talk to out something outside me. I send it to the gateway, right? Or I send it to a gateway. It doesn't have to be one gateway. And we're going to talk about this in a minute as well. Okay. So that's very important to understand you guys. Very critical. So now the gateway has many names. Gateway sometimes it's called next hop sometimes called the router. But essentially it's it's if you don't know where this packet goes should go it's outside your network. Then you just send it to the gateway. And your gateway must be in your network. It must be. It has to have a link Direct link to the gateway. Right. And this is a classic example where you have a router between two networks. Right. So there is some sort of a let's say you have a I don't know, this router happen to have two routers, just another machine. Right. It's just not really magic right. It doesn't have to be look like a router. You can have a machine and make it a router and literally have two network cards, one configured to be this IP address and another one that has another IP address, another network. So this and because each network is assigned a mac address, that is how we uniquely identifies the link address of every single machine. Right. So in this particular case the router has two link addresses z and x. And the gateway belongs to both network in this particular case. And the next thing is, you might. You might say, Hussein, you said that. Okay. If I don't know if this machine is not in my network, then I have to send it to the gateway. How do I know my gateway? This is also configuration. This is part of the. You can have it hard coded as part of your configuration. Or you can have another protocol that configure you something called DHCP, not in the topic of today's presentation, but assume you just know. You can look at your network and you'll find that there is always a default gateway. So in this particular case this is my gateway. This machine is called a. This is the IP address 1.4. This machine is called B 1.5. And this is the gateway. There is a switch here. And this this router has this IP address. And this router also has another IP address. Another network ten 001Z. And this machine default gateway is Z ten 001. This machine is ten 001. And see. So let's just take an example where everybody just identifies each other. Everybody is connected to the network. And then just like hey, it's me. And when we do that the same logic still applies. Hey this is a so port one on the switch right. This is a different switch than this right. So this machine goes and says, all right this is a so port one is now marked as a. This is B port two is marked as B. And now router also does the same thing right. Because it's just another machine. This is X from this side. Remember now port three is x from the other side. The router also broadcasts its presence on the other network. Think of it like you almost like have two network cards and two Ethernet plug in on each, right? One one Ethernet are plugged in on each router. Okay. So now the other hand Z broadcast itself. Now port one has Z right. This is a different switch. Of course there's port different port one and D is this guy and Z is this guy. All right. Everybody's connected. Good. Let's take some examples. Now let's take an example where we're talking about link networking example. Right. So 192.168.1.4 wants to talk to 192.168.1.5. It only knows the IP address. And how does it know the IP address. Well someone in the code wrote hey I want to talk to 192168115. Well, how did or maybe you did a DNS and you found the IP address which is a host name hostname Him of the machine always translates to one or more IP addresses. This is not a DNS topic, but it's a different beast DNS. Well, what do we do? Well, is is 1.5 is the same as my network? Well let's check what's my subnet mask. Oh this is my subnet mask okay. Let's apply it. And and I get 192168.1.0 which is the same network as me. Because if I apply this one four on the same network, I get the same as I same network. So they are in the same network. And if this is the same network, I'm allowed to ARP, I am allowed to ARP, and I'm allowed to talk directly to it because it's in my same subnet. So we do we talk directly and then what do we do. We just ask for them, hey, what's my Mac? We do an ARP and we send a broadcast says, hey, who has that one? 1.5 because I know I am. I'm supposed to talk to it directly, but I have no idea what the Mac address is. So we send this, uh, the router gets it right because again, we don't know. Right. It's a broadcast. So we send it to this, the router gets it, the router will say, no, that's not me. The switch also sends it to another port. It says this gets it. It says oh yeah this is actually me. Let me reply back. Now. We reply back 192115 replies and says, hey, it's actually be the switch gets it. The destination is where a because whoever sent it before right. Whoever sent it was a so it just flips it and says, all right, send this to a please. So the switch knows that oh A is actually on P1. So it only sends it on P1 on the port P1. So why did it send it to this. Well because the switch is smart. It knows that. Hey you want to go to A? Only A is on this guy on this porch. I'm only going to send it here, and then we just resume the communication. What is the communication here? Well, here's what we do. Well, the frame we talked about the concept of the frame, the layer two frame, which has the destination Mac. Now that I know it's B, the source Mac, which is me. Now, what we put in it is, is another layer three packet, which we didn't talk about before, which is called the IP packet, which has a different destination address and a different source address. The source address in this case is the IP address. And the destination address is the destination IP address. So I want to go to 192168.1.5 and 192168.114, and that in itself has data in it. And it becomes like this matryoshka doll, the Russian doll thing, if you if you've ever seen it before. So like a doll inside. Nested inside another doll inside another inside another doll. So this IP packet you put your data inside which like in this case of a TCP, you put a different TCP segment inside this IP packet which has a destination port and a source port, which then you put your HTTP protocol or your other SSH protocol or any other on top of TCP, right. So it's just like layers on top of layers, but the IP is only the only thing you need to understand. Anything on top of that is just data quote unquote data. So now we just throw this and say, all right, go there, send it to this destination. Again, we're looking at the frame. Now forget that we have the data IP here. We send it to be this which is smart. It says all right you're going to be I'm only going to send it to port two. Good. Be gets it and says all right this is for me. And I'm going to unpack the frame. And I found an IP packet. And this IP packet is actually in fact me 192168.1.5. That is me. So it will just unpack it. And whatever the instruction has in the IP packet, whether it's TCP or UDP or something else, it will just deliver it. And that's a whole different discussion. I talk about that in detail in my operating system course. Like the kernel gets involved here and all sorts of things happen, right? That is if you want connections. And we have, you know, TCP stack, TCP IP stack kicks in and does a lot of stuff. We're talking about just the routing aspect. We and we achieved it. We got it. Now let's talk about inter network communication or inter network internet is just the whole thing. Inter network is like I want to talk about two machines. Two different machines. How do I do that? So 192168.1.5 I want to talk to ten 0002. Well, I can do ARP because these are not in the same network. There's no direct link. There is no link directly. I'm not allowed to because technically it's not in the same network. And that's the that's the rule that we made, guys. We made a rule that if you want to ARP and you want to use direct link, i.e. Mac address, you have to be in the same network. That's the rule. Who made the rule? Whoever designed the ARP, can you break it? You go build your own stuff. You don't have to use any of this stuff. By the way, guys always have this open minded when you design stuff, when you've used this is what what people built. Go and build your own thing. If you don't trust in any of the build, if you're building like something local, you don't even need to use IP. Just use the Mac addresses and build your own frame. You probably build a much more efficient and networking solution that way. Right. So keep that in mind. This is this is what we're doing today. You can trash all of this scrap and build your own. Right. Always have that as a backup. Okay. Okay. So now not on my network, which is true, right? This thing is not the same network. So what do I do? We're going to talk about the concept of default gateway. Default route. If you don't know where to send this, send it to the default gateway. And we're going to learn later that are multiple routes not necessarily default but the default here says, hey, if you don't know what to do, send it to this 1.1.1. That's your default gateway. And if there is no nothing configured at default gateway. You're stuck. But yeah, this. This guy will know. Might know. He will know. Might know. Right? So, hey, go ahead and do that. So. Well, well, how do we do that though? Same exact thing. Well, because I don't know ten 002. Yeah, I don't know that. But I can talk to 11921011.1. I'm supposed to talk to this, but who is this? Yes, he is in the same network as me. That means I can ARP. That means I need to ARP to talk to the router. So even to talk to the router itself, that's also an ARP. And this is where attacks can happen. Like ARP poisoning attacks can happen. Someone can pretend to be the router. Right. So what we do is like all right who has .1.1. Same story right. The router gets it. This guy gets it. he says, no, that's not me. And if there's a hacker, he will say, actually, yeah, wink wink, this is actually me. And it will send the Mac address pretending to be the router. Right. And then you essentially you would be talking to the to the wrong router. And that's how ARP poisoning happens if you're interested. But yeah I know the gateway. Now I know the IP address of the gateway. So the Mac address of the gateway it's X right. So now we answered the gateway answers that hey .1.1 is actually x. And then we just were ready to send the actual data. So what we do now is we create a frame destined to X which is my gateway. But the IP packet is a different address. And boy this is the whole power here. The Mac address changes but the IP remains, you go to zero two, which is where we want to go, and you say, hey, I'm .14. I just shorten it a little bit because it's very hard to type in. 192.168.1.4 and you say say go, go go, go. So because we're going to X, the the switch is smart. It will send it only on port three and it will get it. The router will get it and it says all right. Oh that is actually me. From a layer two perspective, this frame is destined to me as a router. As a machine. Router is just a machine. Right. And it's just all right. It's it's all excited that our gateway is all excited. Says, oh I got something. So it cracks open the frame and it opens it. Um, the doll inside. That's not for you, my friend. This is not for you. This is for zero 210 002 is like what? That's not I don't have any IP address ten 002. But I'm a router. I'm a Linux machine that has enabled the bit that is called IP forwarding, right? There is literally a bit called IP forward equal true. If you set that to true, your machine becomes a router. And it will just say instead of checking your local interfaces for the IP address of that you receive, you will actually be configured to repeat the same IP packet into all your network interfaces or or the network interfaces that matches what you have. And that's what the router does. So what you're going to do this is this frame is for me x. But the IP packet is not for me. But I am a router. That means I need to forward this which you can enable if you want on any machine. And that says oh ten 002. What is this? Well, I have another network that happened to be in the same network as the thing that you're trying to go. Ten 002 is my ten 001. It's the same network, right? So the router does that. All right. The best network to send it to is this network is this Ethernet cable. Is this Wi-Fi. And I'm just like all right let me do the magic here. So it does forwards the same IP packet on the other hand. But wait a minute. As we said in the normal situation this this this is just dropped right. Nothing. There is nothing there for me. Right. But the host kernel has enabled IP forwarding in this particular case for this router. So you receive the packet and you will just forward it on the other interface that happens to match the IP address that you're going to. And this is all right. But we cannot just forward an IP packet naked. That doesn't make any sense. We need to what do we do. We need to we need to go to somewhere. We need a mac address. What's the Mac address of ten 002? I can ask this question. The router can ask this ARP question. Why? Because the router is does belong to the same network as the machine. As the machine we're trying to connect to ten 002. Who has this ARP. And we just do the same thing. Same exact dance. Do an ARP everyone. Hey who has ten 002 everyone? The router probably has the value cached, but let's let's entertain that. So the router now will set its own Mac address which is Z not x. That's a different Mac different network. Right. So it says all right whoever have it please reply to me. Of course this guy gets it. This guy gets it and will reply back and says all right it's actually D. Now we know that ten 002 is actually D. And we reply and we go, Whoa! Let's go, let's go, let's go. This is continue. Now I can encapsulate this IP packet that this guy sent, and I put the destination as D because I know and the source as Z. Okay. And I just send it and it will just arrive. And that's how we do routing across networks. We kept the source IP address in this particular case. And I want you to be careful with this. The IP address the source we kept, we didn't change it. There are some cases where you absolutely need to change it. Like if you're going to the internet you cannot send like my IP address say ten 002 right in my laptop. And I want to talk to Google. You cannot just the router cannot just send the source IP and keep it on the internet as ten 002 because Google have no idea what ten 002. It's a private IP address. Right? And we don't talk about it, but there are like sets of known, well known private IP addresses. 192168. x dot x ten zero zero 24. That's also private and also 172. 168I think something like that. These are private addresses. So in this particular case the network router needs to change the source IP. And that's called network address translation. I have many videos about that topic right. But here we keep it as is. The machine receives it. It says all right thank you I actually got it. It's actually for me the Mac addresses for me D. And the IP address is also for me. And the kernel just delivers it to the application. And I talk about this in my network. Sorry the OS course if you're interested in that. So how do I reply. I want to reply. Up until here you probably can finish the story. How do we reply? Well, I just changed the IP address. Now it flips, right? I want to talk to .14, which is this guy 192168.1.4. And my source IP is ten 002. Same story. Well, 110 002 said well this is not my network, so I need to talk to my router. And so it talks to this gateway and it just like say all right. Uh I don't know. So I'll talk to Z which is my gateway. It says, all right, hey Z, tell me how to get deliver this for me. And it will just send it here and the router will send it. Will does its own thing here. Uh, does our ARP found out that it's actually b oh, did I, I think I messed up here. Yeah. That's wrong. This should be should this should go here. All right. So I just fixed it. So we're going to one four Okay. Not this guy. This guy. So B is the correct one. We're coming from X. All of this stuff is same story, right? You can find it C going this way. Go to Z. Then I do the IP forwarding but on to another network. Now I have to change my Mac as a as the router because the source Mac is now X and the destination is B. Assuming I have this ARP cache and I go and then we receive it over here. But here's here's an interesting thing guys. You technically you can have multiple gateways. No I mean it's not usual in normal home setups to have multiple gateways. You only have one gateway. And that that takes you to the internet. But in the internet there is always multiple gateways like like take, tik, tik tik, Verizon or or or frontier or these ISPs, you know, each of these have network of different routers and each router has like different subnets. And each one has like a you can go to this subnet. And this subnet is reserved for this particular ISP. And the rest is just the same really. So you can have multiple gateways with one default gateway. Things doesn't always work. We need this granularity. We need the ability to send like well if I'm going here then go to this machine or this router. If you're going to this site then go to this because they they are connected differently, right? You cannot have just one gateway. And that's why the routing table was invented to solve this problem and make it just a generic way. There is no more just default gateway. There is a gateway, and if you want to go here, then go through this guy. So let's explain how this works. Let's add more networks. And I'm going to disappear for a second because I'm adding a different network. Another private one by the way. Right. 172.16.6.2 Mac address M. This is its default gateway. .6.1. This is the network .6.1 and its Mac address is in this guy. Now all of a sudden it's a router, which is just another machine that happens to have two two network cards, right? Which you can just create unlimited number of network cards if you want virtual NICs. Right. And then we have this the similar guy. We have the similar guy similar set up here. But let's solve this. Try having ten 002. This guy wants to talk to this guy. How does that happen? How does this happen? Well, if we don't do anything. Assuming the same configuration will, ten 002 will say, well, you want to talk to 1726.6.2. Well, I don't know where that is. So I'm going to talk to my router to my default gateway and it will send it. Indeed. Right. It will send it. So 062. All right I'm going to see because my router will know. And we'll we'll go here to this router which is incorrect. And this router will say wait a minute. Yeah. This is for me from a data link perspective. But 621722 16 .6.2. I have no idea what that is. It's not in my network. It's not this one. So it will just say, hey, I'm sorry, I'm going to drop it. and we'll just drop it right there and then. So how do we solve it? Meet the routing table. So what we do is we create a bunch of fields, a table with a bunch of fields that says, all right, where do you want to go? Which network? That is the network with the subnet mask. And if you want to go here, which gateway? What is your next hop? What is the thing that is in my network? The link address. Right. Things that I can talk to directly. If it's if it's not in my access that I cannot talk to it directly. Right. What's your gateway? Which gateway should I send it to? Is this a direct link or not? Can I talk to it directly? And what? What is the weight of this? Because now we have multiple paths. Multiple routes. Which one do you prefer? What's the priority? You can have a number here, a value that says, oh, this route is better than this. And which interface should I go to? Is it Ethernet? Should I go to Wi-Fi? Should I go to this Docker interface. Which one. Right. So that's basically the routing table. And this is just an example as usual. Because nobody talks to each other and everyone does things by own. The Linux have its own routing table with different columns. Mac will have its own, windows will have its own. But essentially that's the gist. What I summarized from all the three guys, it's always the next hop, the network, whether it's link or not. Wait, and the network interface card. So let's go through these example. This is just an example routing table here that I have. And it says line three. If you're going to any network, the star you will see sometimes represented as 0.0.0.0. That means anything. That's what it means. Anything if you're going to anywhere, then you go through this next hop, which is the gateway. If you don't know where to go, if you're going to Google, which is eight, eight, eight, eight, let's say this is the DNS for Google, just don't send it to this guy. This guy will know. And that's someone who wrote this route in this routing table and says, all right, just go there and it will follow. And you can change this thing. You can do whatever you want. You can have the route to take you to a machine you can have such that, uh, if you're visiting Google, you always go through this machine. For example, for some reason you want to monitor all the traffic. You can send the traffic to a machine, monitor it, and then have that machine be a router that forwards all the packets back to the actual router and have all the things route logged. That's how the networking folks do do tricks by understanding all this stuff. Tough line for if you want to go to the machine. This one machine on 192168.1.0 slash 24. Does any machine on this network now? It's a network. Then there is no gateway. There is no next hop. It's a direct link. You can just talk to it directly. You know how to do it. But how? Well, through this interface. Ethernet one. Ethernet one is your IP address interface that happened to be in one of these. So you can talk direct directly. You can just ARP and have fun. Well, if you want to go to this network then you use the other interface because that's the interface. You have an IP address on that matches this. And you can use link directly. And it's it's the lower the number the higher the metric the better. That means this route is preferred. And let's go. Let's fix this. How do we fix this? So how do we fix it? Here's a sample routing table to fix this exact problem. And this is the routing table for this. That's one way to fix it. By the way guys there's so many other ways to fix it. But if you want to go anywhere send it to your default gateway. This you can remove you can delete this now because that line replaces this default gateway concept. Because it's actually what a default gateway means. Literally anywhere you want to go, go here except when you go to this particular IP address. Now I hard coded the IP address. If you want to go to this, actually your next hop, your gateway should be 0.6. Like what this guy over here. Yeah. And how do I do that? Well, simple. Just, uh, send it to see. Of course, you need to ARP because it's in your network. So you just do an ARP and go your merry way. And now we're going to send it here. This machine will will receive it, of course, because the destination is C and the source is zero two. Now this machine has IP forwarding enabled. Same logic. It knows how to get to .6.2. And we'll just send it and we'll of course now change the source IP source Mac to be N, which is now the Mac address of the other network, and the destination becomes M. And then how do we reply back? Can M reply back if it wants to? Well, it can because the default gateway. So saves us here because hey, if you want to go to ten 002 in this particular case just send it and it will know because it only has one gateway, and it happens to be a gateway that that knows how to talk back to this. If it couldn't, then we need to have added a route to actually explicitly talk to that and go directly to that route. And we can go see see what we did there. It's very interesting you guys. You can and how do we else how else we can you fix it. Well here's another way to fix it guys. Just think through this and tell me another way is to have there instead. What's the problem with this solution? The problem is you have to configure each machine with this routing rule. Right? Because if you have another machine over here, you have to have the same route, right? If you have another machine, you have to have the same route added to every one of them. Okay. So what we can do instead is add this. here to the actual gateway of the default gateway of these guys. So what will happen is it's like, hey, if you want to talk to this, just go. If anyone wants to go to this network. Of course not. This one. Right. Then go talk to 006. So what will happen is that gateway. If I go back, that machine will send it to its default gateway normally. But instead of dropping it, the router will check its rule. Says, oh, this guy wants to go to 172.16.6.1. Well, I have a route that says, hey, if you want to go to this network, not even just host any host of this network, just go to this machine. 1006. It knows. So it will talk to that in a link way and it will send it to the IP address and then the path will be something like this. Go here. And then this will go here which is slower than going of course here here here. Right. Because that's slower. This is faster. So there's always a trade off in software engineering. And network engineering is no different. There's always trade offs. Which one is faster. You can have less configuration by putting it in one place here and have everything goes through the router. Or you can make it faster, configure every machine to have this route and just do you go this way and the the the the world is your oyster. Now anything is basically just bunch of rules that you play with them. Sometimes you send a there's a lot of bugs like that where we send a packet, it arrives, but it doesn't know how to send it back, right? This is where things like Nat can be playing. You need routes on the other end, right. To play with this. All right. So keep this video short. So what. Updates the routing table. Literally everything updates the routing table. There is the kernel. There is the DHCP dynamic Host Control protocol which assigns the IP address for us if you don't want. To statically configure IP address there is the OSPF the open shortest path first I think. That's what's called BGP Border Gateway protocol which is what the internet runs on all of these guys. If you learn them first you'll be so confused. But now they are makes perfect sense all. What those guys do is just update the routing table. End of the story. That's it. All what these protocols does is just they write routes to your routing table. That's all what they do. And by understanding that they become so easy because they just play tricks. They are applications running on user space processes that does stuff. Open shortest protocol first. It's just it's a scary name, but what it does is like, hey, I'm connected to this network and this is my machine and this is another network, and this is this is bandwidth of 100 megabits. This is one gigabit. And it just makes a list of a database of all possible links that it can talk to directly. It says, hey, I can reach this network through this interface, I can reach this and it will propagate this knowledge across everyone. Eventually it says, all right, based on that knowledge, the shortest path is this, and it will write the shortest path in the routing table only. Right. Based on the knowledge of the graph, that's just like a short description of that. And what the what the kernel will do is just we'll look up the routing table and follow the instructions there. And it happens to be that the best route is written in the route table. It could be that the wrong route is written in the routing table. Right? BGP another beast, but eventually does the very similar thing. If you want to go to certain network that happen to be Google or Facebook, go through this IP address and it just you will find multiple networks and routers that are configured, their routing tables configured so that they can follow the path of the autonomous networks. And we'll talk about this in another video. Better okay. But yeah, eventually everything pours into the routing table. Let's do some, uh, some some demos. We're gonna rest. Rest the routing table. I'm just gonna. This is how you list all the routes, by the way, in different networks on windows, you do route print for some reason, but the prettiest output is windows. The rest are just atrocious. Linux and Mac. Right. But I like the windows one because it prints the headers and it tells you clean. It's a cleaner way of viewing the table. Uh, Linux you do IP route show or netstat RN also works on both Mac and Linux. You can do it this way, and you can list your current route and read it and understand what it does. So what I'm going to do is I'm going to have for some reason, if you want to go to Google or example.com, I'll, I'll, I'll, I'll force a route to go through the wi fi Iffy. And instead of the the Ethernet, because you'll see that I'll show you my network card and I'll have two networks cards, one Wi-Fi, and I have an IP address on the Wi-Fi. And I have also an Ethernet. And also I have another IP address on the same network. Right. But I'm like, think of it like as one machine pretending to be two machines, right. Let's go. Let's go through it. Okay. Now I am on my machine. I'm going to do IP route show and show. Let's explain these things again. Every routing table will be will be different. But the gist is the same essentially right. So we have the default gateway here on 192168.4.1. And and this is my default gateway on, on my machine on my network .4.1. Right. That's the same gateway. But I have an Ethernet network, the device Ethernet network on. And then I have also a Wi-Fi network. Right. 59 is zero and 60 is zero. Okay. And this is the protocol. It was assigned by DHCP protocol. That means this was assigned by DHCP for me right. So the dynamic host control protocol the router assigned this IP address for me. And if you the IP address for my Ethernet network is 179. The IP address for my Wi-Fi is 122. Okay. And this is the source IP address. And of course, uh, my metric is 100 for the Ethernet and 600 for the Wi-Fi. That means if you want to go anywhere, the default anywhere, prefer this this interface over this. That's what the colonel's saying. Okay. Because, of course, Ethernet is faster than Wi-Fi, right? That's the that's the reasoning behind that. But I can of course I can override that and change it. And there is I have a Docker interface. So of course also I have an IP address assigned on 1721.7.01. On my Docker network virtual interface and it says, hey, if you want to go to this network, go through this, right. And this is a link, you can directly talk to it directly. This means the link. Or if you want to go to this, then you go through this other network which is apparently I have another network here that I don't know about. And you can use this particular IP address, right? Here's another one. You can you can directly talk to any IP address on 19668.4.0 22 here, not 24. Right. So the the network size is 22 bits. So that means it's a little bit smaller. That allows me to add more hosts. Of course. And you can go through this network and this is preferred. So and the same thing here you can directly reach it through a link this network. So just do an ARP and do your malware. Or you can do the same network exactly through the Wi-Fi. But this is slower. So 600 okay. And you you use your IP address if you want to go there. Your source IP should be this one okay. If you want to visit this, your source IP should be this. If you want to visit this, your source IP should be this okay. Because you can have different IP addresses of course per network. And of course I have a direct link. Say if you want to go to this you can just directly reach it. That's another link direct link here. So what I'm doing here. All right guys. So I thought the best way to do this actually to explain a problem that I'm trying to solve. So here I have a machine. It's a Docker machine. Okay. So ubuntu this is the IP address that is publicly available publicly in a sense that is in the same network that I'm trying to connect to. Then there is inside this there is a Docker host. Right. And there is a Docker IP, a Docker network with a single Docker container that is of this IP address that belongs to the Docker network. One 7217 011. Okay. So this machine can easily access this IP address. 172.7.0.2 because, well, there is a route that says in this, hey, if you want to go to 17, any network, go through the gateway that is the Docker zero, uh, gateway Virtual and you'll be able to access the Postgres machine. Okay. Which I'm going to demonstrate in a minute right here. Here is the machine. If I do. Telnet one 717 172.7.0.2 on 5432. Which is the postgres port? You can see that I'm connected to Postgres. Okay, so it works. I can connect to Postgres from this machine and if I do config grep 192 you can see this is my IP address. I have two IP addresses here on this machine. But this is it 179. Okay. So what I want to do is I want to go on this machine, which is my Mac, which is outside this machine, and I want to be able to access this guy. Currently I cannot, of course, because this is not aware of this network. Right. So let's let's try it. Indeed. This is my IP address. And if I do telnet 170 keep forgetting 172 dot 17 .0.25432. Of course it's going to keep trying and there is no way, no path to this network. What does it do? It will. Technically it will. What will try? It will try to talk to probably my default gateway, which of course there is no path there. Right. And if I do try to actually try it. Uh. Oh, n. You can see that it tried to go to my default gateway and it went to my public IP address, which I'm going to blur here. And of course, it went. No way. Because there is no way out there. This is a this is a network that exists in my in my other machine. So how do I fix this? How do I fix this? How can I make telnet work? How can I connect from my other machine from here all the way here. Well, we learned it's a nothing but a network. This guy knows. So what if I added a row that says, hey, if you want to go to this, go through this. Literally that. So how do I do that? So this is the command sudo root n add network. If you want to go to this go through this. That literally is this is the means. This is the gateway or the next hop. If you ever want to go to this now this is a specific IP address. You can change it to a network. But by adding that and of course as an administrator operation, just like that, I added a route that says, hey, if you want to go to that, then go through this. Now if I do, a telnet works. So what what happened here? Now if I close it and I do a trace route. Look what happened. The traceroute clearly tells me that. Oh, we took a different path. Maybe we took. We went through 179, and now we immediately, once we got to 179, we. 179. Uh, of course, I had to do one more thing, which I forgot to mention is I had. IP forwarding enabled here on this machine. And you can do it, uh, via vim. Yeah. This baby. If this was not enabled, that forwarding would not have happened. Why? Because the machine will receive it and say, well, I did receive your IP, your Mac, and it's not for me, it's for me. But the IP packet was not for me, so I dropped it. But by enabling IP forward, you can essentially allow people to communicate. So yeah, that's what I wanted to talk about. Very cool stuff. That's essentially the idea of networking. As a summary, we talked about each device has a unique link address and that is the Mac address. That is the network card address. And even you can have a virtual one, which we showed in Docker in the Docker case. Right. And devices can talk to each other via direct link. Direct link can't scale for large number of devices. We needed the idea of IP. Then of course the idea of network was born. Then we idea of routing basically based on the network. You can route it or if you want to go here, you go here. If you want to go here then you go here. And that's basically the concept of routing. Hope you enjoyed this video. I'll see you in the next one. Goodbye.


### Networking with Docker vtt

I've been playing with Dr. Networking for the past week or so, and I thought I'll make a quick tutorial to show how Docker networking actually works. So we're going to play with the Custom Network and get a spin up a Docker container, put it in a network, going to spin up another container, but another network, and we're going to show how we can eventually make those two talk to each other. I'm going to kind of demystify some of this stuff behind Docker networking. If you're interested in this into the details of doing things, this might be for you. Let's jump into it. Okay. Let's start by spinning up a new container Apache Observer and expose it to our hosts through published Port 80. Right. Very simple stuff we use to do this all the time. Right. So I'm going to do a Docker run dash p I am going to expose Port 80, which is what is running on my container to port a T on my host, which is basically running Docker. Right? The application that's called Docker, which means any one who tries to access Port 80 on the public machine, Josie and Mac will be forwarded to this container inside it and that request will go right there. And that's basically what publishing really does. Port Then I'm going to do DD http D That means hey detach so I can do more stuff in my terminal. CBD is the image that is the official Apache image, which is the HTTP naming. And when we do that, we're going to get a beautiful new container for Docker. PS can find This is my container ID, I didn't give it a name. It was wonderful stuff. I still so if I do a curl on my machine. Ryan. It works. If I go to the browser and I go Hussein, Mac and Port 80. It works, and it only worked because there isn't anyone running Port 80 on my machine. But this when we actually did this, the container was placed in a network. Technically, what is this network? If we do inspect this container, you can basically take the first two letters of a container ID, and it will. It will take you. If I can type. Right. So that's that's the network. So notice that it's a network. It's a Jason. Right. And you can be in multiple networks. Yeah, sure. Why not? So in this case, this container that I just spent is in a network called Bridge, and that's the default network. And this bridge network that is basically inside this Docker host and on my Docker for Mac. As a gate recalled one $72.17 zero one And that's the IP address of the container. Now, what happened if I do a curl is http one 72.17 022 from my host. You notice that it didn't work. Why? And the reason is because there is no at least in Mac Docker for Mac, there is no bridge that takes you from my Mac, which hosts Docker for Mac down to the container. What what Docker does is actually spins up a virtual machine and there is where Docker actually live inside my Mac, and that if I manage to get into that network, then I'll be able to do it. But from public, my machine, I can do that. If you have Linux, you can easily access that because it will be native installation of dock, right? Mac doesn't have that unfortunately. So that's the bridge network. It also means that if you go to the container, that container actually is bridged back out the bridge. The container can go out, but you cannot go into the container. That's the only difference here when it comes to Mac. And the reason it can go out is because if you like, for example, you did a Google ping, Google from the container. We're going to show how to do that. We try to ping. Google is going to do a DNS even before before pinging. The DNS call will have to go outside to the host. And to do that, you need to go through the gateway and then you go through through the actual host, Right. And then the host will resolve. The DNS is a this is IP address of Google 14433112, two. And then that will also the request will go out right to the gateway and then all the way to the Internet, through my MacBook, through the basically the router, my router and the ISP and goes all the way there. I need to actually have certain tools to debug my container. Right. Like ping and trace it out and it's lookup containers. Don't have that by default for good reason. So I don't need to need these utilities, but I actually do. Right. Because I'm going to do a debugging, right? I'm going to say, Hey, where are you now in this thing? So what we're going to do here is that effectively I'm going to build my own image from a DVD that has a little bit of a fanciness to it. So I built up a Docker file here where I'm going to inherit from the HTTP feed image and the moment we build the image, right at that time we're building this image. We're going to build a brand new image from this. We're going to run an apt get update right there. And then because the feed runs on Debian, right? So very similar to a ponto, it's going to do an update. It's going to run another run again. Run. What run does is it will execute whatever command this at the time of building the image. Right. The other one is called cmd. CMD will run at the time of running the container. Right. So be careful with these too. So I don't care about anything here when it comes to running the container. I'm good there, but I want to install IP utils ping. I want traceroute. I want IP route. I want curl. I want telnet, I want DNS tells for a nice lookup. I want them to edit. So all of this stuff, once you build all of that stuff, I'm going to share it with you guys. You save it and quit and then you do Docker Belt Dot. I am in this location. Dashti I'm going to call it PN HTTP. So that's for networked activity, right? So it has more tools for me to do stuff. And now at that time of building all that stuff, everything is cache because I did this before. Now I have a beautiful new image. So what we're going to do is I'm going to remove my container. What was it called, Docker or MX five? A Let's go ahead and delete that. Of course we've got to stop it first. Five a. And then we're going to remove it. Now let's go ahead and spin up Addo-carr. And I'm going to start giving them names now. We're going to call it s one server one. I'm not going to publish it right now, and I'm going to spin it up right there because it's one. I've got to detach it. And I am going to call it NHT. TBD. Now we're going to create the container from this particular image. Did I explain the difference between an image and a container? Well, very quick. An image is basically the template where you can spin up in a number of containers. Any number of containers. Think of it. If you view code and programming and object oriented programming, an image is like a class. You build a you created a class and then the containers are objects, instances of that those class. So containers are instances for those images. Think of it. Think of it this way. Right. So you can build you can spin up many containers and we're going to do this right? I'm going to spin up one container called s one from n n TBD, which is the one we built. It's the identical, but with a little bit tooling so that we can test stuff. And I'm going to spin up another one. Right. So now we have two containers as one and it's two and the source is an HD DVD. So technically. Now notice what I did. I did not publish it to the server, right? I did not publish it, publish any port. So technically I cannot call anything. Right. It doesn't make any sense. Of course this won't work. Says, Hey, what are you talking about? There is no path to access these guys. Right? Right. But what did we say when we spin up a container? By default, it goes to this bridge network. Right? So if I inspect this one now. Notice that got IP address 172 at 17.02. It's in the bridge network. If I inspect is to. It has. 170 2.73.03. So we have IP. IP address two, IP address three and both of them are in this network. And another way to do this is you can actually inspect the network that's called bridge. I think that's how you do it. Right. Or is it flipped? No, that's right. If you expect the network, you can see all your networks. And this is we have one network current. It's called Bridge. And this bridge has the network has two containers as one and it's two. That's another view to view your containers. So now you notice that how many people users will be spinning up containers and everyone will go into this default bridge network? All right. Could be bad if you have sensitive containers with sensitive information. That's why we going to learn how to spin up our own custom network and then put our stuff there. But still too early for that. So how about we actually bash into my container here? An interactive terminal. It's called S1 and bash into it. Now, if I do that, I can pink. I can pink google.com. Wait a minute. How can I ping google.com? That's actually interesting, right? I am inside the network that's called bridge. I am my IP address. This is my host name. It's gibberish. 849 And this is my IP address. 170 2.27.02. That means the reason we got access to the internet is because of our gateway. Our gateway is dot one, and our gateway in itself has another interface that exposes it to our my network, which is my MacBook right here. And then my MacBook is connected to my wi fi, which is connected to my router, which connected to my ISP. Right. So that is like a line that takes me right out there. And the way it works is just the default gateway concept, right? If you're going to Google.com, this is the IP address. How does networking work? Says, Hey, every network has very simple logic, at least locally. Right? I'm going I want to go to 142 to 2, 52 to 1, 7142. It will ask this question. It will do a subnet mask on itself says, okay, is this guy in my subnet? How does it know that basically does a slash 16 on its own, its own IP? That's basically where the subnet mask is, right? So my seven is 255.255. Right. Because it's like 16 .0.0. So we'll do that to 55225 do an end is 142 250 the same as one 72.17. Nope. Nope. That is not definitely the moment says nope, all bets are off. It's send it. It sends the IP packet to the gateway and the gateway will just do the same logic over and over again until eventually the the packet will reach someone who is in its known network. It's not quite that way in the internet. It uses something called the BGP border Gateway here, but per protocol. But that's outside there. The discussion here for DAO. But yeah, that's basically the simple logic, right? So now cool. I can ping as one which is well like wait a minute, a pinging is one. I can't ping this one. How about is two that can't ping is one or S2. Why. Well, if you think about it, let's do an endless lookup, you know, and let's look out basically the way to give me the IP address. What is the DNS address for Google? Look what happened. It gave me the IP address. Right. We're going to come back. Why? I couldn't ping my containers in a minute when I did an anonymous lookup. This was the DNS server that hosted me. I have no idea what's this because that's not my subnet and my host. My seven is 1922168.255.254. Right. So this is what this is is actually this is the VMM that I talked about. This exists inside my Docker for Mac, and that is basically where Docker is running and, and that is the DNS server for Docker when it comes to accessing bridging the Internet. So if you want to do any DNS, it goes to this query. Why? Because that's the bridge network. The bridge network is lives there and as a result, all the data will go to that. Well, that's why when I do an NS lookup on my own hostname as one you might say Josina and so on is not really hostname. Right. Well. We can do this right? Then let's look up and then do this same thing. Even that will won't work. And the reason is because that query goes to this. Well, think about it this way. This query, this DNS query will go where? We'll ask the same question. I need to go to 192168.1.2625.5. Right. Where is this? It's not in my subnet. Same question. Right? It's not only subnet. Let me set it to the gateway. The gateway is 170 2.0.1. Right. And that will take you out to this. And until it reaches the host, the Docker host, which is this guy Now this guy is going to ask the question, what is this DNS? It doesn't know because that is you're asking someone outside your network for your hosts that are existing on your network. They don't know that. And that's the why you cannot access things by hostname inside the bridge network because the DNS is just outside. Right. That's why. But but if you want. You can ping the IP address normally. Camping yourself. You camping? This guy the gateway. You camping three. Which is the other container? That's definitely fine. You can curl. 170 2.07.03. Yay. That is pretty cool, right? And that's myself. We're going to change them so they give you a different result. So we know where are we? Right. We're going to do that right now. I am in S1 right now, so now I can't access anything by hostname, which is very. Limited, Right? That's the limitation here. But we can we can definitely access by IP. Right. And the problem here is also you not only you can access by IP, everybody in this network can access your stuff. Nope. We want to limit that. Right. So let's go ahead and actually change a little bit here. I want to go to the what is it called the docks here. And there is an indexed document and I'm going to change. It works here to call it Yo, this is S1, right? And then I'm going to exit here. I'm going to do the same thing, which is to go to the is to go to PhD docs and then index the HTML, go to that, and then I'm going to change this to S2. So we know. All right. Now if I curl 172, what was the IP addresses again? I keep forgetting the subnet. If I curl 172 to 70 to 73. This is a stew. This is this one. Cool. That's nice. I like this touch a little bit. All right. Again, these are these are containers that are inside the network, and they can only talk to each other and very limited by IP address Now by hostname. Why? Because DNS is going all the way to the host and trying to figure out things and it can't. Right. So how can I fix that? Let's fix that. Well, fixing that is not hard. We can actually create a network. Let's go to Docker. Network Create and we call it, I don't know, backend, whatever. Right. And when you do that, you can actually do so many fancy things, right? One of the things you can do is actually specify a subnet subnet. You can't you can go by the default subnet, so it will generate a subnet for you, but we are fancy people. So what I'm going to do is actually generate a subnet and that's my subnet. So what I'm going to do is how about we generate subnet ten .0.0? And let's make it slash. 24. Which means this. And this. And this. Eight, eight, eight. Right. This is the network. This is the host. So I get to 55 containers per network, plenty more than enough. And then I'm going to call it. Did I call it right? That's it. I think. Do it. Do anything else? No, I think that's good. Just like that, we created a network. Now, this network, if I inspect this network. It doesn't have anything called it back end, Right. This is my subnet and it doesn't have any containers. So how about we actually attach connect my containers? To this subnet. Well, how do I do that? You do Docker. Network connect. The network, which is backend and the container is one. I want to connect this one to this guy and I want to connect S2 to this guy. I don't know if you noticed this, what's going on here, but we're going to we're going to come to that now. If I do Docker inspect Docker network, inspect the backend. You'll notice that. My daughter's got an IP address. Two, three. You can also. Set a bed here in the network says, Hey, this network is internal. That means only the guys inside this network can talk to each other. And nobody, they cannot leave the network. Even if there is a route that says, Hey, go to this network. Nope, they cannot leave it. So what what we did here, if now, if we inspect this one, for instance, what is what will happen here? Look at this. It belongs It's one now. Belongs to two networks, right? This guy and this guy. No, we don't want that. And this is a mistake I made a couple of times where I was like, Oh, I connected this network, but it is still connected to other network because you can absolutely have to network cards in a machine. Think of it. The container, very similar like that. Right. And if you think about it, the container is just a process that happened to have to be in a in its own namespace. Right. Which means they have limited set of resources and you can add and remove resources so that other containers, other processes cannot see it and you can add it to a control group to also limit says, okay, you only get this much of the CPU and you cannot exceed this much memory. That's basically how containers work. In a nutshell, container control groups and namespaces just isolate them from the word. Now I need to really remove those suckers from the network. So I do a docker in network disconnect is one know the network first back bridge is one remove from the bridge. Remove the bridge. Now the those suckers are alone in this network and let's inspect it. I suppose you can do a docker, inspect the backend. It's just better to add in as well to know that, hey, by the way, I'm inspecting I work, but Google dual docker actually overrode overload that operator to inspect both containers and networks. Look at this. Now we are alone, my friend. OC Docker Execute Dash Interactive Terminal. I don't know if it stands for internet statement. This is how I remember stuff, by the way. Guys, are you going to see weird stuff? That's how I remember. By the way, BASH I'm going to bash into the container now. I do not look up as one. Boom. And us look up is to bomb a. Look what happened. Look at. Look at this. The DNS server now became this local instance in the container. That's that's 170 2.00.. 11 and that's local. This is loopback effectively. So now that is why we were able to resolve that as one which is technically not the hostname even to the IP address. Docker knows says, okay, now that we're on your own network, you're going to get your own beautiful DNS provider. Actually, every container has its own DNS provider and technically they are all connected to each other. Somehow they share information or they because if I add another container to this network, automatically the DNS knows about this. Alternatively, I can also use this docker and look up. This guy. And that also answers me because that's the actual host name, right? So now I can do curl HTTP as one I can do. Curl. It'd be a stew. And now if you have like engine X or H a proxy running and you spin up a configuration, you put in an X within the same network. Beautiful. They see each other. You can use hostname inside engine x y because the DNS resolver is that thing is not going outside and trying to figure out what the DAS is. Right. If this was like a public thing, you know you have your own domain server and that's a good idea sometimes where if you have like in the cloud and you have your own DNS server and so you probably have a DNS server and that DNS server knows about every single entry that an actual legit DNS server, you know. And you can add entries there, you can add fancy thing. It's like fancy, dull, bland C dot com, which doesn't exist in the Internet. Well, maybe it does. And that. Can point to your local container. Sweet. So we did that. We build a bridge. They can talk to each other right now. Right. Beautiful. They can call each other. Call each other. The containers are also have access to the Internet. This is something that you might not want to. So if you want to actually disable that, you can create the network with a dash dash internal right and that will isolated from the bridge. What does that mean? ALTER I'll give you an example. If I do a traceroute to S1 right now. It goes directly there, right? If I do a trace out to S2, it goes directly there, right? Because it's in the same subnet. If I do a trace route to something above me, write a trace out to my router, my actual physical router to 2541192168.254.254. This is my router that is actually sitting on my network, right? It can find it. Right. And the reason is because. Well, what is this? I don't know. It's not a mine. It's none of my subnet. So it was like, okay, Gateway, take care of it. Where's the gateway? That's the Gateway ten 001. Who assigned it? Well, the bridge did right. The network. When we created it, it created a bridge. And the bridge has a default gateway, by the way. And that's why we started the containers with two and three, because one was reserved for that. And there was many articles, by the way, I would like to have access to that sucker and I would like to configure the gateway. I would like to make a container my own gateway Docker. Don't let you because Docker do. It is so restrictive, dude, there's so much restriction when it comes to using even the internal network. We talked about it. Well, the moment you do it internally, you will you will not have access to the gateway, right? But you will also not have access to other networks. Why? If I added a rule, follow it. Just it's just all or nothing. When it comes to Docker, they don't have like a middle, middle ground. It's like, okay, dude, all right, I want an internal network. I want another an internal network. I don't want a bridge. I just want this these guys to talk to each other and want to talk to another network, because this network is a database network and this network is the Web servers. And then revenue calls the reverse proxy. And I want certain people networks to talk to each other, but not everyone. But I don't want them to connect to the Internet. I couldn't find a way. Maybe someone will let me know in the comment section, but I couldn't anyway. So you notice that it goes to the gateway, and the gateway says, Oh, I don't know this. Let me go to my gateway. Right? And that gateway basically will take me to to the default gateway, which is which is where I can do the same thing with my other machine, which is the same thing. So we go to this and then this will lend me in the network. And that network basically will take me to my host. So this is how it works now and this is what I need to blank things out. And just so I don't know, I don't show my IP or public IP address if I do dot com. Same thing. Goes to my network, goes to the router, goes to my public IP address, which you cannot see. Right. And that goes to another public IP address, which my ISP goes there goes there, goes there up until we find the actual gateway. How does traceroute work? Basically a bunch of ICMP messages going back and forth. You said the TTL and then the shorter the title until you reach the next hop. Because IP packets, every time you get an IP packet is routed the TTL. The time to live is the decrement on the IP header. And that's how there's a nice game you can play with traceroute. So now that's, that's the path. So if you cut the path, if you cut that bridge, the gateway, you have no access to the internet and that's how you basically can do it now. But we're not going to do that. All right. So now I have a do not PS. Ellis. Oh, I'm in the wrong thing. Of course, that that that that image doesn't have to be darker. If I do a darker. Network, as you can see that I have now my back end, right? Right. My back in network. What I wanted to do is effectively spin up another network and put S2 there and see what will happen. So I spend up to network, right? The other network is going to have a different subnet and I want them to talk to each other eventually. They're not going to be talking to each other and we're going to talk about why effectively. But let's do that. Docker. Hopefully the command is still there. I didn't write a lot. Say, Hey, there you go, Thank you so much. Backend, I'm going to call it back into right. Maybe. We can call the front end. Front end. And you might say, Hussein, why are you doing this? Why are you creating 200? Well, it's actually a very common thing you do in production where, let's say you have I'm going to draw a draw up picture here. You're going to see the picture here where we have let's say you have you have engine X or proxy or envoy or something that talks an API gateway that talks to the client directly. And you have maybe five instances of those to load balanced right through the DNS and those talks to load balance on the back end with a bunch of web servers. Right. And those who are observers talk turn around and talk to the database. One configuration might be put everything in the same network. You can do that. But why is that not a good idea? Generally, it is because in genetics or this, it is very dangerous to expose this publicly. The attacker, if they compromise Engine X or the the gateway, they got access to your entire network. There is no other line of defense. The only line of defense is there. And that. Right. So what do you what we do effectively is put engine X and all load balancer in its own network and put web servers in their own network and put the databases. Definitely the precious data in its own separate network. Why do you see all all the headlines, oh, seven terabytes of Elasticsearch or MongoDB or or Postgres database has been leaked because people don't do any of that stuff. What they do is they spin up a podcast instance on Amazon and they make it public and they listen on on interfaces. So that's why it's publicly accessible for anyone. That is a really bad idea. That's hopefully, of course, that extreme right. But what you can do as a network administrator, I'm not a network administrator. I just love this stuff, right? But I'm a back end engineer. But what you can do is effectively play with this thing. So I want to simulate this in Docker by creating multiple network. Then I want certain rules to be applied so this network can talk to this. So that's why. Right. So let's call this, this is the frontend, that's the back end. So I'm going to create the frontend network now. Pull over laps. Oh, what the heck. Of course. I just said enter. It's the same subnet. What are we going to do here is actually add one here. So now this is my subnet ten zero zero. So that means the first one is ten 00.1.2.2255. Right. The second one is ten 0.1.1.2.255. So you have two different unique subnet effectively. Now, now it's not yelling at me. So wait. Now we're going to take the sucker. Disconnect is one. The disconnect is one or two. Let's remove S2 as two from the back end. Right? And then connect S2 to where? To the front end. What did I do wrong? Yeah, I spelt connect with three ends. Genius. Nice. Now, Docker inspect is two. It has IP address done. 012. Docker Inspector. An inspector is sworn. This one is just this one still the same thing? Well, let's let's bash into this one. And say pink as to what? It. What do you mean? I can't ping this one. It's too. Well, there is a you can't ping as to it's because it's not in your network. All right. Because what happened is, well, in this case, it's a little bit complicated. So let's do. What was the IP address? Ten 002, I think. Let's exit and and confirm. Just do a hostname dash i That's the IP address. All right. So what we're going to do is the ping ten 002. Right. Which is the earlier I'm going to do. Right. Which is this IP, I want to ping that that will work either. Right. And I get I went to the extreme, I just did the hostname what happened with the hostname is like I couldn't figure out even what your IP address is. So the DNS failed here. Well you don't have DNS, you have the IP, but you couldn't figure out how to get there. Why? Because it says, all right, I want to go there. But what's my gateway? My gateway is ten 0.1.1. Right. And I took it there. I'll take it there. But the gateway doesn't know how to forward it. It says, okay, I'm going to go. I, I have no idea how to do this. I have no idea how to reach this network because the gateway doesn't know. And so we need someone who knows we need a router, we need to create, right, a router. So we need a router. So we're going to spend out our router, our own gateway Docker run Dash that name. I'm going to call it GW. The Gateway, right? Router, a router or gateway that's called a gateway. That's called Gateway. GW You guys know it's going to link us from the front end to the back end and vice versa. So now Docker Run does that name. I don't need to publish anything. The host has nothing to do with this. What are we going to do? I want the gateway. By the way, you can. While creating a container, you can assign it to network. And I want the gateway to be in my back end network for now, because I'm going to add it also to the front frontend. Yes. This router, this gateway is going to belong to both network. Why? Because this will be the route. It will route us from one place to another place. Very beautiful. So we're going to there. We're going to detach it again. Our HD DVD, which is our beautiful custom image container here. And I think that's it. I don't think I need anything old. So this new. Again, I. Okay. I lost it. Duncan on that name. Gateway Dash, Dash Network Back and Dash D and Noise, the DVD Sync Suite. Now we have the gateway and in the back end network. But that's not enough. Docker Network connect the front end to the Gateway. Can I do it both? I never tried to add to networks, and I probably can probably can do that slash network back in front then and I will work, but I never tried it, so you guys feel free to do that. So now, Now. That's still not enough for you guys. We're going to find out. If I execute, if I bash into the gateway. Right. And I say ping, it's one. If found it. Pink is too. If found it, why? Because if you. If you look at the graph. Yeah, it's two and it's one are right there. Right. Because they are. And that's why I found this one. Because my audience knows about this network is right there. And I found this too, because my audience knows about it, too. It's right there in another network. That's why it knows about it. But technically speaking, the other guys don't. Right. So it's if I if I exit now and I go to S1, it's one still of course cannot ping S2, Right. And vice versa. It cannot know. It doesn't know, right. It doesn't know the gateway. Right. Because it's in its network from this side and the other guy is to it. Also know it can take the gateway from because it's in the network from the other side, but they can't talk to each other. So here is where the final piece. So do we do we know what we're going to do now? Hostname dash. Oh, that's my name. I am S2, right? Nats look up. I can do a gateway. That's my gateway. All right. So what I want to do effectively is I want to add a rule. Such that. IP route add I want to add a route in my host as is true, such that if anyone want to visit this network to slash 24. They have to go through this gateway because it's a different gateway and that's what we need to do. This will fail because I forgot something important to do. The reason this will fail is because this is an administrative operator, right? And you cannot add it. You have to start the container with the ability to change administrative setting. And so I have to restart. The container is one and is two with the flag that is called dash dash. Cap, add equal net admin and that would be a good a good thing to do. Right. How about we do this again? Right. It won't hurt Docker to stop as one Docker stop is too. Dr.. Our aim is one is to wheelspin them up. We're not afraid. All we have to do is just literally Docker run dash, dash, name S-1, and then the network is the that's the back end, right? That was in the back end. And I want the let's spin up with the administrator of option cap add. Right. Equal net admin. Right. And then. Dee dee dee dee. Dee. Dee. O. N. Right. So we have a networking access. Sweet. So now I'm going to open up my first network. Did I forget anything? Nope. I'm connecting it so this way I'm going to spin it up and then go in and learn something right here. We're going to spin up and also connect it. Beautiful. Do it up. And now this guy is going to be in the front end, right? Let's do it this way. And this got to be a stew. How about that? Like nothing happened. Oh, we did. We get the same IP addresses. Probably not. No. We get the IP is because it was it was released and then we got it back. Sweet. Now let's bash back. Into the container. It's one bash now host named I. Right. So that's my first. I want to do a nice look up. What's your gateway? And let's add the rule. So now I say hey in my S-1 if someone. Well, let's go to the other container. Docker execute dash it is to bash. I'm going to do the same thing so I can. I can see every IP here in this lockup. Get. Oh, now I am in a stew. Right. That's my gateway. That's the supposed to be my gateway. But it's not really in the network. It's not my gateway. My gateway is something else I want for cert. I want to add a route to send packets this way so that it can reach anyone in that network. How do I do that? IP add route. Right? Also, it's our IP, that IP route add. If someone is going to 0.00 slash 24. Any IP address on this network. Pleas for them to ten 028123 instead of the default, which is ten 011 because that's the default gateway. Now I'm adding another route. If you think about it, you can actually change the default gateway so all your traffic goes there. But no, we don't want that. Certain traffic still needs to go to the default gateways for internet access, because if you change the default gateway, this gateway will get in and it will be confused. It might actually still work because it will be as if it's a very good idea. So you can you can have a centralized place for all Internet access and you can do logging and monitoring. Dude, a lot of stuff, man, you can do. Now, if I do a trace route on ten zero zero, let's do this. Ten 00.2. You know. What is that? Look what happened. We went. We want to go to 002. We correctly follow the route, says, Hey, I'm going to go to ten 00013, and then we drop dead. Why? Right. If you know how to race or out works. Penguins do the same thing, by way. They will just say, Hey, I couldn't do anything. The reason we couldn't get a result is, yeah, you added a route to take you to the gateway from as to right network. But. It's one has to respond. Right. So I would guarantee that Gateway actually took the package all the way to S1 and S1 tried to respond, but I couldn't. It says, Hey, I want to respond back to what is it, ten 012. Right. That's my that was my IP address. 2.0.1.2. But I couldn't because I don't know how to follow the message. It's one couldn't. So what we can do is go back to this one and add a very similar rule so it can know how to forward the packets back. Right network is not magic, my friends. It's all logic IP route ad now if someone wants to go. In this network .1.0. Please go via ten .0.0. Dot three, right. That's my my gateway from this side. Right. Because remember, everyone sees the gateway as a different IP address based on their network. And I'm going to draw hopefully the the the pictures and I show them as in the screen, you can pose the screen as you go there. Beautiful. Now let's do ping. Ten 0.10200 works curl. Ten 0.01.2. Nice. Right. This is it works. Why does it say, oh, we didn't change it? Because remember, we changed it and then we deleted the container. But remember, just just take my word. This is S2, right? Right. A problem is like we cannot do curls to still. And the reason is maybe going to make another video because technically, Dad is still not aware of this thing. Yeah. Dennis is a slightly different beast in this scam. It couldn't figure out. How to route this traffic. So there is another route I'm missing, probably to propagate DNS so that everybody knows about everybody. But yeah, so this is one more limitation of my tutorial. So we'll figure it out eventually. Maybe, maybe it's not possible. So now if I go to S two and I say PINGEL ten 002, it works if I do curl. Works by the NHS's. Look up. 002. And that doesn't make any sense. If I do a trace Route ten zero zero to look at the path, that's pretty, pretty neat for me. You go. You want to go here? We're going to take you to this gateway. And then that went it took it took us all the way there successfully. The only missing thing is the DNS, and I couldn't figure it out how to propagate DNS. All right, guys, I think we're going to stop here. This is a huge, large tutorial. Hope you enjoy it. I'm going to see you on the next one. One more one person piece before we end up. You can also add a script to add these rules every time they contain a restart by adding a script at the end of the Docker run. I didn't do that. But it's possible, right? Because these IP routing are ephemeral. The moment you restart the server, the container, they are gone. And yeah, one more thing. I know that you can all do most of this stuff with Docker compose, but I thought I'd get a little bit dirty and do it manually by hand. It's just I just like to know how things work. Yeah. Got to use the docker compose. I don't like yaml, but yeah, it's just another way. This way you can actually see exactly. You're building everything by hand. I like the building thing. My hand. It could be just me. You can have of course use Docker compose to build everything with one single script and just spin out. But what is the fun in that? There is no fun. There is no joy. Right back to the video. So yeah, I thought I had fun doing this stuff, so I thought, I'll share it with you guys. Hope you enjoy this video. I'm going to see you in the next one. You guys stay. Awesome. Goodbye.


## Analyzing Protocols with Wireshark


### Wiresharking UDP vtt

Hey, guys, what's going on? It won't be a cause if we didn't inspect the most important tools in networking. That's Wireshark. Wireshark is a tool that allows us to sniff packets, segments, frames, really anything as going out from your computer out into the wild, wild internet or any other machine for that matter. And today we're going to Wireshark, UDP. You know, to to kind of verify what we have been, you know, talking about all this time, you know, and so go ahead and install Wireshark. It's really straightforward, you know, just don't go to our dot com download the approach version for your operating system and once you're on it, you're going to listen, you're going to capture, you start capturing to a certain interface I'm now capturing on my way, right? Anything on my Wi-Fi interface I'm going to capture. So today what I'm going to do is I am going to start capturing. So I'm going to start capturing. And obviously let's not save anything. But this is going to capture a whole everything. That's a lot of information, right? So I want to filter that. So I need to filter all that stuff, right? So I want it only to capture a UDP packet, single UDP packet. What does say UDP? Give me a UDP server DNS. Right. Any DNS server is actually UDP server. Yeah. Unless it's D.O.A., which means it's a B, TCP is devious over the DNS, but we're going to go with the simple DNS server. So one of the simplest ideas of again, our tool and it's CAT, we're going to do a UDP version, we'll get to connect to eight, eight, eight, eight. That's that Google DNS server. Yeah. On Port 53, that's the default DNS port. And I'm going to send the message test. But guess what? It's there. It's going to be buried with all these packets, you know. So I'm going to do is actually filter and you can do the filtering like this IP editor, which is address. I'm interested in the eight, eight, eight eight address, which is that's the destination address that we went to write this way or only show our beautiful data that we sent. Obviously, it's not really a great. It's a malformed DNS packet. That's fine. We don't really care about actual protocol here. We really want to take a look at the length and be like, I like the source and all that stuff. So just like that, let's ignore the last one here. But look at this. We have a frame layer too. That's a that's also layer two, but it's the Wireshark effectively explain things and try to pass things so that it's more readable for us. You know, they're going to see this all the time. So frame as the frame how it looks like it's 47 bytes, Ethernet, which is the frame, really the protocol itself, Internet protocol version four, which is that layer three, right, Internet IP layer. And then user data protocol, which is layer four. And it gives you a nice summary of the information, information that we really care about and obviously the application layer, what is this thing? Wireshark tries as much as possible to port and understand what this thing is about, so we will all figure it out. Oh no, this is actually a domain name system. I know that because I know this port will figure out. I know this port and I know this IP, so we'll figure it out. What is the content of this? But here's the the biggest thing here is actually the source Mac and the destination Mac is shown here and this is shown by the frame, right? Because in the frame, in the layer to really care about the source Mac and the destination Mac very powerful concept and layer two, this is what we care about. If we go to the IP layer, the destination is eight, eight, eight, eight. The source is 1921682552144. That's my private IP address. Well, technically speaking, if you capture outside the router, it will be my public IP address of that out there. But this I'm capturing on the device, this is what Wireshark seizes and the user data go. What do we care about the layer for the export destination port? So that's the random port that is was assigned and that's the destination port. So Google didn't even bother replying to me. Right. In this particular case, they find this very, very interesting to talk about all these kind of things. So it's, it's, it's fascinating to explain all that stuff. So. Let's unpack layer three and a little bit. Read more information about it. What do we have here? Most of the stuff we kind of talked about, some of the stuff we do like the flags. There are no flags set. We don't have any don't fragment bits or anything like that. All right. Time to live. You guys remember this one? Remember the title in the IP packet? How long can this packet survive as an IP back here? 64 is the is the value. That said, after 64 hops, this will die. The protocol, which is a value of 17, indicates that hey in me as an IP packet, I have the UDP protocol. Very fascinating that Jason talked about all that stuff that verifies all that stuff, you know, and for some reason the validation is disabled. I don't know why. Probably it's a side effect of the capturing logic source address and destination address. So that's all the information that we really care about. It's not much you don't you see there isn't any sense in echoed anything like that because it's just pure UDP logic, you know, fascinating stuff. If we go to the user data again, what do we have here? The data gram of the UDP is really small. It's eight bytes. Right. It has the source and the station port. Right. As. And that takes most of the bite, really. It has the length of the content, which is 30. And I believe that's what I what I sent the content and plus other stuff time stamps it's stamping the all the information in the data and the payload itself. So the payload is actually five. That's what I sent with. The total length is 13. So it adds more stuff to my packet, right. So yeah, that's what I want to talk about. Just showing you this the raw UDP. How about we jump into it and do another one on TCP this time?


### Wiresharking TCPHTTP vtt

All right, guys. So we did want to know UDP, Wireshark and UDP. How about we do a Wireshark TCP, and what is the best thing to Wireshark TCP? Then just do a simple HTTP request. So let's kind of do that. Let's go ahead and do that. So I'm going to close this old request here. I'm going to do a curl on. Example dot com, not the secure one. We're going to do another video just as soon as you secure HGTV. I'm going to do the example that's coming, but I need the IP address of example dot com. And what's the IP address of example? I was going so I'm going to call example or com, but I'm really interested in the IP address of example dot com. Right. Because I don't want all the other garbage. Right. This is the IP address of example that comes. So let's go and copy it because it definitely made it in my Wireshark but is going to be lost with all the other stuff. So I'm going to filter this and let's go ahead and clear it and start over clear and then go to a curl. SDP example dot com hit enter. Look at this. We received all the information. Here's what I'm interested in. Okay. As. It's TCP now. It's not it's not UDP. So there is work at the beginning. So those send their sequences. You know, the sequence starts with zero. And this is called the relative sequence because it never actually starts with a zero. It starts with an actual large number. But Wireshark, to make our life easier, they, they make it a relative sequence to the through the handshake. So they start from zero and show us the relative because it's way easier to read the relative. And you can, I believe, do right click and show the whatever the. So do somewhere here you can show the actual values. I don't really care about the actual values. But look at this, the sin sin act. And then let's go through it. Overview. Sin Sinek. That's the handshake. See the local board talking to the 84th 80, which is this TV that 80, talking to the local board and then the local park talking to 80. So that's us again and then us sending. Right. You can also see it in the source and destination here, us sending a get request, which is the content which all fit into our segment. Right. And that will be sent will acknowledge that request. And then there is some more acknowledgement happening and then that we are going to send some more information. We're going to acknowledge the content of this is the content that we get added delivered. Right. And then finally we get Wireshark tries to help us by actually passing and show us at record about the result that we received. Let's start over. And then finally, the end of the day, fennec. Then this is the fin. Right. This is the four way handshake. All right. V-neck. V-neck. And then it's actually fin, and then fin and then ach and then ach. But wash our groups things together. Right. And maybe it's the way this is actually sent, you know, just save and peccadillo. I do. Said first and then ach, just send Fenner together. Right. And then finally act as though we opened the connection. We sent the data, we received the data from the server, and we closed the connection. Let's go let's go through this. Let's break it down. A beautiful frame with all the low level information about this, not really interested about the Ethernet. What is that about? The source frame? The destination frame. What do you think this is going to? This is my daughter. This is not this is not the example that go mac address. This is my adapter. We talked about this right. If if your IP address is not in the same subnet, you send it to your router. So that's my Mac address of my daughter. And then let's expand on the Internet. Look at this beauty. What do we have here? Version. I have the burden. Right. There's room for. The length of that or the internet length header 20 bytes. Which is number five. Right. So we didn't add an extra options or anything like that. Right. So twice is the default here IP header wise differentiate service field. Remember with that, to be honest, it will tell you like how I am easy and capable. I'm not easy and capable. That explicit congestion notification. Right, that we talked about that as well. Do we support this or not? Hey, I'm not supporting it. And I thought the length was the length of this IP packet ID zero because we don't have any fragments. But look at this. The flag, there is a flag, a set don't fragment. So curl which is the client that uses we use to fetch a vedge decided to set this bit to don't fragment again we're in the first segment since all right so they don't want to fragment probably for good reason time to live. Only 64 hops. 64 hops. And look at this. The protocol six. What was the UDP 15? Right. So now the IP is aware that we're on TCP land. So the IP has a header that tells you, okay, this, this, this value. That's the protocol that is inside me and the source and the destination IP. Transmission control was the source board decision for this is them. At this point, this is what we really care about as as as engineers with software engineers, if you think about right, because I use the source board to uniquely identify that so I can add it to an array and I can look through my connections and identify the clients this way. That's how I identify it and that's pretty much it. This is a sequence numbers and we talked about this is the zero is the first sequence number from the client side. The actual robot you look at that is so large, you know, it's a huge number. That's the wrong number. But the one shark shows us the relative one and the next second was number two. Also relative acknowledgment zero because we don't have acknowledgement here. And the flags, look at all these beautiful flags, conjecture, window resumes. Nope, echo ECN. Nope, nope. Acknowledged push. Nope, reset. Nope. But then Yasar said, we're in the sin land or spinning window. Guys, this is a very critical the window side. Remember this, the flow control stuff so this tells the client still on the server that, hey, we have a huge Windows 64, right? We have 64 key window size, not really that big if you think about scheme of things, you know, but this is it options. What do we have? Look at that. The TCP, it is the maximum segments. I didn't know that it's actually an option. The maximum segment size is 146 60. Guys do remember that. It's actually tells you, oh, that's fascinating. This is telling the server that, by the way, this is my maximum segment size and and this is probably calculated for my end to you it is five 1500, right. That's I can't go beyond that. Fortunately, I, I really want an empty or rather than 1500. I want to see what happens. I know I kept saying that, but look at this guy's window. Scale is six multiplied by 64. So this is that to the power six effectively remember the window scale because the window scale really is just a way to increase this. Bobby So the calculated window size is says 6500, but is actually way more than that. You have to multiply this by that, but this is only shared during the handshake. So that's why we see that value here. Oh, okay. More TCP options timestamps. We don't talk much about terms, but they are very critical selective acts, SAC, as they call it. Now, I again, we don't talk about that. But, uh, times two. And that's it. That's our sin. Let's go to the snack. Or do the server say. What did the server say? The server replied back, We can go through the IP packets, let's go through the IP packets and that's pretty much I'm not going to waste a lot of your time, but this is fascinating stuff. Time to let 57. And this is the time to live. Guys, this is the time to live that we see. That means this has been screwed over during the Internet. Right. Because it came from example to come to my machine. So this have seen, you know, the scary out word and it has been recommended. So maybe it was 64 and down to 50, 57. I don't know the protocol and all that stuff, the transmission and what it is, is the length, the sequence numbers, again, not adjusted, all that stuff and but here, send and ach. So this is also a snack. That's the flags, windows, eyes, 6500. Not urgent the options. This is the server telling you, hey, this is my maximum signal. Size is 14, 16, right? I support selective acknowledgement. This is not really rocket science. Let's explain. So selective acknowledgment if you have like segment one, two, three, four, five, six, seven, eight and you sent all these segments right from 1 to 10, let's say, and segment one was lost. But segment two through ten was arrived. In normal situation, this the client, the server cannot acknowledge any segments because technically because the first one has to arrive because everything is in order and transmission right in TCP. Right. So in this case, it cannot do anything. It will not say anything. The server will sit in its end and the client will end up sending one, two, three, four all the way to ten. But the server in this case, if you have selective acknowledgment enabled, it, will stay. I actually, by the way, are as if selectively acknowledged from 2 to 10. I actually received them a client so only send to be one. So in this case, the client was, oh, okay. You said, okay, I don't have to send 2 to 10. I'm going to only send one. And this is all the problem with ordering of actively. Let's continue. When to scale. So. The window scale of the service is larger than us. It's nine. So we multiplied by 55, 25, 12. So to devour nine effectively. Bom, bom, bom, bom. And then this is all this stuff. And then ask. Asking the clients that can finishing the handshake. And then the client sending some data. What is the kind? The client sends. Let's go to IP client IP. Again. The client is eminence that do not fragment. They don't want anything fragmented. Right. So because of that, if ICMP is disabled in the new using kernel, you might you might get into a TCP black hole where you're that the handshake will definitely succeed because you're sending small packets. Right. Unless you have a ATF override. TCP fast open. I we talked about all that stuff guys. I expect to to go back to the lecture if you want a refresher. But yeah, so the sense and act are so tiny that they'll be any chance to to fragment so they will succeed. But now the guilt, of course, can be really large. So if you said don't fragment and we reached somewhere where we need to fragment, hopefully we don't because now the maximum segment size, we know the client and the server understand that and they want to send something larger than that. But let's say you want to throw a different path. And that path happened to be a very small m to you. And on 500, right. You need a fragment. That router will tell you, hey, you told me, don't fragment. So I'm sorry. I'm going to return fragmentation needed. And if ICMP is disabled in any of these nodes P, you won't get that message and all of a sudden your packet went and all of a sudden your packet will never reach the server. And that's called the TCP blackhole so you connection succeed for but why is my data is not arriving. So that's a that's another debugging tip for you. Boom, boom, boom. We're talking about transmission control, all this beauty flags or pushing data. Right. Every time you want to send data, you have to set the flag push. So from what I understand and by the way. You're going to always see ACH because hey, might as well just act. Right. Even if we send them back before with every data. Let's just sit there a bit because it's just it's literally cost us nothing. The beaches there and the flag is their acknowledgement. So and this is just in case the acknowledgement alone didn't arrive. This is if you send data, you can acknowledge it, hold it together. So this is just a protective kind of a scenario. Look at this. The window size of the who is this now? This is our strike client sending get request. It's 2000 bytes. Very tiny. If you look at this it will be receiving the. This is my current window it oh my God. Why? Because I multiply by 64. How did I get 64 from the handshake two to ball whatever tomorrow six was it forgot already. So if you. Oh, what happened? So if you multiply this by this, you're going to get this large number. That's my actual flow control window name. And you were telling this over that so that the server knows. Oh, all right. Again, this is done by shock. Thank God for whitewash. I was doing a lot of our job for us. Agent All that jazz, you know, it's not urgent timestamps, right times those like really critical sounds like right and it's on by default. And then obviously the actual get request if you're a front in or back in doing you know this is your bread and butter right get you know this the method slashes the path and this is the protocol. It should be one one and then the header is the host. What are we going. We need the host really for proxy seeing reasons and as an I reasons you know for one to host multiple servers on the same machine. Yeah. Multiple websites so we can go to the same IP address, but I can specify different host so I can have example one dot com example to this example theta and all are hosting in the same IP areas. Beauty of web hosting. I think I'm going to stop here. You get the gist of this stuff. You know, let's just go the thin route and then in this video. So this is the fun. We're closing the connection. So we said the fun bit here. Hey, I'm closing. So and that's how we close connection. And then look at that. Our sequences keep increasing with the amount of data that we sent, right? So at the end of the day, the sequences will reach the end, right? What are two of the 32. Right. Which is how many? 4 billion, whatever. So you reach that and you might cycle back and you might at the end of the day, go back to the same sequence. If your connection is so long lived, you will do that. And that's where timestamps comes into the picture, where you need the timestamp to differentiate that. Or I did receive the sequence 300 and. And one days ago you know so I from the camp same connection so to to avoid replay attacks and stuff like that and corruption you basically need time stamp to identify who is which was the newer one. Oh this is this is a new I trust this one in one I guys I'm going to leave it at that hopefully and I enjoyed this lecture let's I'm going to see you in the next one the guys they also to by.


### Wiresharing HTTP2 (Decrypting TLS) vtt

What's going on, guys? My name is Hassan and welcome to another episode of Wireshark. Them all we we where we Wireshark every single protocol in existence and look at every single packet and understand what's going on in the back end because we are back in engineers and we need to understand everything is going on. So today we're going to Wireshark http two and guys http two is secure by default because of protocol ossification, because we have a lot of dumb routers on the internet and those routers try to be too clever by half those boxes, try to be too clever by half and try to intercept the traffic and do stuff. If it's unencrypted, if it's port 80 specifically, should it be so? It should be too. Has to be an encrypted. Encrypted for that reason. And because it's an encrypted, we can't really. WIRESHARK It can we now we cannot. So thanks to Daniel Stenberg, the creator of Curl, he created a blog on how to decrypt stuff with Wireshark. Right. By giving providing the keys. How about we jump into it, guys? Let's show you how HTTP two looks like while it's encrypted in Wireshark. So this is Wireshark. This will be my destination CDB to server node and then obviously the source IP address. I guess I didn't put it the source. We don't really care. But then if the destination and the port is four for three, this is the Raspberry Pi that is running a node node actually a node server running a CDB two and let's go ahead and do a curl. And since it's insecure because I have a self signed certificate, I had to do this and then do setups. They do 192. I think it's just Raspberry Pi one. I think that's it. Boom, just like that. If we do that and go back to this, look at this. This is what happened. We opened a brand new connection DHCP and there is the TLS and I'm using TLS 1.2 because my Raspberry Pi have an old version of whatever open SSL, Libra, whatever, and that doesn't have TLS 1.3. So it uses TLS 1.2 for communication that does hello, all that stuff. And then everything is just application data, application data, application data which is encrypted pretty bad ish. Right. Otherwise we cannot see stuff, right? We cannot see the HTTP two protocol. It's everything is encrypted and until the end that's the only thing we can see. Right? So what we want to do is do the following, go to curl and there is a key that curl and also most browsers listen to and a little bit dangerous, but it will spit out all the keys it used in the communication that if Hellman symmetric keys and all that stuff that generated by different helmet and spit up in that directory. So all you need to do is just do export and I think it's called SSL key log file and you say equal that you do export in in Linux and Mac and Windows, it's set path right or you go from right to click this PC and go to environment variable global and set that key. And then you say where's the where do you want the key? I'm going to put it here. Users. Hussain Nasser I think I have a folder called Tim Keys and I'll call it at Key, literally just just like that. So now this is the path. And now if I do a crawl. Daniel will read that and we'll spit out the keys that we use for symmetric encryption in that file in the puppy. So now if I copy that puppy and. I go to Wireshark, play this stuff and go to I believe it's Wireshark preferences, protocols and then go all the way, find TLS, find TLS boom. And then there is a pre master secret log file, right? And then you just say, yo, go directly to the path and then just go poof. Select that key file. See? Did you see that? This is there is the key file that got generated and called generated at for us. Right. And then we do okay. And now if I go back every time basically we generate we we make this call curl will always write the new keys for us. And when it does, Wireshark will actually read those keys and like this beautiful thing. Look at that. HTTP two. Beautiful. Let's go through that now. Now that we had decrypted Wireshark, we can actually look at this stuff and explain everything. How about we jump into it, guys? All right, so what do we did? We made a get request to the index page. So this technically is just a single request. So what do we do is beautiful? Send ach. All right. And then client Hellotel is 1.2. So it's going to be longer, right? It's going to be 241.2 because basically. Right. Because we say, hey, I support this, I support that, I support everything. Let's use this for key exchange. Let's use this for is just so, so much madness. Get to the point. Get to the point. Right. It's just get to the point. TLS 1.3 gets to the point immediately. The US 1.2 is this okay? A server. Hello, here's my certificate and then let's agree on symmetric key for this algorithm. Let's agree on on on key exchange. All these are I go to them and says, okay, we'll use development or RSA publish. Hopefully we're not using RSA for a key exchange because just a bad idea not perfectly forward you that key exchange change over stick and finally the first packet of data look at this beautiful thing the client now this is the server actually. Oh the server since the first packet. Interesting. The server which is a one sends the client which is moi. This is ten, it sends an entity packet and thanks to Ms. you Wireshark, we can now actually look at the packet and look at this. This is the most important piece stream ID so stream it reserved apparently from my my initial research for settings and windows update and and and things window update that the packet sizes and things like that for system management so stream zero right so now server says okay here's me, this is stream zero. Let's start using this channel. Right. And then we talked about V two that that single TCP connection have multiple streams. So, so obviously the client says ACH and I'm going to ACH. It's probably this ACH is, is for a previous TLS thing and like, like this magic. I'm going to explain that the client sends a magic bytes. It's called magic bytes. Sometimes magic numbers, sometimes called connection prefix. And the main reason of this is to confuse every single HTTP server out there. The main reason we're sending that is that if there is if the back end server supports STB one, right, we we will confuse them by saying, okay, so let's send some random number so that all bad clients write bad servers that are receiving this stuff will freak out and disconnect immediately. Right. So this is the kind of of a cleaning the pipe, the clogged pipe kind of a thing. Okay. Just send it over. So we know that SDB two, is it required? I don't believe so. Because during TLS. Hello, by the way, guys, we in the during the handshake, I hope it's somewhere here. It's called the application layer protocol negotiation and we say hey during TLS hello I support H two server if you support H to let's connect to this too. And when the server hello starts they say okay, I agree on edge too and they will communicate with edge to it. There you go. So Server says, hey, let's talk too because I understand issue two. So I don't believe we need the magic. Right? I might be wrong. The magic number here. The magic packet is sent because of another method that is basically the HTTP upgrade. So I don't think personally we need that to be honest because we just agreed. Why are you sending me an extra dump package? Right. That might not be the case in case of like proxies. Right. But if a proxy supports the B two is going to respond. Well, it should be two. So I still. I don't think we need the magic. Right. Which is that number. I might be wrong, though. The servers now. But look at the stream, guys. What did what stream did we use to send that stream? It's its own stream. It's a magic stream. How about that? Oh, if I continue. Now, look at this. Now the sitting is also on stream zero. Now, this is now the client saying, hey, I support maximum of 100 streams. You can put. You can send. You can establish. Maximum 100 stream. Who is this? This is the client. The client supports up to 100 streams. So that's curl send saying, hey, I can create 100 streams at a point and send it all over to you. Right. More than that, I don't support. Right. This is obviously a logical thing that the client and the server agree to. That's part of the setting and we send it to stream zero and the server sends back. This is the server right now. This is still the client. This the client send on the stream zero, which is the same channel, send some more information. The Windows update is like this is the my size, this is the stream identifier. This is the it's like the TCP agreeing on the window size. But at the at the higher level, a higher layer. And the first beautiful piece of data we're sending the headers and guess what, the headers are sent on a different packet then the data self and we know why guys. Right. Because we need to send the headers first so the server can, can make the decision before we actually send the data. And that's very, very, very important. Right. So look at that actually. What, what stream did we use? We use a brand new stream because now we're sending actual data. When I say data, I'm talking about the logical part of it, right? Stream zero is reserved for setting and system stuff. Stream one is the first request that we sent, which is the get request and we sent as part of it what we sent the data, which is the headers right as part of it, OC headers length, we send the get request. Obviously this is what we talked about. Path slash squeamish DPS authority. That's the server. Raspberry Pi one user agent. Look at that curl I'm using 770 everything basically the headers and beautiful Wireshark just shows us everything because we decrypt it right settings. What is this the server uses still anything settings has to go stream zero right. And now now TCP takes action because we send a lot of stuff from the client. So the servers just say, okay, aq, i aq and then it sends some more setting stuff just to agree on the streams and agree on stuff. Right. Still on stream zero. Right. But did it receive the headers yet? We still don't know. The client acknowledges the settings. Right. So there is a lot of chatter going in this in the stream zero. The system stream. Right, whatever is called. I just made up this name by the way. And now the server 81. No, there's a client. No this is the server. The server. Look at that. The server actually starts sending the data because what is the get. The request doesn't have anything. It has only headers most of the time. Sometimes the URL parameter, if you have that, it's going to be snuck out as part of that the data packet but header the data, the records don't have body, right. If it was a post request then we might send another request. But look at this in stream ID one. We know that I'm using the same stream for request and reply and response. So you don't have this problem of TB smuggling or silly stuff like that. Feel free to send as many stuff as we want because we tag every request with a stream ID. Right. All right. So now we're sending back I'm sending back my headers application dot JSON data. This is all my headers. And obviously what what Wireshark does here is it group stuff together as much as possible. I see it says reassembles two TLS segments into one just for us for convenience. So this was probably two segments coming. But Wireshark just group them together so he can show us to us, right? Yeah. So hypertext transfer protocol number two. And this is the data. What do we send? What did we send, guys? We send the actual data. And if we looked at this part like that, that's the complete part of the data. That's a beautiful JSON that we get. I guess I didn't show it. Huh. It's it was that's that's basically the result. It's the JSON object. All right. Again, on watch stream a stream one. All right. And the client says, yo, I acknowledge it. And here's some alert that says, okay, we're about to close. I think this is a TLS 1.2 things because Curl just closes the stuff because it sends all the data, right? So the client says Fin Finnick and the server say Fin V-neck and then we exit. And there is one packet there which is out of order and that's it. So that's. One request. So we got two streams. One stream is the system stream stream zero and then stream one, which has our first request. What if I send to request guys in the same TCP connection in parallel? Can I do that? Of course you can, my friend. Let's do it. Boom, boom, boom, boom, boom, boom, boom. We do curl insecure, and then all we do is add a space and do the same thing. Raspberry or raspberry berry pie one. If we do that, curl is smart enough to know that step two is going to establish a TCP connection. It's going to create one stream for this request and another thing for this request. Right. So can you get the stream numbers exactly one and two? Because zero is system. Let's try this out. Clear this out and let's do it again. Boom, boom, boom. Let's do it. Three way handshake. Hi. Hi. Hi, client. Hello. All that jazz skipping setting server sends. Hey, let's start the settings again. Stream zero. Magic stream zero. Right settings one does. I just show the magic. Is it like it's own stream? I'm. I think it's own stream, huh. Because it didn't say stream zero. Might be wrong there. All right. Setting zero Windows Update all that stuff. And here's the thing. First request, first request on stream ID one, setting the data get. Look at that. If I go, that's just part of the settings. I'm going to ignore this and the data responds back on stream one. So we send the get request and that's the response of the get request on the same stream stream ID one. And that's just this JSON data, right? That's the application data. Look at that and look at this thing, guys. Stream ID three. Why is it three? Okay, that's interesting. We skipped stream ID two for some reason. Right. I'll, I'll take a look. I think it's a bug to be honest. Oh, my b I'm missing something. All right, so that's the second request. So stream ID three, the connection is still open. We're just sending streams, right. And then we go there and receive the data back. Right. That's the data. Awesome. Got back on the stream three. Right. Get the BAC data stream three. Acknowledge alert and then we close the connection. Can to see in the next one. You guys stay awesome. Goodbye.


### Wiresharking MongoDB vtt

What's going on, guys? My name is Hussein and welcome to another episode of Wireshark them all where we Wireshark every single protocol and existing and back end engineering. So how about we jump into it today? Today we are Wireshark working MongoDB is going to be extreme fun. So MongoDB guys is a NoSQL database that's a key value store and I already spun up Mongo Atlas database with a user and, and all that stuff. They, they give me a free instance with three three shards, three clusters, one cluster of three shards. And we're going to just basically run this code which connects to the database, does a little bit of sleep just because I want to see what's going on there. And then connects to the database called Thunderbolt, which I created there. It gets the employee and literally find the employee that is named Hussein and then execute a search cursor. And then once we get the search cursor, we print it and then after that we close the connection. So I'm going to show you that. And then slowly I'm going to start breaking up the connections one by one after each operation. Right? So how about we do that? So here's a Wireshark I filtered based on the Amazon's public IP addresses, which is the which is the provider I'm using for Mongo Atlas for MongoDB Atlas. So that's the IP address. So now anything going to that IP address will be logged here. And another thing I did, I use SSL key log NPM package for Node.js to spit up the session TLS keys because MongoDB is always encrypted by default. Golf upload for MongoDB team. That's awesome. Right? So because it's encrypted, I won't be won't be able to see it in work unless I decrypted. And in order to do that, I split up the key logs, write the key log, which I showed how to do it in http two video when we wireshark STB two and once we do that, I said that that key ta ta ta ta Wireshark And the TLS option here, where is it? Right here. And that basically will allow Wireshark to decrypt the keys because no one in the middle can actually decrypt anything. Right until unless they have the final exchange keys. And this works on any cypher, whether development, elliptic, curve development, anything. Right. Because you are at the end, you're at the client. So you do have the keys. This is just an option for for for for Node.js to spit out this key so we can use them for debugging purposes. All right. How about we jump into it? So I'm going to go ahead and run. I'm going to go ahead and run node test dogs. And what this will do, it will literally connect. Spence. Brent and then close. That's it. So Prince Hussein, which is the employee we found and then close the connection. Let's go to beautiful, beautiful Wireshark. Look at all this stuff. So that's what I love about Wireshark. It actually know that we're dealing with MongoDB. Look at that. It actually knows the protocol. It does all that stuff. So how about we go through that stuff and then slowly after we go through all the operations until we close the connection? Where's the fun? Right here. I don't see Fin. What is the fin? Fin? There is a fin. Yeah. Until we close the connection, let's go through that stuff. So Mongo uses TCP. So obviously, since then. Ack ack. So that's the three way handshake which we talked about. So every TCP connection has to be first agreed on the sequence number that we will be used. It's basically a number and you have to to agree for client sequence number and a server sequence number that will be used to label those packets that you send across the wire so they can be reorders, so they can be retransmitted if they are lost and things like that. Client Hello TLS 1.3 Go beautiful one Go look at this. There's one with the three and this is my mongo client, Node.js, right? And so look at all this beautiful cipher suites, powerful suites, powerful suites. Look at that stuff it elliptic curve two for Hellman. Yeah. There is a Diffie Hellman one. Right. It's all right. All right. That's good stuff. Good stuff. Good stuff. You can use any of these encryption methods, cipher suites, methods, right? And then since we're using TLS 1.3 sort of her name is and I will go into this shard so shard 0001 that's the primary chart that we're hitting and then doing all that stuff. Support, supports, support the session tickets, that's part of the TLS or is it part of Kerberos? I'm not sure about that. I don't think you can do Kerberos until you support a version. This is the most important thing. We support 1.2 and 1.3 and we send that stuff right. I'm not going to go through all that stuff. We went through the details. The server says Let's use TLS 1.3 to communicate, but we have to say 1.2. Unfortunately, because there are dumb routers in the web that unfortunately blocks the USB 1.3 on the version. So this is fake. This is not really two years. 1.2, right? The real version somewhere right here where we actually say, hey, it is to us 1.3 supported version. So there you go, it's under supported version. That's where we had to add a new kit just to do that stuff. All right. So we exchange the key and then acknowledge, send the certificate, the server sends the certificates and all that stuff. Verify the certificate. That's the server and acknowledge, acknowledge some TCP loss. And there is this. The first thing we send a query. What is this query? All right, Mongo wire protocol. We're using the wire. Mongo protocol. So it's a specific protocol for mongo. It's around 300 bytes. Not that big. That's good. And look at this. We're querying a database called admin. Technically, I didn't say any of that stuff. The admin database is not created by me. I didn't create it and didn't do anything. And there's no collection called Dollar Sign CMD. That's probably some some built in collection that we just apparently use. All right. And then we query this is the thing we ask for. We ask for so many other information which as for the compression type we ask for, that's the SSL, forgot what it stands for. This is the the LDAP alternative I forgot was stands for the master is is the one I'm connected to is the master. I want to connect to the master client. Can I say the driver I'm connecting through from Node.js and this is the version of the MongoDB client and all that stuff you look at. That's compression, what kind of compression I'm using, and that's it. And the server obviously acknowledges that I received your query and then sends back the reply. And the reply in the reply says, Hey, here's the list of hosts that you can connect to. This is awesome, by the way, guys. So this is the three shorts that we talked about. So I connected, I believe, to this one, this is the master and we're going to find out. And they gave us like, by the way, this is this is another two shards that you can connect to. So it's almost like a client side sharding. Right. And that's how MongoDB works. The client site is aware of the Shard. All right. And set name the atlas we're using. Atlas is master. Yes. You're connected to the master or you connect to the secondary. Nope. Or you connect to the primary. This is the primary one is the primary. Right. And that's all that other stuff. Look at that electable. Whether this is elective or not, a W is region. Where? Where is it? It's in the east. All the information is there. By the way. This is encrypted, guys. But I managed to decrypt it because I'm using that we talked about right. Like that. What other stuff? Election ID don't care about that really late last right. Max Bhasin the binary JSON that is that it. I think so. I'm not an expert in Mongo, so I don't know what what all this stuff is. Most of the stuff I know basic stuff the the versions meanwhile are version max erasure. This is the response. That's a little bit of a lengthy response, huh? And that's it. So query. And here's the thing, guys. You'll notice that after that. The client also sends another query, and that's what left me a little bit baffled. So we're sending another query around 124. And what we're asking here, we're doing this SAS cell start thing, right? So let's let's Google this. All right. So it's simple authentication and security layer protocol. So simple authentication. Security protocol. So that's the protocol that they're using. SCRAM. That's another hashing algorithm that Mongo uses. And apparently this is they they are agreeing on this stuff. I didn't know a scream is I had to look at this scram. So it stands for salted challenge response authentication mechanism scram. So this is apparently an authentication mechanism and there are two there are many types there shall one. Why are we still using sha one guys? Cha cha one is dead. It's been proven to be weak. So an apparently. What am I using here? I'm using SHA one. I probably is not a bad idea. No big deal because this person has to also decrypted this in order to get into the show and to to to break it. I don't see I don't see it as a big deal unless you guys disagree. What do you think? So, yeah. So this is the first societal query. And then it replies back and it says. We are having conversations. So this is like a conversation. So there's a lot of shadiness just to establish the MongoDB connection. Hmm. All right. So we have done. Are we done false? We're not done. So it's almost like a conversation. I don't know the protocol and how it works, but this is extremely chatty. Look at this. We're going to see that this is the first query reply. The second query and reply. The third query almost the same thing. We're going to the admin cmd collection. We're just having the conversation again. There is some negotiation that's going on and then we get a reply back and it's not done yet. And then until the fourth query, the fourth round trip, we say, Yo, let's continue hassling. And then we say, done, and then we're done. The connection. So this I believe this is just the connection part. So we're and we're going to see that later. When I continue that, I think we'll just hear. That was just here. So a little bit slow, if you ask me. Right. And this is the first request, request, extensible message format. Is this the XML? Is that what Extensible Message format is that just XML? That's what XML stands for. Write, Extensible Message, Markup Language. Maybe it's different. I think it's a different thing. All right. So what are we're sending the query. The user is sending the query defined find. Right? We're sending a find. That's the find right here. Right. So this we're going to find out that this doesn't do anything. This is all client side logic. We're getting getting the database, getting the collection until you want to do something. It's the client. And Mongo is lazy is a lazy load approach, right. Where I found out by actually stopping in each step and there are no network packets sent when you're doing this stuff. Only even here you don't do send anything. It's only here actually we're going to find out that right. So find we're going to find. And on the employees collection, did we specify the the collection here or the database? I mean. The database that we specify the database, we specified the collection, but none of the database. So that's the filter. Hussein, I want to go to who find me Hussein. The name Hussein. Retain key show document. Look at all that stuff, man. The timestamp. And there's the DB thunderbolt. Right. Awesome. And we acknowledge the server. Say, I got your request. And now here's what I think is this is just weird. It's not really a request because it's coming from the server, so it should be a reply or a response. So I think this is a problem in Wireshark working. Wireshark just expanding the messages and treating it as a request where it is actually a response? I think so. So we're going to cursor, a beautiful cursor. This is a client side cursor. We're going to show that because we asked for everything, basically, right? We ask for to array and we get back an array of every single element, which is one basically, and the ID, the object ID of the element. Right. And the string, which is name Hussein. If you notice here, this is 5f2, 5f2 and the name is Hussein. And then we get the the collection and the ID and all that stuff. Okay, the cluster time we're for, for some reason, we're exchanging the cluster time a lot. And I don't know what's the purpose of this. That's very interesting. I think it's just for syncing purposes. That would be interesting to know. All right. Some lost packages. No problem. Another request. Hmm. We didn't send another request. That's weird. So if you look at this request, that request. I didn't do that. Just say in sessions. It's like. Almost like it's ending the session. Well, that is a document and says IDE this gives you a value a for a five, eight, eight. I didn't do any of that stuff. Only thing I did is print it and just close the connection. So maybe that's part of the closing the connection and that's the client sending it, right? And that's and then the time stamp and we're going to all look at this admin. I didn't do that. We're doing something to the admin that MongoDB client is sending a request to the admin. Oh, look at that. This is part of a negotiation with the shorts. Look at that. Says, Hey, I prefer the primary. Well, we are connected to the primary. All right. What's wrong with you, sir? Oh, awesome. No, no, no, no. This is good. And then we get a response back from the server saying what? Server saying OC one cluster time. Don't really care about the cluster time now and the operation. That's nothing. It's just responding back with a response. So I didn't send that as a client, as a user, as a developer, I didn't write that. It's all Mongo. And then obviously this is the close of the connection, the fin or is it fennec so that the client initiating the closing of the connection. Look at all that stuff. All right, guys. So here's what we're going to do now. So we went through all that stuff. This is a very basic MongoDB request to pull all that stuff. How about we spice things a little bit and I'm going to copy this thing. And do it right here. Right after the connect, I'm going to close the connection. And I was sleeping here just because I noticed that. I think that it's almost like a bug here. When we don't close, we close immediately. We get a reset on the can the on the on the TCP stack. So I'm just adding milliseconds so we avoid the reset. So let's go ahead and clear, which we shouldn't get. If you think about it, we shouldn't get this reset less than. Then one second. And then done. Let's take a look. All right. Three way handshake. TLS handshake. All the way. We get a query reply, a query reply, a query reply, a query reply. That's where we show it. Right. And then often. So for round trips, that is, man, if you the next time you establish a DHCP connection, a MongoDB client connection. Think about this as a back end engineer. Think about what you're doing. Right? So pull this connection as much as possible if you can. And then the eagerly load them because lazy load this thing they use there will feel it because it is extremely slow, especially if you're if your MongoDB is all over there. And this is another thing. Guys like your MongoDB clients should be very close to the to the to the MongoDB database. Mine is mine is in the West and a client is the West and the database is somewhere in the east. Right. So that's bad anyway. But all right. So let's move this a little bit. Just this sense for four packets. Yikes. And now I'm going to. Do it all the way here. Right. So this, this and this. And you've got a guy is going to notice that nothing I'm saying nothing will be sent to the to the server after that. It's the exact same thing. Exact same thing. Right. So. Let's go over them again. Query. Reply. Query reply. Query reply. Query reply. And that's it. And Finn? So whatever we did here, all this work, it's all lazy, loaded. All of this is the lazy loading client side, which is good thing, right? So this is to avoid extra roundtrips. Right. And DB Thunderbolt. This is. Yeah, give me the Thunderbolt database. But why do you if not, we're not going to use it immediately. I'm not going to send a request to get you a thunderbolt. All right, so let's let's pool this connections that cost as much as possible. So you want the Thunderbolt database, you want the collection employees. Okay, I'm not going to send the request yet. You want to find the the employees that name Hosain. Okay, I'm not going to send it yet. Why? Because technically, you didn't consume the search cursor yet. Because what other methods do you have, guys? It. Is it next? Yeah, I think it's called next. So this is next. We'll do something else than to array for example, right. Once I do next or to array that will do all of this in one trip. So they're kind of optimizing things which I'm going to give them. I'm going to give them the props for that. That's good. But look at this. Look at this stuff. And here is my message. Request, message, request. Here the request extensible message format. So that's the extensible message format that we sent, right? That's our query. And the second one is basically the response for this, right? Well, that's that's the first one. Sorry. That's the second request that that the client just does send with that. We don't know what's going on there. It's sending it to the admin database. So some sort of messaging. So this is like I think the exchanging of information about the cluster, about the stuff like that. But yeah, that's the idea of this stuff. And then finally we close the connection. All right, the last thing I want to do is something I didn't do offline. But I'm going to do it with you because I'm I'm very courageous. Not really is I'm going to try this. I'm going to see what happened when I do this cursor dot next. So this will give me the next result. We'll see what will happen when I do this. I'm going to resolve all this. Let's just do this and see what will happen. And it's just. Sure, let's just print it. That. Oh, it's clear. Clear. Clear. And then. Boom. So the normal queries, no more queries, normal queries, extensible message format. The same thing. It's the same thing, guys. It's the same thing. Maybe it's not really an array. So this is the request. This is the response. Are like, that's a little bit different. The cursor. It's first batch. It is an array. I guess is the same thing. All right. Never mind. It's just giving you the same results, right? But just one at a time. So. So this is. This is a whenever you want something, you just ask for it. And you get the second one and the second one. Second one like that. So there will be a roundtrip for each one of them versus if you do two array, they will all be brought locally. So. Kind of a server side versus client side cursor. All right, guys. That's it for me today. That was Mongo. Wireshark, MongoDB. I'm going to see you on the next one. You guys stay awesome. Goodbye.


### Wiresharking Server Sent Events vtt

What is going on, guys? My name is Hussein and welcome to another episode of Wire Shark Them All where we basically wireshark every single protocol in existence today we're wireshark working server sent events so it's it's kind of a derivative of the HTTP protocol, but it's a fancy little tool that allows the server to send events to the client. Right. So it's a uni directional. So I have here a Node.js server listening on port 1880 and it doubles as a server sent event server and it's running on express. And if you make a call to that to certain endpoint is going to stream certain events unlimited. So what I'm going to do is I have Wireshark here already put in the necessary filtering and ready to rock and roll. How about we jump into it? So I'm going to go a new batch here and I'm going to curl curl supports server sent events because it's just literally transfer chunk encoded. That's it. So I'm going to do a HTTP. Slash slash Raspberry Pi one my Raspberry Pi, which my God, it helped me a lot in all my videos. Right? This piece of PC, this machine. So I'm going to do a just curl and HTTP and raspberry and slash stream I've always started doing and we started receiving unlimited stream. So this is going to go forever, right? I'm going to let it go for a little bit. And as I you can see, you can start seeing all these packets come in, write all of them. Right. And I'm going to wait for a little bit until it reaches like 20 and I'm going to kill it right now. And then that's it. That's enough. That's enough. Beckett, this is what we got from the server. How about we jump into it, guys, and let's just look through what we got. First, three beautiful things. Send Snack Act. That's the client, which is more ten, and the 81 is the server and three way handshake. That's a normal thing. The first thing after the three way handshake is we want to establish a pure TCP connection. And does the does Wireshark help us? Yes, it does. Right. It actually tells us like, hey, this is the jet stream that you send we about to send to Kroll, and that's it. This is just a normal call. But the response that comes back from this git request is the tricky part here, which tells Curl to act like a client, a dump client that receives server side events. So we're acknowledgement here, the server acknowledges that request and the server start pushing stuff. That's it. The first data that it pushes is the largest biol is 184. BYT And it includes all the stuff that we discuss in the in the server center events. So powered by headers and content type text event stream, it has to be text event stream and that triggers the server sent events, the client to start to stop everything and just start listening for events. All right, then the date and all that to keep alive had to be alive over and transfer chunked encoding chunk. We're sending chunk because we don't we can we cannot use the content length guys like can we now because we don't know what, what is the size of the response that we're going to get back from the server. So it just took a chunk of events and the first one is a little bit better largest one, but that's bathhouse. That's pretty much it. The second thing we're sending, what was that was a bunch of hellos and a number that keeps increasing. So if you calculate that's the client acknowledging this first event that we received. Right. And then the server sends a 20 byte packet, which is extremely small. And that's one advantage of the server side sent events to compare to like web socket, which is like a little bit larger payload because we have web socket headers and stuff. That's that hello one. And then the client acknowledges it and then Hello two and still it is 20 bytes and you can see the same pattern. That is a three transmission but same better duplicating acknowledge it push, acknowledge, push, acknowledge, push, acknowledge every time everything is server sending this stuff 2020, 20, 20, 20, 21. There you go. We got 22. Why did we get it? 22. Because we reached 11 essentially. So two extra byte for a single measly digit. Hmm. That's probably because it's a Unicode, I guess. Yeah, that's probably so two bytes instead of one. So. Yeah, same thing. Same thing, same thing. Look at that. That's very interesting. I just kept sending the packets. And this is just Wireshark trying to make sense of what is whatever is coming back. But it cannot obviously, because it's like it's not really a step per se. These are just packets, right? And here's when we actually killed the connection in the client side. The client says, Hey, Fennec. And then the server says, Fennec. And then the client says, Arching your fin. That's it. That's server side events. It's really lightweight. That's one of the beautiful things of it. It's it's really, really lightweight. The packet sizes are really small. So if you if you design your own protocol using server side event, you could save a lot of packet sizes compared to like web sockets or HTTP. Right. Yeah. The only, the only hat you're going to get is the first one. What is it? This one, which is like 184. It's not really that big either, right, guys? So that was like server sent events, wireshark ing them. Help you enjoy this video. I'm going to see you in the next one. You guys stay awesome. Goodbye.


## Answering your Questions


### Should Layer 4 Proxies buffer segments vtt

Hey, guys, what's going on? And welcome to our first Q&amp;A session. For the network fundamentals course. So I do this usually on almost all my courses where every month I would pick some of your questions in the Ultimi course and then elaborate on an answer. So I try to answer them in writing as much as I could. But some of the questions really deserve more attention, you know, more time, more elaborate answer. So that's was this show, if you want to call it about, right? So there's a can you name it, so you get it monthly. I try as much as possible, but this is one for the April 20, 22 for the networking course. So let's just get a started for this particular question today. This question comes from Fano. Fano. So this is on the OSA model lecture at 528. Does the layer for proxy transport layer weight all for all the segments of the TV request or to track it transmitted one by one. So I all I answered here. All right. But I want to own kind of clarify what that question is or a little bit here. So we're talking about proxy ing, right? And proxy can happen in many, many layers. Right? So at layer four, we see TCP segments and vitamins. And you should really watch this after you've finished the course. So you have a whole idea about kind of the basic fundamentals, right? So in layer four, we have access to the ports, we have access to the IP addresses, right? And that's pretty much it. We can't really access the content because the counter could be encrypted by higher layers, the application layers. Right. So that's that's the only thing we have access to. You know, if we go a little bit higher, we can have access to the connections and the metrics and the counters of the file descriptors and and how long the connection was alive, all that stuff. Right. But we don't. So I layer four segments and port. So the question is should I wait when I receive these proxies, when I receive these segments, right. I need to take this segment and for them to do the actual destination. Right. Yeah. So I like waiting to see that question anymore. So. When there is a layer for proxy or a reverse proxy or a load balancer, the IP address that. It receives is itself. So it's like you're talking to the load balancer directly. Right. But the load balancer itself need to turn around and send whatever you did to the back end, to the actual real back it. Now in layer seven. We need to literally read all the data that we receive and buffer it in order to make sense of it. Because an FTP application layer could get requests. Could be. Composed into multiple segments. So we need to read all the segments, buffer it up in the proxy, in the load balancer and move up to the layer. So, so that's layer four. So now move up layer five and there's the connection, terminate the connection because it's a brand new connection and then move up all the way up the move to layer six, do the decentralization move all the way through the layer seven, layer six. The character can describe the tiles because you have to you need to look at the data. Okay, so layer seven and then you move all the way through the layers of application and then make sense of that. And then only then you can route the request and rewrite a completely new connection to the backend, new segments, everything is brand new. You just write it to the back end, right? So there is a new connection, nothing else new. Everything. And then. You can choose based on the higher level protocol if it's stateless or not, to load balance these requests. So if you receive one request, you can go to this back end if you're safe. And that, of course, you can go to this back. That's fine. But in layer four, you can't do that. If you establish a connection with the load balancer, a layer four, Lloyd Bentsen, layer seven, then it becomes the final destination when it comes to layer four and layer five. Right? It is defined as the initial when it comes to the connection that your connection ends here. That's true. But the layer four load balancer need to turn around and talk to an actual back into. Right. So it needs to re-establish a brand new connection on the back end. And here is the thing. Because it doesn't know the content of that connection. It is responsible to make that connection sticky. That means any segment you receive. To the load bands are on connection one and this connection is uniquely identified by the source partners IP Destination Port Destination IP Destination port is the load balancer port destination IP is the load balancer IP. Then this unique connection mustache to one and only one. Back end you cannot send when you receive a segment on this connection. Right. It has to go to one, two as a certain connection to the back end. And if you send another segment on the same connection, it always has to go to the same backend and same connection to the back end. So there is a path that is sticky that you have to follow it all the time. Right. So that's that's the difference here. Now it's up to you as a designer of this load balancer. And let's go back to Thanos question. Should we buffer all the segments in layer four? And send all of them right to the back end? Or should we? When we receive a segment, whenever we deliver a segment, we write it to that to the back in connection. It really depends. One. Implementation can be just, hey, whenever you whenever you receive a segment decided that's the fastest if you think about it, right? Because if you buffer there is a latency, you're buffering, you're waiting. But sometimes sometimes we need to wait. All right. And the only thing I think I can think of when it comes to this, because there are maybe use cases I'm not aware of, but one use case is if you empty you on the front end, which is the loud bands that a client side is different than the MTU on the back end, which is the load balancer back end. And in this particular case, you need to buffer those segments. Wait, wait, wait, wait, wait. And then once you have enough for that. Destination empty you. You can write one big segment that fits the back end. Empty you write. So this way you buffer. So you like, let's say three segments or five segments. You can wait for them. And then you write one segment that represent the five, right? Assuming the back end to use larger than the front and empty you. So that's one use case that I can think of. There might be other use cases where you need to read and wait for certain information. One comes to mind is as IP or fragmentation, like when there is a lot of IPS that's coming right. All IP packets and these IP packets are fragmented and belong to one. Then you really need to buffer it up because your back might not support fragmentation and you don't want to really support back fragmentation on your back and you let the load balance or do it, for example, or the reverse proxy. So that's done a lot of use case. You just wait, wait, wait, wait for all the IP packets and just write them in a single if possible in a single segment. So that's a fantastic question. As you design these kind of back end, these fundamental networking really comes in handy. Once you understand all these kind of things, it becomes really interesting. Uh. To design a proxy from scratch. Knowing these fundamentals, because you can optimize, you can be more performant, right? You can you can write your application in a way that is more performant and and avoid any latency that might be there. Or if you run into a problem, you kind of understand what is happening here. So if you're using engine X as a layer for app, understand that sometimes index Baidu does buffer. I don't know what are the cases. We can look it up easily in the dark red, but if it's buffer, that means there is a time delay here. And can you configure this or not? All these questions really becomes in the advance section. You really in the tuning and the advance building proxies, which is which is really very lucrative. You might see with them Kubernetes and microservices, architecture proxies is really, really big things now these days. So if you know these building fundamental and those building proxies is really huge and this is not what the course is about, this is its own course, just proxies and load balancer. That's why I didn't mention here. I'll be going to talk about it obviously when you ago was asking a question. But this is not the intent of this part. Of course, this is this is a networking course to understand these backends. But feel free again, ask these fantastic questions because we're going to keep this show going every month. You're going to get a new lecture and a brand new section talking about that. Enjoy the course. See you in the next one.


### How does the Kernel manage TCP connections vtt

Hey, guys. So this is a good question from a subud talking about, um, the sin and the accept. Q Specifically here, the question is do colonels maintain one Q each for sin and accept Q per process or the Colonel maintain contains one Q each for sin and accept. Right. So the second question is part of it is how do you maintain about these two? Is it basically fee for principle? So let's discuss this a little bit. So when we create a listening socket in a process, what happens is we specify the address. This is the IP address and this is the port. And when you do that, you get back a listening socket which is in Linux, a file basically in Linux, everything is a file file descriptor, right? And when we do that, the Linux kernel will create two queues for you. One is called the Syn Q and one is called the accept queue for that listening socket. And that, like any other files, is not bound to a process per se, right? That file and that socket can be shared with ten processes, which means ten processes can read and accept connections from that socket. So what happens is when a connection is attempted to this socket, that that is a syn request comes in with a target IP address matching the socket listener and the target port matching that 100%. Then that syn entry is added to the syn queue, which is a dedicated queue just for that listener. But the kernel immediately just replies back. Okay, acknowledge I'm good. Boom, let's just acknowledge this. And when you do that the client gets back that syn ack. Right? The acknowledgement basically you get a snack and the client gets that syn ack and replies back with what were the finishing the TCP handshake. And when the kernel receives the ack matching the the syn that it was in the syn q then you have a full fledged connection. Only then an entry is created in the accept queue with that connection. Yet up until that point we don't really have a connection yet. We have a ready to be accepted connection. Now the back end application will call a method, a system call called accept on that listener and that will pop that entry from the accept queue. Right? By that time the Syn has been popped, right? Because we have a connection. So the syn queue is now empty. Now the back end will call an accept and then go get that connection. We get a file essentially. So technically to answer a question here is any process with access, with a pointer access to that socket listener can call accept. So we have multiple processes can fight essentially to accept the connections and why you want to do this, you might say the reason is one process might not be fast enough to accept like a lot of connections, right? So that's what you do. Essentially. You accept, you spin up multiple processes sharing that single socket, all competing. The problem with this model is you get contention because because it's a queue, it has to establish a mutex, right? Like how do you pop something or push something from queue? You have to lock it, right? Otherwise, especially in a multithreaded or multiprocessor environment, you have to lock it. And when you do that, well, problems happen because now you have contention between multiple processes competing to accept connections. So that's the first part. The second part of the question is, is it simple? A is a simple really Fifo principle. I would say yes. I'm not really an expert in what is how is that actually implemented in the Linux kernel. But I would say yeah, it's a basic simple things. Unless maybe some cases like the kernel not particularly for the accept queue, but definitely for the receive queue where the kernel will say okay, we have like a lot of packets here received, what are we going to do is we're going to merge all these packets into one place essentially and combine them into one packet, just that we can shrink and remove the headers from the packet so we can merge them into fewer and fewer packets. So that is happening. But essentially I think it's going to be basic Fifo principle. Yeah. So yeah, that was a was a good question. I thought I'd get to create like a Q&amp;A section on it. See you in the next one.


## Course Summary


### Course Summary vtt

So, guys, this is the end of this course. I appreciate you. Thank you so much for reaching at the end. Was you this is not while this is the end. I'm going to inject content in the middle right as the months ago because I like to listen to you guys, hear your feedback, get ideas of things I need to add and then improve the content. I've been doing this since all my courses really, and I and I absolutely love it. So this network engineering is a fascinating topic and it's definitely one of the major pillars when it comes to backend engineers. That's why I'm passionate about it. Have you you might have seen this in the course itself, but I absolutely love it. And I hope you enjoyed the course and. Thank you so much for reaching the end, and thank you so much for getting the course. Good to see you on the next one. Stay. Awesome.


## Extras


### What is SNI (Server Name Indication TLS Extension) vtt

Hey guys, I hope you're enjoying the Fundamentals of Network Engineering course. And. It's almost like a year since I published this course, and there has been a similar questions that have been bubbling up. And one of the, you know, the the question that keeps coming up is related to SNI or general TLS extensions, right? And so I thought I'll bring some of my lectures that I did on YouTube and group them curated a little bit and add them as its own lecture in this extra section, and I think you're going to find it useful. Hope you enjoy it. What is going on guys? My name is Hussein and in this video I want to talk about SNI server name indication. And I did make a video about the SNI, but I learned so much after that. I want to kind of remake that video, but with a with a kind of different eyes and lens. How about we jump into it? So server name indication guide before we explain what it is, we got to know why does it exist. There is a there is a problem. There is a solution. And then SNI came to solve this problem. And here's the problem guys IP addresses this. Assume this is 1234. This is one public IP server and this is the IP address. And I have a website. And I purchased this public IP address. And you know how expensive those static IP addresses are because you want your IP address to be static. Right. Because you don't want to update the DNS every time. I mean, you can, but that's just painful. So you get a static IP address and you host your website there and you run port 80. Let's assume no security for now. Right? And then you start hosting your beautiful website. And let's say this is called a.com right. That's the website okay. And then you add an entry that says hey 8.com is actually this IP address. And that's the DNS right. Do on us. There is now the encrypted DNS and all that stuff. But I don't want to go there, man. The mess that goes recently. Oh my God. Right. It's another topic for another day. But yeah. So that's the DNS. So this client want to connect to this server says hey I want it to go to a.com when I get to make a git request right. Slash eight or com. First of all says, hey, where's the.com ping. Get the IP address and establish the TCP and three way handshake and all that jazz and says, hey, I want to go to 8.com. And now the server will say okay 8.com. There is a property called the host header here in the get request. Right. And that includes a.com right. And with that, the server receives this. Okay, you actually want to go to 8.com and you might say, Jose, what the heck are you talking about? Of course they want to go to 8.com because we just DNS that thing literally a second ago. Right. So why does the user need this extra header in the Http protocol. Well that's a good question. We'll come to that. So at dot com there are probably there is a folder here somewhere called a.com. And that is like there is an index dot HTML and there is the content JavaScript and all that jazz right. And that's fine and dandy. But here's the thing. Webmasters are greedy. They want to host multiple websites. Just one website is just a waste for a whole IP single static IP address, right? You want to host seven eight websites on the same because what what's the website is just a bunch of content like static content. Yeah, with JavaScript and all that stuff. But but yeah, it's just folders. It's just content. Just code. Right. So people and says, okay, here's what we're going to do. You can create another folder called b.com right. And then create a completely different website. xHTML, and then that should work, right? Because when you make that DNS right, entry 1.3.4 also points to be.com. So you're going to create a new DNS entry that points to the same IP address. So wait a second. Now when you make a Get request. You just specify the host header I want to go to actually b.com. Now when you actually want to make a Get request, you do the DNS, you get the public IP address and then you say TCP connection, all that jazz and then oh get request I want to go to b.com. So the server receives this all you want to go to b.com or b.com. There is the here's the folder. Take it. Be dotcom content. Awesome. But guys, all of that stuff is what this is unencrypted. This is all unencrypted. So when the encryption when Https came into the picture. This whole thing falls apart. Because here is the thing, right? I didn't still explain what this is, right? All of this stuff fall apart. And let's explain this with the Https. Right? It's the whole the whole thing with the steps. Awesome. Let's now do https. Right. So what's the difference? Hussein. It's the same thing. Well we'll see. Here's the thing. When I first of all I want to go to let's say a.com. Right. That's good. That's that's fine. I need to do the DNS. I get one, two three, four and I establish a TCP connection. Right. It's a two way connection. Send send ack ack. We talked about the TCP handshake. Check it out. But what is after the TCP connection if I am using https tls handshake. Right. What do we do in the TLS handshake. The first thing we do is client hello. Right. And we say yo sup I want. To establish a communication with you. I will establish encryption. So I'm going to agree on the key. And you're going to agree with the key. And you give me your certificate sir. So all right. So that's and that's the IP address and all that stuff. Right. So the Diffie-Hellman I'm going to go to TLS. Check out this video here right here if you want to learn more about the TLS handshake. But yeah if I get this handshake now the server is supposed to respond with the certificate, right. And once the certificate we talked about. What certificate? The certificate proves that you own a.com. But guess what? How the heck does the server know which certificate to serve? Because he or she and why? I'm misgendering the server, right? They write actually. Have two observers, so they have two certificate. How do I know which one to serve? You don't. So you're stuck because do you serve a dot? But how do you know that the user actually requested a dotcom? Nothing in the TLS. Hello. Actually specify the host name? Nothing. Absolutely nothing. It just there is an IP address. And that's it. There's nothing else. So how do you know what certificate to host? Meet is an AI, which is just a bunch of extension that adds this extra TLS. Hello? The name. And that's. So in the hole in the hello, we add a string called the SNI Server Name Indication and it's almost available on every single web server. Every single web server has this SNI. So it says a.com and now say oh, you want to go to a.com share because the server is from Boston. You say share. Let's take it to the top and bring it that certificate. All right. And then we're going to serve it a.com certificate. Right. And then the server will serve the.com certificate. If you want to establish with.com just specify b.com. And then we're going to send you back b.com certificate. And after that. Because after that we send the actual after we establish the TLS encryption and all that stuff. This will be encrypted. That was supposed to be a lock, by the way. That's a beautiful lock. And it says, okay, I want it to go to a.com. And and yeah, I want this is the host and I want to go to a.com. Right. And here is where we actually serve the content of a.com. Right. The problem here with the TLS is we didn't even reach to the stage where we don't know which certificate, which certificate to serve in order to actually reach the content. Right. That's why the connection is always terminated. So we needed an SNI. So we can either provide default certificates that capture both, which is very, very expensive. Those wild card certificate. But that's that's much better way. Obviously there are some problems here. So what happened guys questions comments in the comment section below. Guys what if I do this in Google 1234? HTP is just the IP address without host. Will this work? Write your comments below and then pause the video. So this will fail and I take it back. This. It depends on the configuration of the server. It will fail because first of all it's okay. We're going to do 12341234. Let's establish a connection to port 443. By the way if we're going to talk about that. But you guys know already I'm going to establish and I'm going to do TLS. Hello. And the SNI name will be 1234. And the server will receive it and says what the heck is 1234? I don't have a site called 1234. I don't have a site called 1234. I don't have a certificate called 1234. So that will fail. If you configure your web server to have a default certificate like let's say one two, three, four always serves 8.com, then you're going to get certificate 8.com. However, this will fail because a get host will also have 1234 on it instead of the 8.com. And then when you try to actually serve the content it says like what the heck is one, two, three, four? I don't have a host one, two, three, four again, if the server is configured to serve default content, you can serve this, so you might get away with it if you want to. Right? But yeah, I show all of that stuff in practical details because some of you yeah let me like Hussein you don't do practical videos anymore I do. I just like to relax sometime and and do virtual boarding. Sue me. Don't sue me, guys. All right? I love you. All right? So, yeah, I like to mix things up a little bit. All right. Obviously, guys, final point. SNI is not perfect. And the reason is because when we send this beautiful. This is a mess. I'm. I'm making a mess, guys. All right. If when we send this TLS handshake, there are colors. Sheesh. Why why am I not using colors? This is the TLS, by the way. If I'm sending the TLS handshake right, I'm sending a.com in plain text because this happened before the encryption. Right. So this is in plain text. So Karen here. That's supposed to be a skirt, but. Sure. Yeah. Karen can sniff, and that's how it is. Sniff, by the way, guys, and know where you're going because of and. They still know where you're going. All right. So they can know. They know where you're going. Right? Obviously they also know where you're going because you just requested a DNS. And DNS uses UDP right. And UDP is not encrypted. Right. Unless we go with DDoS, which is Firefox is enabling for us recently, which just destroyed firewalls enabled for a day or for a day, I think. And it just slammed down the DNS provider next DNS, I think. And it just like died because they had like a bug. They they basically lost the DNS provider to do the encryption. But yeah. Anyway guys. So yeah, there is a new extension called e SNI that encrypted SNI which essentially encrypt that stuff, but it will go with the Dodge the DNS over Https. All right guys, that's it for me today. That's a quick video. Just chatting a little bit about the SNI technology over virtual board. And let me know if you enjoy these kind of videos. I'm going to see you in the next one. You guys stay awesome. Server Name Indication, or SNI for short, is a TLS extension that allows the client to specify which host it wants to connect during the TLS handshake. This allows multiple domains multiple websites to be hosted in a single public IP address. We could not have done this right. Shared hosting before SNI, right? At least secure shared hosting before SNI, right? So in this video I want to talk about the following. I want to talk about what is a CNI. Right. It's like a little bit why did we invent this deck? Because that's what we don't invent anything in software engineering. Right. Without a reason. Right. So there is always a reason. So why is this we're going to talk about that a little bit. And we're going to start seeing timestamp. You can jump into the interesting part of the video. And the second part of the video, which will take most of the part of the video is really is we're going to set up an SNI with a proxy. So I am going to actually show you how to do an SNI. And what does that mean? I am going to set up not one, not two, but three. Secure Http two running on TLS 1.3 website on my Mac book, right on my Mac. And guess what? I'm going to. I'm not going to spend a single penny. Everything is going to be free. It's going to be public. Three public distinct websites. Right. And I'm going to give them distinct names. This website I don't know is going to call it Jenny's Donuts. And this is a marked tire. Pros. And the third one I think is Ali's iPhone repair. Right. And all of them are just a bunch of applications that are running on my website. So we're going to show you how to do that. Stay tuned. And finally, we're going to talk about a little bit of the limitation of this server name indication. Because nothing nothing is perfect and how Cloudflare and others from Apple, I think invented this new proposal called encrypted SNI. Right. We're going to talk about that if you're interested about all that jazz, stay tuned. If you're new here. Welcome. My name is Hussein. In this channel we discuss all sorts of software engineering by example. So if you want to become a better software engineer, consider subscribing and hit that bell icon so you get notified every time I upload a new video. With that said, let's just jump into the. As in I what is this and I. Okay, so here's the problem we're trying to solve. Okay. I want to host a website. Why do you need to host a website? Duh. You need a public IP address so people can communicate with you routing. Or you cannot host it on 192168.1.1. Right. Or ten .0.0.2 unfortunately, because that's a private IP address, it's not routable. Well, you can do some port forwarding, but that's out of caution right now. Yeah. So you need a public IP address. You need a content right. You need HTML pages NodeJS application Python Django right IIs anything right. Proxy nginx anything that serves web traffic. Right. The final thing you need is a DNS record that tells, that makes this ugly IP address into a beautiful address that you can send to your users, and they can go to your website. Jeni's donut.com right Mark tire pros and is iPhone services repair right I don't know Steve's brothel right. All anything right. And that domain will point to the public IP address okay. So. What I want. If I want to start multiple websites, right? What do I do? Well, you need another public IP address. Nope. That's so expensive. Nobody has that money, man. Nobody has that kind of cash. Public IP is are so expensive. So people turned into this thing called shared hosting, where they want to host multiple websites into a single public IP address. Yes, sir. It is possible. So we did hacking. We did some hacking stuff. How do we do it? Well, that's how we do it. We want to host multiple websites in a single public IP address. Well, we need some trickery. People of the internet in Http 1.1. They invented this hackery and it's called the host header. It's not really a hack, but it's very critical. This solves the problem of proxying because you couldn't do proxying without the host header back. In Http 1.0 and Http 1.1, they introduced the host and they killed two birds with one stone. I'm sorry Peter. We fed two birds with one stone. Okay. Geez. Okay. Host okay, so we introduced the host header. And now if you specify during your get request you have to specify which host do you really mean. Right. You might say well I am actually I just connected to it. But no that's not enough. You have to tell me with every request. What do you mean? It's a stateless request. Http is stateless. So once you tell me that. Well I have a key to key off with. Right? I'm going to key. Hey, if you really. Yes, you're you have established a designated TCP connection with my public IP address one 144 .1.1.1 yeah, but if you send me a Get request saying, hey, I want I want Jeni's donuts, right. Host I'm going to look at the host. Oh you want Jeni's donut. That's that folder in user var okay. And if you say I want Steve's brothel oh whoa. Let's go to that website then. Okay. That folder or maybe it's, it's another port running on another application on port 8080. Oh. Let's go there. That's where we put the brothel there. Okay. And then if you want to go to I don't know Mark Tyler Pro. So that's folder. And then we can key off the stuff that seems to be solved. The problem. Why do we need a CNI then. Well host sir is part of Http which is the data portion which is not secure. Right. So if you used Https then and then you establish the communication. We don't have that information right during the TLS. So we need a problem. We need to solve this problem. We need to know this host earlier okay. We need a host header during TLS. Right. And we talked about I'm going to reference the video here okay. So if I'm doing TLS with you I'm doing a client. Hello. Give me a way to tell you the host. Right. And you serve me what certificate matches your host. Because that's part of the Https, right. Because you know cert https, you come with a certificate that proves that you are a marks tire pros or Jamie's donut. Right. You need to obtain this certificate from a certificate authority. So how do you know during TLS negotiation what host to use. Right. That's part of another problem to solve. Right. So we need to know that does meet SNI sing server name indication okay. So server name indication during the TLS I'm going to go through an example here. Right. And it's supported in major browsers IIs supports it. Nginx supports it HAProxy supports it. Other proxy supported I don't have a whole list, but it's a thing now right. It's already have this limitations and we're going to talk about it at the end of the video. But yeah. So jumping to TLS 1.3, if you're not on TLS 1.3 please be in TLS 1.3. Guys don't be on 1.2 okay. Yeah. It's much, much more powerful, much more secure single round trip instead of two. All that jazz, right. And here's the thing. We have Mark Taylor prose here. And there's the IP and there's the index. Right. The HTML. And this guy sends a client. Hello. That's how you always use work without SNI. Okay. This in the client. Hello says hey, I support this at Key Exchange. I support these symmetrical key algorithm ciphers. Please tell me what to do okay. And it's going to do a Diffie-Hellman in the middle as well. It's going to generate the private and public key, merge them and send it to the other side. I'm not going to talk about that in this video, but the public the server here receives it and it will send a server hello with a certificate immediately because it only have one certificate in this case, right? Because it only hosts one website. Right. So it will send the certificate of Mark to browse. Okay. Along with maybe an ocsp stapling and other jazzy stuff. Right. And then obviously once they establish the symmetric key, they agree on it. This guy encrypts the Get request and send it and then yeah, this receives it. And that's how TLS 1.3 works. Right. That's how we serve the certificate. The problem with that what if I have three websites I have Jenny's farm, strawberry farm, I have Alice I. On come. I have marked pros and all share the same public IP. So it's a single TCP connection, right? But it serves a different domain I need more. Discriminator, some other stuff to discriminate this thing. So and I meet S and I make a client. Hello. And in the client. Hello. By the way I am going to ally-iphone.com. I don't want Jenny's. I want an iphone.com I. You have to explicitly tell me now that you told me that. Well I can okay I I'm going to look at that and I'm going to show you how to do it, initiate proxy and look at that. And then it says oh Ali iPhone I'm going to serve you the certificate of Ali iphone.com okay. And then obviously the negotiation of Diffie-Hellman have done they they agreed on a symmetric key and they encrypted it perfect for security and all that jazz. They decrypt the bat, send the headers that come all the beautiful stuff. Right. Cool stuff guys. Right. Let's go to the demo, guys. All right. So here's what we're going to do. We're going to host not two not one not three. We're going to host three secure Http two websites on my Mac right. With TLS 1.3 and all of that with three distinct domains and all of that for free. We're going to use no ip.com because I'm cheap like that okay. I'm not going to pay a provider to do all that stuff when I can do it here. Okay. Just to show you that you can actually do it. You don't need fancy stuff, right? Eventually I'm going to make a cloud video, but I'm just like that, right? I want to do different things. So what I'm going to do, I'm going to open a port 80, port 443 on my router that maps to my Mac address so that an external traffic goes to my internal Mac address. I'm going to close those ports after the video. So this is just to show you. And here's what we're going to do. We're going to do a proxy because that's what I know I know I'm going to make another video in the future. I'm going to reference the full course H.R. Proxy that I did. It's free. Yeah. Go check it out. It's a proxy from nuts to bolts. Is that how you say nuts to bolt? Yeah, all that stuff, man. Go enjoy it. Right. I'm going to show I alpn right. Application layer protocol negotiation to do H2 because H2 is cool ish. Right. Then we're going to do TLS 1.3 as well. Right. Let's just jump into the demo. All right guys I have here on my Mac three web servers that I spin up with three distinct websites. Right. The first website is called Ali's website. This guy's repairs iPhones and the application is running on port 8080. And the second website is running on port 8081 and it's Jeni's donuts website. And then finally there is Mark website, right? Yeah. Mark repairs tires and he's an expert garage, right. He has a garage who repairs cars, all right. And tires and all that stuff. And his website is running on port 8082. All of them are running on my Mac. Right. And let's just test this thing out. So that's the first website is running on port 8080. Okay. Just an image. That's the second website 8081. That's the third website 8082. Cool. All right. So what I want to do next is I'm going to spin up an H.R. Proxy. Right. I already installed the proxy. I'm not going to go through the process again. I'm going to reference the video for the H.R proxy course, just to go through it and to do the details of it. Okay. So let's go ahead and spin up a some configuration. Right. So we're going to do here is I'm going to create a configuration for H.R proxy. Right I'm just going to call it I don't know a website dot cfg okay. And the proxy proxy requires a front end where things connect to right. I'm going to call it F anything. Right. And then I'm going to bind all my interfaces right on port 80 for a start. Right. Because for now let's just do a port 80 okay. And here's what I want to do I want to do right. So I want to use the mode Http because I want this to be a layer seven proxy okay. All right. Now that we're playing on this layer seven proxy layer seven layer, what we want to do is essentially add more information here. Let's add some timeouts right. The timeout is a client timeout. I want the client to essentially give up right. If the client is stopped send me an information after 10s I want to close this connection. And I'm going to create a back end here called Ali. Right. And the back end Ali is basically going to port to the Ali server, right. Which is 127.0.0.1 on port 8080. Right. That's 880 conveniently. Does it tell me? Yeah, this is 1880. All right. And let's use some timeouts. Timeout server DNS I explained all that stuff in the other video. But essentially when do you want how long do you want me to if I want to connect to the server what is the back end timeout servers. Let's just use the default thing. 10s is just good enough. And then Jenny, another back end. What's the server for? Jenny? Jenny server server 127.0.0.1 8081 I think. Yeah. Then let's just use the timeout. Timeout. Right now we have all the timeouts. We need to also make sure that all the modes are also Http, right. So we're all playing on the same layer, the layer seven. Essentially that's very critical if we want to start reading the host headers and other stuff like that okay. So let's go ahead and try that. Looks like it's good. So let's just do that. All right. So now I'm listening to port 80. Let's test this thing. Yeah. So if I go to localhost. That points me to Ali's website. Always, right. Okay. If I go to Hussein Mac. That also takes me. That goes, that's my host name, right? If I go to my public IP address, what's my public IP address? That's my public IP address. If I go to that right. It also takes me to that because my public IP address, I have a rule on my router that says, hey, anything from the public internet on port 80 on your router forwarded to my machine Hussain Mac on port 80 and anything on 443 also do it the same thing. Okay, we have all the pieces. The next piece is let's create three domain names. That all goes to each one of these guys. Okay. How do we create domain names. Right. Very simple. Go to no ip.com and register for free I already registered right. So if I if you go there I'm going to create now three host names. Right. So let's go ahead and create the host names. I'm going to create a host name here. And I'm going to use void default I don't know something like a DNS king. Let's do DNS king. Sure. And I'm going to call this Ali Dash website okay. And that points this. Ali Dash website dot com. It's a free obviously the DNS because you don't get to choose a beautiful name, right. That points to this public IP address and you're going to create that host name okay. That's Ali website. The second website I'm going to do Jenny Dash website just to remember. And it's also DNS king. So we don't remember. We don't forget and we create an A record. And then finally Mark Dash website and then finally the DNS name. And I think we're good. Right. So now we have three website. And that's what gives you it gives you three four by default. You don't have to pay anything okay. So now after a while right. If I take this or this or this, all of them will point to what to my public IP address. Right. Which is this guy 47.111, which will always take me to what? To Ali's website. Right. But I do not want that. I want to do this if statement right, I want hey, if the host which by default all browsers sends right on the Get request because we're Http 1.1 and above always sends the host header, we know which host do you really mean. We can key off that. And we're playing on the proxy Http layer which is layer seven. So we can do that right. So now let's let's try that. Maybe it's it's going to take time to update. But it looks like it's already updated. So Ali's website and Jenny's website all of them will essentially get updated. Okay. Let's go back to the configuration here and do one last thing okay. If we're going to do, we're going to we're going to remove this. We're going to only use the back end. Ali. We're going to add an if statement here. If the header. Host is equal to. Jenny Dash website does dot DNS king? Is that what we called it? Dot com. So yeah. Yes DNS king.com. Then I want you to use that right. If use the back end. Oh, wait a second. This is Ali. So, Jenny, I want to use Jenny if the Jenny's website. And I want you to use Ali back end if. The header of host is equal to alley dash website dot DNS king.com. I think she means case sensitive. I might be wrong there. Okay. Use back end mark. Right. If. Header. Host. Dash I. And Mark website. Dot DNS. Com. Up until here. We don't we didn't we didn't use any and I this is just pre right. All right. Let's clean things up a little bit I think that should do it. All right. So now. All the config looks good. Let's take a look. So if I go to. So if I go to Jenny's website DNS king.com, you can see that now this guy serves me this. How about we go to Mark. If I go to Mark website dot DNS king.com you can see that Mark website is served. And finally. Uh, if I go to. What is the final? Was it Ali? Ali website? You can see that Ali's website is served. All right guys. So now we have pre. But this is not secure. How do we do security. We need certificate for each of these guys. So we're going to use Letsencrypt to get a certificate. Certificate for each of these puppies. Let's do this. Okay. I'm going to spin up a new one. Right. The command for to get a certificate. So let's make sure you have Letsencrypt installed. Right. And again I went through that process. I'm not going to go through it again, but I'm using Letsencrypt certificate authority to get a certificate for my three domains. Let's do this. So sudo cert bot cert only because that's what I'm interested in. And dash dash standalone okay. That's how I do it. When I do that you're going to be prompted and we have to do it sudo because they put the private keys and public keys in a secure place. So you need to be root for that. And here's the thing. What is your domain. What should we do first. What domain. Let's do Ali first Ali dash website dot DNS king.com. Right. Is that the website I think so obtaining a new website. Oh we forgot something very important guys here's the thing. We before you let's do this again before since to do an to do a certificate. What let's the sudo thing what the what the cert bot does is actually listening on port 80in order to communicate with the cert bot with the with a certificate. But since we are already listening to this, we have to kill the proxy, right? Because it's listening to it's listening to the port 80. So let's do it again. Right. And it's called a Ali Dash website. The DNS king.com. And we do that obtaining your certificate and cleaning up challenges. All that jazz and done guys. Done. That's it. Here's my private key and here's the public key. And there is magic that we have to do right. That we need to merge the public key and the private key in one key and one key. And we put that in in a proxy. And we're going to merge this guy using a command called Cat I think as I was called. Yeah I think it's called cat right. I'm going to cat this guy. With this guy. Right. And I'm going to t that. I think I have to do sudo as well. T is basically writing. I want to write this to users as I what's called users. Hussein Nasser and I the same folder and yeah let's call it Ali Pim right. Because that's my key now that's the certificate that the proxy will need to have in order to listen in order to serve through the server. Hello. Let's do that. And I'm done. We have one now. Okay. Let's do the rest of the two as well. Right. Very similar thing. Okay, so we just bought only let's do Jenny Dash website dot DNS king.com. Right Jenny this time Jenny let's do Jenny. Thank you Jenny. Now we do pseudo cat. We take this puppy. Paste. And then with this puppy. Paste. And then I want a t that sudo t that write it into users say author s and I and call it genie dot pen. Done. How about the third one? Let's do a mark. Zuckerberg. Let's do it. Mark dot website dot DNS King. That's a shady website I don't think anyone would click on. Oh my God, that's a shady website. All right. Done. Okay. The shady side is ready. Okay. Let's do. What is it, cat? I don't know Linux much guys. By the way, just FYI, I barely know the basics of things here. Okay, then you do sudo t. I've used windows all my life. Pretty much recently. The past three years. I started working with Linux, so. So I'm not. I know the Linux gurus are yelling at me, and there's pretty much 100 better ways of doing this, I know. Okay. All right. So who's this guy? Mark. Mark, pen. All right. We have all the keys, guys. So what we need to do next if I can type vim. Website dot config. And here's what I'm going to do. We're going to enable bind to 000 zero watt 443 guys we're binding on 443 right. Secure TLS. Hello. In order to do SNI with 443 it's very simple. Literally after 443 because that's what you're listening to. That's what tells me that you're doing TLS right. You do SSL right. Because I don't know why it's called TLS because it's just legacy stuff. Right. And then you start listing your certificate one by one after the other, right? You do SSL, SR, CRT, SRT. That's the Dodge Challenger SRT. All right. Hellcat. All right. That's users Hussein Nasr Xin'ai. And let's start with Ali okay. Dot p m and then space. And then you do the other SRT which is what. A user's. Hussein Nasser as Jenny. Jenny. Let's serve Jenny. And then foreign one CRT. What do we do as users? Hussein Nasser. It's an. And then finally marked Penn. And then finally we want to enable Http two as well right through which is an application layer protocol negotiation. Right. So in order to do that you literally just do a LPN space H2. And I want to also Http 1.1. Is that how you do it. No I think it's just slash dot 101. Right. That's it. You save and we try. Hopefully we get it from the server time proxy F website. All good. Looks like it's working. And we explained what what this error means. Actually it's just the the length of the bit length of the Diffie-Hellman parameters telling you that it's too low. Right. But our websites are shady anyway. Who cares? Nobody's getting hacked them. All right, let's test this driver, guys. All right, let's do this. Ali, go! Oh my God, we are secure. Babes, we are secure. Ali website is legit. And not only legit, it's also TLS 1.3 babes. Because that's. That's how we roll, babes. Look at that. Look at that. That's actually TLS 1.3. How about that? It is public. Guys. This website is public. Ali. Website about Jenny. Jenny. That's not. My name is Jenny from the block. Look at that. Jenny from the block is selling donuts. Babes. Look at that. Jenny, go! Jenny. About Mark. Mark brothel. Hey, Mark. Website is up for grabs. Look at that, guys. All TLS 1.3. All right, guys, that was the an. Let's go back to the slides and complete this video. All right guys. So we have done the demo guys. Isn't that cool right. Wasn't that cool ish right. So is this thing perfect? No guys, that's the problem. It's an AI does. Here's the problem with this. An AI. It sends the hostname in clear text. Why is it clear? Hussein, you said it's part of the TLS. Yes, it's part of client. Hello. Hello? Hello is not encrypted yet. Because the client hello is the first attempt to request encryption. All right, so we don't have encryption there. Right. So any extension there is probably not encrypted. That includes the Ocsp online certificate status protocol request for stapling. You know, to to make sure the certificate is valid or not. All of that is not encrypted. So. If you're paranoid of privacy, right? Governments, your government still can see which domains you go to, right? So all these nasty domains that you go to, they see them. It's clear DNS is unencrypted. Well, until someone figured out or not. Right. But until now DNS is not encrypted okay. If you use Firefox by the way, you can really you can really be in a tank, dude. Firefox is badass, right? You can just do aboutconfig. Oh my God. You can enable Esni. You can enable Doge. You can enable so much cool stuff with Firefox. It's a beast of a browser. I don't know if you can do those chrome, Chrome just kind of give you out of the box. I might be wrong though. Right, yeah. So what's the solution for SNI? There is no privacy. Right? We talked about that. No privacy at all. Right. Because they can see the domains. They cannot see what you're searching for. Right. If you're Google, they cannot see what you're searching for. But they they know you're going to Google, right. Maybe. And they can block you if they want to. Right. With with with this and I, they can look at the client. Hello. And look at that. It says hey you're doing that right. SNP is the new proposal to encrypt SNP. Right? The client. Hello. Union client. Hello. We want to encrypt that portion that says hey server. Hello sub I want this host by the way. It's encrypted right. And the way Cloudflare really figured this out with Apple and other people. I'm sorry I don't remember their names right now. I'm going to add them post video. Right. But the way they figured it out, the encrypted SNP is actually because think about it, you know what do you encrypt this with? You don't have anything right. What do you encrypt this with. You cannot encrypt it with anything. You just started about to start the session for God's sake. You don't have nothing. Okay, but here's what they say. They say, okay, what do people do before TLS? So like, well before TLS, I established the TCP connection. Well, what do you do before TCP connection? Well, I need the IP address. Right. And I need the port. I have the port. It's port 443. But what do you do. You need the IP address. How do you get the IP address. Right. Well the user doesn't give it to you. You give you give you the domain. So you do a query on something this thing called DNS domain name server. And you ask for the IP address and you get the IP address. And from the IP address you connect this TCP TCP connection. And then after you establish a TCP connection you do all that jazz right. Pretty cool stuff right. So they said Cloudflare said wait a second. We we are a DNS provider. What if we provided a record in the DNS with a public key encryption matching the domain right, and the user and the owner of the domain just puts that up, right. So the public key will have a matching private key. So when I want to go to Steve's brothel right I will make a query to the DNS. And then Steve's brothel will have a public IP address. And I can get the public IP of Steve's website. Right. And then I get that, and then I establish the TCP connection three sink and all that jazz. Right? Three way handshake. Got the TCP connection about the client. Hello. I got the public IP and public key of that domain. I will encrypt that SNI and I will send it across the client. Hello. So the server now knows obviously because now I'm using a new extension called Esni. Right. The server will receive okay this is an SNI. Oh this guy is smart. They they actually obtained it from a DNS record okay okay cool I think I can deal with it. I have the matching private key. It's an asymmetric encryption obviously. And we talked about encryption. Guys I'm going to reference it here. And then we're going to take the private key and lock it. Take that key. Oh this guy wants to go to Steve's brothel okay. All right. Once we got that and then Steve's a what happened to Steve? Yeah. Nothing. So we get that, and then. And then obviously everything happens. We surf back the certificate for Steve's brothel and all that jazz, right? Cool stuff, guys. Cool ish. But you got to think about that. Guys, this has to happen. The DNS request is a UDP connectionless thing to the server right. So it's unencrypted. So. The whole thing is moot if you don't encrypt DNS. And I'm going to make another video about Dodge and Dot, which is DNS over Https and DNS over TLS. And there's a lot of drama between people fighting over which people which thing to use. I'm leaning toward DNS over Https because we don't want to use custom port for things, and I don't care about you measuring DNS request. Who cares? It's a request, right? Nobody needs to monitor any traffic. But the on that I'm on that boat, obviously. Right. But yes, you have to encrypt the DNS and so does a lot of stuff to encrypt guys. Right. So all right summary. What did we discuss guys. Well we discussed what is SNI. This is server name indication during TLS. You tell me which host do you really want right. I'm going to give you the certificate of the host. And I'm going to do smart thing behind the scenes to to actually serve you content for that host. Right. So there's like three folders where the website are, I'm going to serve content from the right website folder. Right. So we showed how to set up a CNI with a proxy that was long. Right. We did that finally. And then finally we talked about the limitation of SNI and the invention. Beautiful invention of S. And I hope you enjoyed this video. Guys, I want to see you in the next one. You guys stay awesome.


### Replacing TCP  for Data Centers (Part 1) vtt

Hey, guys, as we reach the end of the course, I thought that I'm going to include a new lecture detailing a brand new protocol that is not really in production, but this protocol really takes the limitation of the TCP protocol and looks at it and tries to improve it by really changing fundamental things in the congestion control algorithms and the flow control of things. So this this video is already on my YouTube channel. I started about a little bit. I did some editing, but I thought I'd include it here as a supplementary. Uh. If you reach this level, in this, in this course, then it's working course. You will be able to understand almost everything that I'm discussing here. So this is like at the end. So it's more of like an advanced concept and just pay attention to as we go into each of these in the bowels of this protocol, it's going to be really interesting to to learn and see how people think, understand first the problems and then as a result, uh, find solutions that fit this. All right. So, uh, let's go ahead and jump into it. I stumble upon a new paper, uh, written by Professor John Ousterhout, uh, from Stanford University. Uh, the paper summarizes a new protocol that replaces TCP, which is one of the most abundant if if not the abundant protocol that is used everywhere, you know, and it's not directly used. It's probably used by your application right now, even if you don't know it. Right. Like Http uses, uh, tcp, I know Http three does switch to Quic, which uses UDP, but a lot of applications SSH, you know, SMTp, all of these protocols use directly uses TCP as a protocol. You know, uh, and it's been very popular for years. So why change? Apparently when I, when I read the title of this, um, paper, you know, I was skeptical. You know, we need a replacement for TCP in the data center. Emphasis in the data center here because, uh, don't don't get defensive like I did when I first read this. Ah, another someone who wanted to change things the way we do things, you know, and it's working. And back end engineering. Uh, I try to be objective and look at the actual paper and see the original problems that TCP has because we agree, right? TCP is not good for everything. We know that. That's why we have Quic. That's why we use UDP directly, especially for gaming, for video streaming, things like that. Because we know that TCP is great, but also it has limitations because it was designed, you know, it was ossified is the right word, you know, and it's very hard to change. But people worked around it. So in this episode of the BAC Engineering show, it's going to be a little bit different. It's going to be a little bit longer. So get a drink, uh, relax. And uh, let's enjoy this show. I think I think it's going to be a good, uh, discussion. Really. I think it's a good paper. Or do I think this new protocol that is called Homa, by the way? Uh, uh, a very common Persian name, like, that's how I know it, because my mom is, you know, my mom's side is Persian, so that's a very common used name. And I think Homa is, uh, is, uh, some sort of a mythical bird that always flies and never lands. I don't know the history, but it's. I don't know if it's but it's inspired by this, you know, myth, mythology or not. But regardless, I think this is going to be interesting. Uh, let's just keep an open mind. Read it through. Obviously, I have my criticism. As usual. I'll try to keep an open mind. Read it through. Let's have fun. Let's jump into it. Welcome to the backend engineering show with your host, Hussein Nasser. Today we're gonna read this, uh, summary paper. That's not the actual the actual paper for this new protocol that attempts to replace TCP in the data centers. Again, very important to emphasize that in the data centers, things in the data center, everything is tightly tucked in together. You know, the latency is almost latency when it comes to networking is, is is in a in a in microseconds, you know, hundreds and microseconds or even even less than that. Right. In switches like nanoseconds, hundreds of nanoseconds. Like so they they invest a lot in this, uh, making these equipments as fast as possible, unlike the internet, which is like Wild West. And obviously there is latency, there is limitation when it comes to the MTA's use during the maximum transmission unit. We don't have these limits in the data center. We we are so fast and TCP is apparently slowing development, uh, of applications on the data center. Let's find out. I never worked in the data center. I don't know the challenges of data center. That's why I'm taking the word of, uh, Professor John here. And, uh, the references he references. Let's get started. We need a replacement for TCP in the data center. Let's read the abstract. And by the way, I download the PDF and I started highlighting the important parts that I believe it's important because it can go through the whole paper. Obviously it's not that long. It's just six pages, you know. But I only don't highlighted the interesting parts that I want to discuss here. Right. Abstract. In spite of its long and successful history, TCP is a poor transport protocol for modern data centers. Every significant element of TCP, from its stream orientation to its requirement of in-order packet delivery is wrong again for the data center. Professor John emphasizes on this statement. It's just for the data center. If you're using it on the internet, don't change it. So think about stream orientation. So when we talk about TCP now the idea of TCP TCP sits on top of IP and IP has packets. And if you want to send data through the TCP you ship them into segments. Right. No mention of segments at all here, which is something I have to criticize. You know, it's very hard not to talk about TCP and not mention very critical concept of TCP, which is the segment, not to mention not in the original, uh, paper, not in this one. Z nothing. You know. So. TCP has this idea of segments, and segments will have the TCP header, which includes the ports information about congestion control, other information as well. Can't remember right now sequences, you know, window sizes, stuff like that. You know, and this is where the headers actually all the all the information these segments becomes carries your data. So if you send a bunch of data, let's say I want to send I don't know, I want to send a whole word document, you know, that example. But that will be. Try to send into this TCP socket. You know, when you create a connection through TCP and you just stream that whole word document into the socket, your application doesn't really know about this concept of segments. The kernel takes care of breaking things down into segments, right? Sometimes I guess you can have access to those, but you just get a bunch. This will eventually be broken into multiple segments and will be shipped. Each segment will be sequenced with a number. Okay. Segment number one. Right. The first part of the document, second part of the document. So how many segments? It really depends on the MTU that down down link layer. Obviously the how much your Wi-Fi connection can handle, how much your LAN can handle, you know, and that is your basically the maximum transmission unit, your your neck, your network card effectively. And that carries on to the maximum the PDU, which is the, the packet uh, size and the IP packet, which of obviously translates to the maximum segment size. Right. So you can send up to maximum segment size worth of content. But all of this is called a stream. It's just a stream of data. It's just in sequence packet. So if you think about it there is no concept of, you know, request in TCP. You know, you send uh, the word document. How do you know if it actually arrives? Right. The application has to decide that, you know, because it doesn't know the transport transfer protocol doesn't know about this discrete, you know, boundaries of your. Messages, as the paper calls it, or request even the Http request when you send an Http request through TCP. There is no concept of a request at the TCP layer. You know, you send a bunch of bytes, the bytes goes right and the the Http request becomes get slash Http 114. We put the headers, all of this becomes a byte string. And then that byte string is shoved into the TCP layer. The kernel might use one if you're lucky. Like I think the 1500 is the maximum segment size in the internet by default in the data center is way larger than that, obviously, right. Because they have control of all this equipment, they can increase that. That basically controls the frame at the data link layer. So we're still in the abstract and I still didn't get continue I have to I have to shut up and read. So from its stream, uh, orientation to its requirement of in-order packet delivery, it has to be an order because you're sending a stream a better arrival order. Otherwise the word document or the request, the actual content of the request will be arrive out of order. Right. So that's why when you send multiple requests on the same TCP connection, you have no idea at the server side where the request, the first request starts, where the second request end, we don't have that knowledge right at the transport layer. The application has to start receiving everything. And then it says, oh, okay, this is the request number one, because it ends in a new line or whatever the Http standard says. And then the second request starts right there. So that's what we have here. We have an order packet delivery. It is time to recognize that TCP problems are two fundamentals and fundamental and interrelated to be fixed. The only way to harness the full performance potential of modern network is to introduce a new transfer protocol into the data center. Homa demonstrate that it is possible to create a transfer protocol that avoids all of these problems. Interesting. Although Homa is not a API, I keep saying homa as in because that's in Persian. That's how we pronounce it. It probably pronounce it different thing. Homa somewhat. Might be something else, but I'm going to say homa. Although Homa is not API compatible with TCP, it should be possible to bring into this is this is really big, right? If your application is running on TCP on top of TCP directly, then you cannot use this. You have to rewrite your whole application to use Homa. Right. Because it's a different API, right? There is no right or read. The whole thing is different. You know, it's not as simple as it is. Maybe they probably when they say it, it's not API compatible. That means it is literally it's not because the whole thing changes. We're going to find out the paper doesn't detail this and this is really odd. It doesn't even mention the header. How it looks like nothing really. I try to as much as possible pull information. There is no concept of ports as far as I've seen. There's no concept of ports at all. They just say, hey, host connects to a host. Why do we need ports? They just completely removed that concept altogether. So they saved on the header sizes of the, you know, the messages that they send I got out of bed. It's clever. They are reinventing the whole wheel here. Right. So it's scary for us. It's like something new. We get scared of course. But yeah, it's, uh, courageous. Might I say, although humor is not an API component. Okay, we read this. Okay, let's read it. This is the introduction. I'm going to read just this part. I'm going to discuss. However, data center computing creates unprecedented challenges for TCP. So they focus on what's what's so unique about data centers here. Right. The data center environment with millions of cores in close proximity, focus on close proximity, you guys. Close proximity. They are so tucked in together. This is not a solution for the internet. Don't bring this to the internet. I don't think it will fit in the internet to be honest. Right. With the way I I've read the design. There is a lot of chattiness going on, especially with the unscheduled packet in the schedule packet, and the receiver has to say go ahead and now you can send it or we can wait. Oh, go, go ahead and send it. There is a lot of chattiness. I don't know if it would work in the internet at all with millions of course. Proximity, individual application, harnessing thousands of machines to interact on microsecond timescale. So this is the latency is so tiny between the machines. So we want we are burdened by the TCP protocol could not be envisioned by the designers of TCP. Yeah TCP the TCP is designed 40 years ago. Did they didn't know that. They didn't know that it would get to reach this scale. So I mean, what they what they built is amazing that it actually survived 40 years. So I'm going to jump in here and read the requirements. Okay. Obviously this is invisible and I can disappear. That's all right. If I'm covering the page requirements. What are we trying to solve here? Before discussing the problem of TCP, let's review the challenges that must be addressed by any transport protocol for data centers. For data centers, again reliable delivery. The protocol must deliver data reliably from one host to another in spite of transient failures. So they're not taking that away. There is a big difference between in-order delivery versus reliable delivery. So the retransmission we're talking about retransmission here. If something failed, we have to know that it failed. It was dropped. And we need to send it again reliably. We have to deliver that no matter what the consequences were. So here the throughput is the concept of throughput is like how many units can be delivered in X amount of time. So let's say I can I can process thousand packets, IP packets. That is in a given second, right. Versus someone else can process 10,000 packets in a second. So my that their throughput is better than mine. I have more they have more throughput than I do, right? Why? Because I take more time processing each packet compared to their right. So you want to increase throughput. You need to reduce time spent in each packet, thus increasing the throughput. But that's usually called the data throughput as they call it here. Right? What they care about is actually a higher level of throughput, which is the application level message or request. So what does it mean? Even if you say thousand packet, how many requests are there. Those right. If you're sending uh, it doesn't really mean a thousand packet doesn't mean a thousand requests. Not at all. It could be 30, right? It depends on the sizes of the request. Right. So throughput here is is completely different because each request or each message I keep saying request. But you can translate it to a message. They use the word message extensively here. It's very abstract. But uh, think of the messages I request when it's going from the sender to the server, right. And as a result. How many requests can I send in a in a second? How many requests can you process? That is a very important metric in proxies such as nginx, envoy, HAProxy. How many requests can you pull and request and process. And that obviously depends on the protocol at the layer seven. Right. Is it Http is a Http two, is it gRPC? How many requests can you process. And that is another concept that is requirement here. The ability to send large number of small messages quickly. So that's one of their requirements. They really when you notice the the the the the theme that you're going to notice in this paper is they focus on messages and they're short messages to specific I think they define what what do they mean by short messages. And like within the kilobytes. Right. Uh, which is I believe it's very common, especially in microservices. You're probably going to make a small request and a large response, or maybe a large request and a small response. Usually requests are smaller than responses, usually. Right. I don't I don't I can't think of an example where a request. Yeah, I guess if you send like a your writing something, hey, I'm posting a tweet. Right. So the request in this case. So I take it back. The request is large. The response is tiny right. But short messages are are often in this case the response is short. So they want to prioritize short messages. TCP really works really bad when it comes to short messages. Just because. What does it mean? A short message, right? A short message is I don't know, let's say 200 or 300 bytes. You can fit that in a segment, right? And you can say, you might say, well, short messages to fit in a single segment at the client side. Uh, some clients can delay sending that segment and wait until it's actually fills up to fill up a maximum segment size, the whole segment, you know, because it's wasteful. You're going to see this theme with TCP. It's don't never send a single byte in a message in a segment. Always wait to fill it up. Right. That's why most implementations today disable this behavior of waiting to fill a message segment, which is called Nigel algorithm a curl. Back in 2016, they just disabled that altogether. No, it's like I have 300 bytes. Why do you have me to wait? And again it's not just always waiting. It's just if there is acknowledgement. Right. So that's how I guess part of the things that we're kind of frankensteining on top of TCP, that makes sense. Right. So so I get that. Let's continue congestion control. So what is congestion control. So obviously they don't explain any of these concepts, right? They assume, you know, and that's my job here to kind of explain and illustrate and demystify anything. That is not clear, because when I read this, I obviously some of the stuff I didn't understand, some of the stuff I did and spent some time to kind of I might get some of this wrong, obviously, but. Oh, well. Well, that's part of it, right. Congestion control. So congestion control is refers to you know TCP has two. Could control, if you will. There is a flow control at the receiver side where it says, okay, the host that you're sending to how much they can handle, how much buffer they have to receive, data, the host itself, how much bandwidth those guys actually have, you know, at that application site. And that's called flow control, receiver flow control. And there is another level of control which is congestion control, which basically usually describes the middle network. Now things like in the middle, how much how much can you handle in the middle. Right. The routers in the middle and the switches in the middle will buffer packets. And if those buffers are full for any router of any host in the middle before we reach the end, then those packets will be dropped and that dropping of a packet will signal to the sender that say, hey, there is a congestion. Let me slow down. And the author of Homo hate this. They hate the fact that the sender is actually slowing down rating. You know, the transmission rate, right? And the congestion control algorithm goes like the first. There's something called a slow start. By the way, I talk about all of this stuff in my networking course. Check it, check it, check it out. Networks, intercom, all this fundamental stuff I try to explain, you know, so it's all there in detail, but I'll explain it here. So congestion control starts with the CB starts with a slow start. So it will aggressively send send send send send send and then start uh the the another algorithm which is basically the congestion control algorithm, the normal one where it will create a segment by segment right until and congestion was detected. And what does that mean. And this is where the disagreement here, the authors of the Homer paper here, they disagree. Where what does it mean to be congested? You know, the congestion in TCP says if there is a dropped packet or sometimes there is something there is explicit congestion notification set in the IP header that router set and layer three switches to be specific. Also there was a I detected a conjecture or I'm about to be congested. Yeah. Could you slow down so that once the congestion is detected by. If a buffer is filled, packets are dropped? Timing out as a result in the client said, hey segment, I didn't receive an entry for this segment. There must be a congestion. I'm going to have my window, the congestion window, the CW end, right? And then it will go on right and then slows down. And that's just really bad apparently for data centers. Right. It's it's interesting how they are trying to kind of solve this problem. You know, they are instead of. Introduce because let's think about it. Who is introducing the congestion? The sender. Right. Because it's sending data like there is no tomorrow, right? It's like sending, sending, sending. And then eventually it will reach a state where. No, the routers in the middle there was like a weakest link that cannot handle this because the the packets will get backed up and the buffers will fail. And when the buffers fail, then new packets will. No, they will no have will have no room to sit in these routers. And as a result they will basically drop the packet will slow down the code. So they are they're flipping this algorithm. So I guess it's a spoiler. Uh, spoiler alert I'm going to explain what Homa does here. What they did is actually flip it. They make the receiver decide tell the sender, hey it's okay to send now right. So what happened here is the the in homa. The messages will be sent regardless a blindly sent that's what they call it. They just just send anything you have but only things. That's called unscheduled packets are scheduled messages. So they're going to send only few parts of the of each of the message that you're going to send. They're going to send the parts the the beginning of the message which includes the message length. And that gives kind of information metadata to the receiver says, oh, uh, this message is short. Go ahead and send that. This message is long. Let's wait a little bit on this. I'm going to grant you sending these messages, but keep the rest of them. So the receiver is just orchestrating the congestion, not really congestion. The receiving the receiver orchestrating the sending. Right. So you're going to see a lot of chattiness if you think about it. Right. Just this this thing didn't really exist. I can you can argue that as acknowledgement. Right. Coming back and forth. So they're replacing that with this concept of grants. I know, I know, I know, I have to continue reading. In order to provide low latency, the Transport Messaging Transport protocol must limit the build up of packet in the network. Queues. Packet queues can occur both at the edge the links connecting hosts to the top of racks. You're going to see this mentioned a lot in the paper. Top of racks uh, Tor I think stands not the onion router. Not to be confused with the Onion router because I've seen tools like what does Tor has to do with this top of rack switches. And in the network core, each of these forms of congestion creates distinct problems, obviously. All right. This is, uh, some some of this part is, like, actually my favorite here. And again, we are we are here in the requirements section. Right. What is the requirement for a new protocol in the data center. So we talked about congestion control. We want to try to avoid it as much as possible. Efficient load balancing across server cores. For more than a decade, network speeds have been increasing rapidly while the processor clock rates have remained essentially static. So you see, I'm not a hardware engineer. I'm not a network engineer. Right. But this this actually. I mean, this makes sense, right? But I didn't know about this. Apparently the switches are getting better, the routers are being better, but the CPU is staying the static, the speed. Because what is. What's his name? Uh. Moore's law. Moore's law? Yeah, that's Moore's law, not Murphy's law. I'm confused. Moore. Moore's law? Yeah. Moore's law. Every 18 months, it doubles. I think it's slowing down, but network is just keeping the switches are keeping getting better and better, apparently. I don't know that. So that's a again I didn't work in the data center. So any information here is new to me. News. Thus, it's no longer possible for a single core to keep up with a single network link. You have to have multiple cores. That is very interesting. Also, I love it. I absolutely love it. So the the fact that you need multiple cores, which kind of translate to these cores, must be load balanced, like whatever the data coming into your neck has to be shuffled into these cores in an organized manner and in a balanced manner. We're gonna we're gonna read later that TCP cannot do this effectively. It creates this hotspot in a single core per connection. Because of this concept of connections, both incoming and outgoing load must be distributed across multiple cores. This is true at multiple levels. At the application level, high throughput services must run on many cores and divide their work among the cores at the transport layer. A single core cannot keep up with the high speed link, especially with short messages. Load balancing impacts transport protocols in two ways. First, it can introduce overhead. For example, the use of multiple cores causes additional cache misses and coherence. Second, load balancing can lead to hotspots where load is unevenly distributed around, uh, across cores. Right. So we're going to see like all the packets going to one core. The other cores are not uh, as even. And the reason we do this is for stickiness reason. We want the packets to be processed in the same core as much as possible, so that file descriptors of the connection live in the processor cache. And we don't have to go to the memory to fetch those information. I guess that's, that's that's one way to look at it. Right. That's probably why this is the format, the congestion of the software level. Load balancing overheads are now one of the primary sources of tail latency, and they are impacted by the design of transport protocol. It's so fascinating to read this stuff. Um, if you read it in details that is neck offload. I'm not going to spend much on this because, uh, to be honest, what neck offload is like is the network interface controller, right? That means like we want as much as possible the protocol to run in the network card, the network interface controller instead of the actual software. Right. Because it's way faster to run in the in that hardware environment. The problem is, I think Linux is. Very against that, you know, and I when I read that, I think I was like Wikipedia entries and references, some articles says the reason the links do not want to use neck offloading, like running software related to transport protocol in the actual neck is because of patches like what if you want to patch this right? Uh, how do you patch hardware stuff? And every vendor is going to do it differently. Right. So for for specific data center mama me a problem that not might not be a problem because like I say this, the whole thing will be managed by one vendor, right? The whole hardware. But like if there is a security concern, like when their transport protocol and you want to fix it, you want to fix a bug, like what do you do in Linux? You just it's a software. You just patch it. Right. And hardware, it's like it's a firmware. What do you do? You just you have to like. Yeah. It's just it's very complicated to fix. When I read this like, not a lot of people are in favor of Nick offloading, putting everything into the nick because of these problems. Like it's very sticky to update, right? Especially if you have like a security problems and stuff like that. I don't know, I know I might be wrong there, but I kind of agree that it's like, yeah, Nick is faster, but is it worth it? Let's continue.


### Replacing TCP  for Data Centers (Part 2) vtt

All right, section three. Everything about TCP is wrong. Not as that. Yeah. Of course. It's, uh, the language used in this paper is almost used in purpose to ruffle some feathers and ruffle some feathers are dead. I absolutely love it. Let's continue. This section discusses five key properties of TCP, which cover almost all of its designs. That's actually very interesting things. We can learn a lot about TCP from this, uh, this section really steam orientation. The idea of having concept of stream instead of messages, actual discrete start and end right here. Messages. There is no concept of messages or requests in TCP, right? You create this at the application layer. You are responsible to do that when you use TCP. And that's what Http does like Http has this header called Content-length. And that header has been abused to oblivion. Uh with Http smuggling attack. Most Http smuggling attack happens because of this stream orientation, right where we don't know where the message starts and where the message ends, and as a result, where the request starts and where the request ends. What makes it worse? Http has actually Http. The fluid design of Http has multiple ways to indicate an end right of a of a request. Like there is the content length and there is the what is it called the the transfer encoding I believe, which is like, hey, I don't know, I'm gonna abort to send unlimited stuff. So just, just, uh, be ready if I send like backslash n, backslash r, that's the end of my transmission I think twice. Slash and backslash, backslash backslash are that that basically. Hey that that ends my transmission. So just because we have multiple ways the problem is like what if you have content length and you have transmission encoding, the proxy will process it differently. And the back end let's say Node.js will trans will process it completely differently. And as a result, uh, hackers can smuggle, uh, sensitive requests in the second message that will be processed by that will be skipped completely by the proxy and will be processed by the application on the back end. This way, an attacker can call an admin API on the back end that has been actually blocked in the proxy. So that's what Http smuggling. And it's mostly because of the TCP streaming or orientation problem. Right? Right. They could have mentioned that but they didn't. That's like having clear definition of where the message starts and at the transport layer immediately solves this problem. We don't have a solution for this problem, unfortunately. That's why every two days you see uh, you see advisory. Oh, this magnet detector in Node.js, it's detected on, uh, in generic connection orientation. Correct. The idea of having TCP needs to have a connection because, well, I don't know why it needs to have a connection. Now after reading this. Like I am questioning everything to be honest. Like like like why why do you need a connection? Right. Yeah. It's like I have some state and you can store all the state in the connection. That's handy, right? You have port that comes to the port, you connect to a port and this way you can have the same host can have many applications and and the application is the connection level. You're going to see this the very similar thing right with the homa with the concept of RPC. But. Homer doesn't have connections. They removed all that together. You know, because we have connections in TCP. No, we need to store them and we need to store that state. And boy, if you have many connections, then you're going to need a lot of memory. There is a lot of management going on there. Bandwidth sharing, fair scheduling. So I didn't really, uh, know about this in TCP, to be honest. This is a new thing to me. Fair scheduling or fair queuing or whatever is called. I think TCP works at the segment level. So if you have a segment, if you have like ten connections coming, like all coming to your host, right? The segments will be processed in order in a fair manner like so. You have one segment, this segment, this segment, the segment, and it will just round robin through them one by one getting all these segments right. So if your segment happened to have one byte and the second segment is fully loaded with content, then you will be starved. Short messages will be starved in this example. Right. So short messages living in short segments will starve. That's how I understood it at least. Right? Because you'll have to wait because you're processing a larger messages segments while you have ten short segments, right. That needs to that can be processed quickly, but instead you decided to wait and, uh, serve larger segments, right? Because of just the order. It's just fair. Right? We'll we'll go through this in details. We'll talk about sender driven congestion control. We talked about this, right? The sender dictates the congestion and where we're going to send data and data. And data sense and sense and sensor up. I'm. I'm detecting congestion. My packets are not being acknowledged. My segments are not being acknowledged. Drop drop drop drop drop. Right. So the sender. That's effectively the bad thing about TCP in order packet delivery. So in order packet delivery. So the IP packets which carries the TCP segments which have the information about the sequence will have to be arrive in order. So even if your IP packets arrive out of order which will they will. Right. The end of the day, because it's the internet, I mean the data center, you can control that in a sense. Not not much in the internet. Then the application, the transport layer will start blocking says, oh, this is segment number three. Where is two and one. I'm going to wait for them. So the fact that you waited and you did not process segment three despite segment three being complete and a good message to be processed is a wasted latency, right? So the under an order packet delivery is kind of problematic. But we solve this problem. We know about this. We have this problem today with Http right a browser want to send ten requests right. How does it send ten requests to the same host in quick? We use UDP streams, which is the very similar concept. It's just UDP to avoid TCP head of line blocking. But in this case, yes, in this request they are they are completely independent streams. So the application when we arrive at the other end we're going to see oh this is, it's yeah we receive this segment. Segment number three arrived before segment number one. Sure. Yeah. They are out of order but they are completely its own stream. So I don't care. Take it and and start processing it. Do not wait for segment two and one to arrive. Or request 1 or 2 to arrive to process request number three. That's ridiculous. Right. So that's that's basically, uh, the idea of head of line blocking. Again, it still exists if you're using a TCP connection. That's why Http two, uh, still have the idea of head of line blocking at the TCP layer. So that's one limitation of TCP right there. Right. That's why quick solves that completely because quick has independent streams right. That's another reason why probably this paper never mentioned quick because same problem extremes. We don't care about streams. They don't want streams, they want messages. Right. So the idea of having stream, I wish that quick. They just thought of a concept to switch to messages and all of this will be gone. That's it. This this won't exist, right? If we just implemented in a quick. The idea of messages I don't think is going to be easy, but that that will solve the problem, right? Technically, because that's what they want. They want messages. But if you think about it like they also want other things, the congestion control we have congestion control and quick we have each stream is has its own congestion control limit. It's completely independent. Right. So but it's still limited by the sender. Right. So that's something they don't like I wish they really you know what they didn't mention anything about quick. And I'm mad. They really should talk about that because like why didn't you pick quick I know the answer. The answer is like it's stream, so it won't solve anything. Why not UDP just build on top of UDP? Probably they want to reinvent the wheel. They don't want to want that concept of ports which has it. UDP has it. They don't need this concept. Again I'm making a lot of things up implying and I'm not afraid of implying things because like hey, there's it's not written here and not nothing written here. And not in the other paper. So by the way, there is another paper right here. Uh, I'm going to share it with you, which is the actual home implementation. Back in 2018, I read that nothing. No mention of Quic, nothing. Let's read. Let's read. Stream orientation. The data model for TCP is stream of bytes. However, this is not the right model for most data center applications. Data center applications typically exchange discrete messages to implement remote procedure calls. Very critical. We talked about this right when you when you are in a data center or you have like a microservice talking to another Microsoft, it doesn't have to be Microsoft, just the normal data center thing, right? Whatever things they talk about these, you know, Kubernetes or whatever, you know, other, you know, host to host or whatever applications have in their data center. These communication happen as a request response. Give me a request. Give me a response. Yep. And the discreet message is the remote procedure call where, hey, I'm going to make a request. You give me a response. This means when an application reads from a stream. This is very important. That's why I highlighted it. There is no guarantee that it will receive a complete message. True. It's a stream you read. Whatever the Colonel will give you. Will give you. You don't know. That's why the application, the library, the Http library, for example. Node.js needs to continue reading. Reading until it's it's it gets a sense of what what is reading. I said, oh, it's a request. It's actually a request. So there is no guarantee you will receive a complete message or a complete request. Think of a message as a request here. Right. It could. It could receive less than a message, a full message or parts of several messages. Right? Because it might be the messages are so short you might receive, I don't know, three requests in a single read. Highly unlikely Http requests are so large, right? Large is like several bytes. It's going to be a lot of bytes. So I don't know, maybe. Yeah. If you make your request so short like get slash the minimum, no headers whatever the applicable headers you can do it. Yeah. Or use another protocol. The TCP based application must mark message boundaries when they serialize the messages. You have to encode your links somehow in the data so you're wasting precious data. From your side as an application to mark messages length, and you're responsible for managing the length and stuff like that. This is another important thing if multiple threads. Both read from a stream, it is possible that parts of a single message might be received by different thread. That's true, right? If the message is long. If a request is long and you have multiple threads reading from the same stream or socket or a connection, right? Think of a connection as a stream here. So not to be confused, right? Again, this overloaded of terms is is is really killing me. You know, I just I try to parse this paper, but you know, professors love to use abstractions as much as possible. So that's why I try to parse it. I try my best, I but a stream is a connection as far as I know here. A TCP connection. So if you're reading that stream of connection of data and multiple threads are reading it as one large request might be received from one thread and the other part will be received by the other. Yikes. That is the worst case scenario because like, oh hey, you got my request. Can you can you give me the request, please? So now coordination has to happen. It's so expensive. I have to agree with that. So yeah, this is the interesting thing. Load balancing. How do we actually solve this. The first approach used by memcache d is divide is to divide a collection of streams statically among the thread, where each thread handles all the requests arriving on its streams. So each each thread has a single stream. Why? Why can't we just say this? Each thread gets a stream basically right? So one thread, one stream, one thread, one stream. The. This approach is prone to because this way you don't you want you know you're not going to get the like the two threads dealing with multiple you know same requests right. All the requests will be handled by a thread. But now this is a problem with the hotspot. You're going to get a hotspot problem where one connection will be so busy and the other connection is so light, so one thread will be overloaded and the other thread won't be overloaded. And that's a problem in, I guess, in memcached. But then the second approach used by Ram cloud dedicates one thread to read all incoming messages from all connections. So all connections one thread. It's up to you. You can design it any, any way you want. What's the problem with this? So this is so this thread reads all the messages, all the stream data, which is a bunch of segments, which now becomes a stream of data and now dispatches the dispatch messages to the thread. So that thread is responsible to breaking down the boundaries of the message. It's like, oh, you are a Http request. Wait, where do you start? You start here. Oh, this is where you end because your content length is so it needs to pass. This all takes CPU processing power right? And then so imagine if this we can be offloaded to somewhere else. That'll be really interesting I just I love absolutely love that part where the message boundary is handled at the transport layer. I loved this, I loved this, in this design, we. It's a novel idea that I never. Yeah. Because to me, I always think of messages or requests being processed. Application layer. But why? Why do we have to why why not push this down at the layer four. Let layer four does it this stuff an even better. Offload this to the Nic, although I don't know if it's a good idea, I think it's going to bite us in the butt in the future if we float everything to the neck, right? I don't know, I don't know anything, you guys. Dedicates one thread to read all the incoming messages from all streams, and then dispatch messages to the thread of the server. So the thread is doing a lot of work here. This is also much better load balancing work because now the thread knows which thread is which, which thread each thread is doing right. That's actually very interesting. So you can do a better load balancing. There is no hotspotting. Man, this is a completely different world. You know, I absolutely love this, but the dispatcher thread becomes a throughput bottleneck. It makes sense, right? It becomes a bottleneck. The fundamental problem with streaming is that units of way in which data is received, ranges of by do not correspond to dispatchable units. Work of messages. Why not? Why not? Why not let me let me challenge that. Let me challenge that. Can we fix that with DCP? Can we fix that. Can I? Assuming we don't have a limit in my M2, right? Assume I don't have a limited time to you. Right? Because I'm in the data center I'm going to make, I don't know, one gig, my M2. I don't know if that's even possible. Let's say it is right. That means my IP package is so large, right? Can we even have a gig IP packet two to power what 16 bit. So that gives us 65 K. That is tiny dude. So the largest MTU we can get is 65 K in IPv4 right. That is tiny dude 65 K. So that's the largest MTU we can get. All right okay. That is tiny I have to agree with you. All right. But I'm still going to continue my theory here. Right? IPv6. Uh let's see IPv6. Yeah it sounds like a 60 4k. All right. Uh, regardless let's go back. All right. Let's let's let's stick with this. All right. So let's say the MTU is 65 K because apparently anything larger than that doesn't make sense because it can't fit the IP header. And where did I hear that that sometimes the payload is zero. So you put the payload length as zero and you can put whatever you want there. I'm pretty sure there was I read something like that. I might be wrong, but yeah let's assume 65 K. Like don't send any message larger than 65 K. All right. Let's let's let's let's put some restriction here. So now the MTU that large then you have the frame is 65 K obviously. And then you have the IP packet is the 65 K. And then as a result the MSS will be 65 K minus whatever 40 bytes for the headers. That's beautiful. Right this way. Now you can control each message you send. Actually send it in a segment. So now if your message is a thousand bytes have a low level API which you I believe you can flush the segment one segment. It doesn't have to be full and have your application treat the segment as a message. Can we do this? Can we do this? Is this even possible? I'm just challenging this. I don't know if it's possible or not. If you can Ncbi treat the segment as a unit of work, then for free we got the length you guys. That's the message length. Can we do this? Can we do this, you guys? Maybe not. Maybe. Yes. Right. Unless if this is like if this is if middle middle router tried to play with the segments and retransmit it, I cannot guarantee that. I don't think we can guarantee the segment size. Right. If you have proxies, forget about it. Right. If you have like end to end, the host is behind a proxy. Right. And then the proxy is like you're establishing a connection between you and the proxy. And the proxy establish connection with the back end, uh, actual back end. Then you, the proxy have to rewrite these segments to the to the back end. Right. So you have to make sure the proxy is actually rewriting the same segment sizes, which you cannot guarantee because there is no these these are not written in stone as far as I know. Right. These segment sizes like whatever you receive doesn't won't necessarily go into that I think. So it might work. It might not. But I guess it's safer to create a new protocol. But I know I would like to give it a try first. It would be better for each message to be dispatched to a different thread, so messages can be processed concurrently. So that's another advantage now that if the thread knows where the message starts, where the message ends, then it can be, oh, this is one message he thread ticket emitted. One ticket. Right? That's the same thing. If you if we somehow can guarantee that's what I'm saying, right. If you can somehow guarantee a message fitting in a single segment, then we can solve this, right? Can we guarantee that? I don't have an answer to that. Let's read this. Highlighted that I highlighted the red for some reason in this word, the nick should perform load balancing, dispatching incoming requests across a collection of application threads via kernel bypass. However, this will not be possible since information about message boundaries is application specific and unknown to transport layer. I think I mentioned that, but I highlighted it here again because it's so important. The fact that the information, the metadata about the message is available at the transport layer is so powerful, which you think about it, the segment does have a size, right? The the the length of the actual content of the segment. This is how that's the length of the segment. It does have that right. The data length. Right. But maybe it's not guaranteed. Right. That's why we cannot use it. Okay, but what they are inventing effectively is inventing the same concept of a segment, but as a as a decree, a discreet message here. Obviously it's more complicated than I'm mentioning, but but yeah, if the if the Nick for example, knows about the length, it can, it can do the work and then immediately dispatch the message to the thread.


### Replacing TCP  for Data Centers (Part 3) vtt

All right, let's, uh, we are reaching connection orientation. Very critical. So the concept of connections in TCP is, uh, we've talked about this, right? WhatsApp, you know, at some point supported 1 million TCP connections. Then they are back to three. And I believe that now they're more than that 3 million TCP connection per host. And they are doing their best to to make sure that they fit a lot of connection in a single host TCP connection that is in a single host. Right? And that's how they are squeezing as much as possible memory wise. Right? But the reason they cannot go more than that is because of. Because each connection takes memory, right? It's a long lived connection. States left and right as window sizes. There is the sequences. What kind of the sequence? Their file descriptors. Right. And I believe Linux, as they mentioned here. Yeah, it's actually a dimension here. It's it's read actually connections are under the undesirable in data center environment because application can have hundreds or thousands of them resulting in high overhead in space and time. For example, in Linux, kernel keeps about two kilobytes of state for each TCP socket, excluding packet packet buffers. Additional state is required at the application level. Yeah. That's that's that's a positive point. We talked about this many times. Uh, one of the you know, pet peeves in the TCP is the connection is just it's it's it's expensive to keep a connection, a stateful connection at the server. And at the client level as well. Right. So yeah, it's expensive to keep a connection. So they don't like that they want to change that. Facebook found the memory demand for a separate connection demanding. Uh. This allows a single connection for each server to be shared across all applications. Thread a single so they match. They made sure that a single connection is shared across all application threads. Let's continue reading here. Right. So that's what they did. They started okay. And instead of having a lot of connections, let's have one connection and share it between application threads. Right. So effectively we're multiplexing right. That's multiplexing or multiplexing with things going into one right. To reduce proxy overhead Facebook uses UDP because there is an overhead. Now now that we have with proxies and we have this connection, we have an overhead. So they use UDP instead of TCP to tolerate the unreliability. But. This sacrifice is congestion control for the European. We know that, right. But I believe this this is now outdated because I'm pretty sure Facebook completely moved to quick. Yeah. Quick is on top of UDP. But quick supports congestion control at the stream level. So yeah, maybe this this paper was written a little bit a while back. The overhead for connection state are also problematic when offloading the transport to the neck, due to limited resources on the next chip. Yeah. What? You're gonna you're gonna flow the TCP connections down to the neck. That's impossible. Right? What? How much memory do we can you handle on that controller? That's. That's not possible. Right. So that I believe they want to move to the neck. That's from the from the paper here, the Nic, the network interface controller. They will move everything as much as possible from software to the hardware and make it firmware. Yeah, patching is going to be a nightmare, but they they're going to take care of it. It's a data center. Who cares right. Still security vulnerabilities, stuff like that I don't know man. Sounds to me like easier to patch an operating system than an A hardware firmware, especially if you have thousands and hundreds of thousand servers. I don't know if you thought about that. I don't know if it's a good idea. Yeah, it's fast, but. Another problem with connection is that they require a setup phase before any data can be transmitted. Sure, we know that this sends an x ack. Not only that, they didn't mention even the encryption here like TLS. You gotta have encryption between your data centers, right? So you're going to encrypt. So yep, there is a handshake going on. I didn't read anything related to encryption when it comes to humor. Now that I'm actually thinking about it, I didn't read anything related to encryption. Maybe they don't have that. Or maybe they do bandwidth sharing. So there's a concept of fair scheduling, which we talked about in TCP. Unfortunately scheduling discipline like this, which is this fair thing like a segment as a segment, hey, you get a segment, I'll get a segment segment, segment. So even if the segment is one byte and this segment is 1500 bytes, which is full segment, then I'm going to process them in order. I don't care if you have, you might have 1001 byte segments and we have uh, three uh, or we have like a couple a link longer segments, which is like this actual, uh, TCP segment. Right. Then the order in order is, is fair. It's like, hey, I don't care if you're short or long, I'm going to process you one byte, then process one 1500, then process 1500, then process 1500 up until until I go back to you and then I process another byte. So technically the shorter messages or starving because you have ten shorter messages that can be processed in one go, but you have waited because you are fair. I don't know what I'm doing. Air quotes. But. Yeah. So that's, uh, they don't like that. So they're shifting to this thing that's called shortest remaining processing time. And and basically what that means is that, uh. As I am processing something, I'm going to I want to know, like how, how, how long are you how how short are you in TCP? We don't know the length right of the actual message. We know the length of the segment, which is useless. Right? Right. But we don't know the length of the message. So. But if we do, we could have done. Oh. How much, how much are we remaining for this. Oh, we need a thousand bytes for this 20, 20. Oh let me just process the shortest that one is. And then I'm going to hit the 1001 one hit. So you your throughput automatically increases here. Right. It's just fascinating stuff I like to think about all this stuff. That's good stuff. That's good stuff. Provide a better overall response time because they they dedicate all of the available resources to a single task at a time, ensuring that it finishes quickly. Right. So it's like, hey, single task, let's finish you, let's finish you up. Let's let's wrap it up. Anything that can be and that becomes, that comes back to priorities and stuff like that. Again, I highlighted important stuff. TCP has no information about message boundaries. We do not know where the message starts and where the message end. All this sender driven congestion control TCP drives congestion control from senders. Which voluntarily slow their rate of bracket transmission when they detect conduction. We talked about that, right. That's one problem. Another problem with the TCP. Again a problem for data centers. That is so the continue again discussing a congestion control here. They rely on congestion signals related to buffer occupancy. More commonly switches generate ECN notification when queue length reach a certain threshold. I talk about that. Uh, the ECN notification where if the routers or the switches again saying just switches here. To me a switch is a layer two thing. It has nothing to do with layer three. It has it doesn't it doesn't touch the IP packets. Right. Again maybe in the data center they speak this language to me. It's confusing. You have to say layer three switch. You can't just say switch right. So that's one another pet peeve with this paper. It's like yeah. Is it specifically say layer three switch which are switches that look at layer three, which is the IP protocol and does stuff to that stuff at that layer, right. Reaches the IP headers, which most switches just read. You know, the Mac addresses the frames, right? It doesn't care about the IP header. So a layer three switches does a deep packet inspection and determines the IP packet. And when it does that there is a bit that is can set in the IP called the ECN which is the explicit congestion notification. It tells uh as it processes this packet it will say hey I'm I'm about to be congested. There is a lot of stuff in my buffer and marks it up. So as it as the IP packet goes all the way to the host, the host replay back that IP packet and sits it in the set, the ECN header in the IP packet. And this way the sender will know that oh something happened. Congestion happened. But look how long it took right. For the sender to know there was a conjunction. It took a whole round trip, basically. And that's what I mention here. It takes about one root, one root for a sender to find out about traffic changes. TCP does not take advantage of priority queues in modern network. I honestly don't know what a priority queue is. Now. Maybe again network engineers let us know in the comment section. Uh, apparently it's a it's a specific feature in switches that allows certain packets to have a priority over others. That's all packets are treated equal for. Short messages can get queued behind long ones. And that's a bad thing for data center because a short message queued behind a long one. You know the long one will take longer time to process. And when I say process, I don't mean the application actually processing it right in the app. No, I mean just just to receive that message, to deliver that message. It takes time, more time to deliver than a single, shorter message in order. Packet delivery. Another bad thing about the TCP, as far as this paper mentions in data center networks, the most effective way to perform load balancing is to perform packet spraying, which is something new to me as well. Packet spraying, if you don't know, is the idea of having a lot of you have a lot of packets coming IP packets, that is, and then you just spray it across the different links that you have, like for load balancing reasons, let's say you have, uh, your switch or your router has multiple links and all of these links, eventually they are load balanced. Right. And it will it will eventually lead there. It's a whole mesh at the end of the day, right. If you think about it this way then you can send the packet number one here. Packet number two here. Packet number three here. Instead of thinking packet packet packet you send packet packet number 123 on each link. So effectively you're spraying the packets just like it's a hose just spraying it I'm doing that with my hand now. Just spraying the hose. All right. So what's wrong. Can we do this in TCP. Apparently not. So instead TCP network must use flow consistent routing where all the packets from a given connection take the same trajectory through the network flow. Consistent routing ensures in-order packet delivery, but it virtually guarantees that there will be overloaded links in the network. Yeah, that makes sense, right? Because you're now you're now everything just goes into the same link. But it creates a hotspot, right? For a connection. It just follows one link. But I'm really surprised to learn about this. Do we really in the internet, forget about the data centers? I don't have an answer for that. Do routers in the internet actually do that? Do they look at the source and the destination? And then say, oh, your TCP and you're going to the source. So you're always going to take this path so we can ensure in order, uh, guarantee. And is that why multipath. TCP was invented. Right because to to take a different path effectively. That sounds like a bad idea for doing that. Really? On the internet. That sounds like a bad idea. I don't know. I don't know anything anymore. Really. We're just doing a sticky session per connection. Yeah, but apparently from this papers, this is the second time I've seen this turn the flow consistent first in when I read the multipath TCP paper and then when I read this paper flow consistent. Sounds like this is what's happening. It's like I can't find information anymore online about these low level questions. I need to speak to someone who is an expert, not not me. You know, obviously I need to to ask someone who actually entrenched with this on a day to day basis that knows these answers because I don't have answers to this, to be honest. So here's they say TCP is beyond repair. TCP is beyond repair. Again, one of the problems consider congestion control. Right. And this is the data center TCP. Let's talk about a little bit the data center TCP protocol. You you remember when I talked about the explicit congestion notification right. This bit that we set in the IP header. What happens is. That bit that tells you there is a congestion. It doesn't tell you anything else. It doesn't tell you how much congestion. How much are we about to be congested? How much bite left doesn't tell you any of that. Data center TCP sets more metadata to the sender to make better decisions about congestion. That's that's all what it is. And if you want to read more about it right here and that's the abstract, uh, the data center TCP just it's a RFC for those listening. It's RFC, it's RFC 8257 for those who are interested. So again, they're mentioning here that all of these schemes, all the protocols that try to enhance TCP or recreate TCP are based on the fundamental problem that it is buffer based. Right? So if one of the routers in the middle, uh, filled up, tough luck. Right. That's the only signal. Right. Which they want to change that buffer is not just because you're you're filled with stuff. Doesn't mean it's time to drop it. They want to change that. They want to flip that. They want to avoid the buffer to begin with. It's it's very interesting. Let's continue. We're almost there. Now we get to Homer. Homer is a clean slate redesign of network transport or for the data center. So the first concept messages. Homer is message based. It's not biased. It's not streams, it's message based. So at the transport layer, at layer four, you have access to a message. It's a complete message. When I get you something, it's a complete message. Yeah. That's the that's the work we're working around a message. The context is a message. Yeah. We kind of got that with the with the with TCP we got segments but segment didn't really correspond to messages. And that's what breaks this. A client sends a request message to a server and eventually receives a response message. The primary advantage of a message is they make dispatchable unit explicit at the transport layer. This dispatchable unit of work, not a bunch of bytes, right. It's actual message that we can consume immediately and we can work on it immediately. Right. Nic based implementation of the protocol could dispatch message directly to a pool of worker thread via kernel bypass. So now even if you implemented humor in the neck in the network interface controller, the Nic will only give the application a message that it immediately can consume, not just a byte. So the application has to do zero work when it comes to, you know, parsing and doing all this, you know, uh, work that the Http protocol does, oh, content-length blah blah, blah. You know, it doesn't have any of that, right? It's just immediately consume it. Beautiful. It's a it's a commended way. But the moment we work with messages, you guys forget about video streaming. This is useless for video streaming. This is useless for gaming. You know the concept of a gaming maybe. Yeah, well, it depends on the game, I guess. Right? Uh, if a game relies heavily on delivering state from the server, then that would be large messages. I take that back. Maybe it's it's it might be a good idea for the game, but if you're receiving like a video streaming or live streaming that forget about it. Any of that stuff, you know, audio calls that will that won't work. Oklahoma. Because you'll have to wait for the whole message to arrive in order to to deliver it for the application. That is the trick. So if you have a large message, the neck is buffering this message, I think it should. Right. So that's another limitation is like what if you have like a large message, are you gonna buffer it in the neck and does the does does the neck actually support buffering this large messages. So for example, an application cannot receive any part of the message until the entire message has been received. Right. So that's a limitation I guess of this. But at the end of the day they they are fine with this limitation. And that's what I like about they actually mentioned that, hey, this is the limitation. It's not good. It's not great for anyone. But hey right no connections. Humor is connectionless. So the when I read this while humor is connectionless you guys, it doesn't mean it's stateless. It is a stateful protocol. It has a state stored both in the client and the server. About these things. That's called the RPC. Right? So there is a state but there's no connection. Right the concept of a connection. There is no connection set up overhead, and an application can use a single socket to manage any number of concurrent rpcs with any number of peers. Again, here they're talking about a scheduling policy. Let's continue receiver driven congestion control. So that's interesting here. So the difference here is the receiver dictates how the sender sends the information in Homo. Hmm. Interesting. The receiver has knowledge of all its incoming messages. That's true. Right. So it is in a better position to manage this congestion. When a sender transmits a message, it can send a few unscheduled packets unilaterally enough to cover the round trip time. But the remaining schedule packets may only be sent in response to grants from the receiver. That's what we talked in the beginning. Right. So there is yeah, we always send something called an unscheduled packet. So you cannot first of all large or small messages. They homa doesn't send large messages at once, like TCP is like oh let's just send send send send send. No. It divides things into two buckets, if you will. The unscheduled packets which always get sent, the schedule packets, which is like get schedule until we get a grant from the receiver to receive them. And that basically the moment you get a grant that means we will have the congestion is controlled by the receiver and this will guarantee almost no congestion, right. Because we know the moment we receive a grant, that means, hey, I'm good to send. What other problems that does this cause? To be honest, what kind of problems? I don't know, maybe. Maybe the sender will have backed up a lot of scheduled packets backed up. Right? And this chattiness again going back from the server to the client. Once the first packet of the message has been seen, the total length of the message is known. That's very interesting. So when you send unscheduled packet for each message we for free get the headers right and the header includes the length of the message. So the receiver immediately know that they're going to get a bunch of messages. Not complete, not necessarily complete. Some of them might be complete, some of them might not. And they get to decide. The receiver gets to decide okay, let's grant this. Oh these are short. They are already complete. Let me let me deliver them. These are these are not completed yet. Let's send a grant for this one. And now we can you get to choose and pick and choose. So that's interesting design right there. Does it have limitations. I don't know, man. Sounds like keeping stuff in the center buffer. Um. Yeah. I don't know if this will starve longer messages or not. It might. Out of order packets. A key design feature of Homa is that it can tolerate out-of-order packets. Sure, I don't care if if a message was three received before message one. The order of which the messages are sent has no. To be honest, that has nothing to do with the processing of the messages. It's just messages. Why are you blocking message? Uh, three. Just because message one didn't arrive? No. I get to choose. Like, okay, I might block message three because message one is short and I have to I have to arrive message one first. So yeah, you message three is very long. So yeah, you can wait. It's okay. But message one is short and I want to process shorter package. So all of this can be actually controlled. So out of order packets are so fine in this case. But again, as long as we have enough information delivered to us so that we can see these messages to begin with. Right. All right. So getting there from here. Almost done. Conclusion. Almost done. So again, uh, Homer, because of all of this, all the scheduled and unscheduled and grants, it has its own API and it's not compatible with TCP. So what the what those guys did, they said, wait a minute. Yeah, sure. Nobody who who's the last one who built on top of TCP has some application built directly on TCP, but most people use APIs that sits on top of TCP, such as gRPC, Apache Thrift. Right. Those are built on top of Http two. gRPC is built on Http two, which uses TCP. Right. And there's like an ongoing backlog item in gRPC to use Http three. Uh, now those guys are working with gRPC team, the Google team, to support Homa as a gRPC transport layer. In this case, the moment Homa comes to gRPC, immediately all the applications light up. And that's the beauty of this. If you are a gRPC user, you're going to get Homa for free if this get implemented. And by the way, did I mention that they have a Linux implementation already. So those guys already did the work and they showed the numbers. So yeah. So yeah, not all applications are Http. Do I see http on top of Homa http for maybe because Http has nothing to do with streaming or wait a minute. No no no we can't do we can't use messages on top of Http. That's just not not a good idea because HDP is a streaming. Also the same concept, right? Imagine like you don't you don't see the page until everything is loaded. No, I want to see as things are arrive, especially Http streaming and yeah video we watched YouTube. No no no no no no no no. Keep this away keep keep keep using TCP and quick. Uh yeah. No no no no Http three. That's it. No no no no. Bad idea, bad idea I don't think it works for the web Homa I don't think it does. So yeah. Uh, this is their proposal. They're going to use, uh, gRPC. Let's read the conclusion and end this video. I know you guys are tired. I am also exhausted, so I think it's one of the longest videos I made ever. But let's read this. The conclusion TCP is the wrong protocol for data and computer. Again, they focus on the data center here, right? Nothing to do with other stuff, nothing to do with the web. Data center computing. Dhcp is wrong. They want a low level protocol, a transport protocol that fixes these problems, which they articulated very well in this. In my opinion in this paper over this way, this paper doesn't have details about humor. If you want the actual details of the humor, that's there's like a lot of more detailed, uh, paper that I'm going to reference as well that reads like the actual in the weed if you want to go that far. Every aspect of TCP design is wrong. There is no part worth keeping. Again, wrong in for the data center. There is no part worth keeping. If we want to eliminate the data center tax, we want to find a way to move most data traffic to a radically different protocol. Homo offers an alternative that appears to solve all of TCP problems. The best way to bring humor into widespread usage is integrated with RPC frameworks, with RPC frameworks that underlie most large scale center applications, like, yeah, in microservices. Basically, most of this stuff, uh, uses gRPC. Any communication between services are actually used, maybe gRPC. That's pretty much it. That's the like the de facto right isn't it? So yeah, this was the we need a replacement for TCP in the data center. Right. Written by, uh, Professor John Ousterhout, Stanford University. Again, paper is still currently under submission, but didn't didn't prevent us from actually reviewing it and reading it. I think it's a good protocol. I think it's a good idea, uh, for the data center that is, uh, does it does it fix all our TCP problems? No, I think TCP is still relevant. I think we still need the TCP protocol. Yeah. Uh. But it's not. It's not. It's not applicable for everything. It's applicable for certain use cases, especially if you have this request response and responses. And your responses don't have the concept of streaming. Right. So in the web, I don't believe this is good for us. Yeah, Http is a request response system, but. Uh. The response? I cannot think of waiting for the whole response to arrive in order to just deliver it, right. I want to see the HTML headers. I want to see the body. I want to see the skeleton comes in right as content comes in. Let me let me this idea of streaming the HTML page, whatever you see is what you get is important. We need to see stuff, right? Unless you just want to click and then just have a loading screen and then poof, everything appears. You can do that, but I don't know if that will fly in the web. Right. And that just dead on arrival. When when you like watch a YouTube video or stuff like that. This is not suitable for streaming videos, right? Because, uh, the main disadvantage is we have to have a message which we don't have this concept in TCP, TCP is just whatever is in bytes. It's bytes streaming data. It's just like a hose come fills with data. And that is a disadvantage or advantages for this streaming concept. So really depends on you what you want. Uh what's your use cases. Uh, I enjoyed, uh, going through this. I think it's a, uh, it's a good paper. Guys, what do you think about this paper? What do you think about this whole protocol? Do you think we need it? Do you think we don't? In the data center again? In the data center outside? They don't address anything outside at all. But let me know. What do you think? Uh, I'm gonna see you in the next one. You guys stay awesome. Goodbye.


### Running out of TCP Source Ports vtt

You know, guys, I always believed that bugs are what makes our experience as software engineers. It is the encounter. And finally the, you know, getting over the hump of bugs and fixing the bug or addressing it because you don't always fix bugs, you just work around it sometimes and fix it and automatically it got fixed by itself. Because bugs sometimes are exposed because other things, it has no related with the thing that the bug is exposed to. You know, if you if you if you code it for a long time, you will know what I'm saying. But essentially in this video, I want to talk about a very interesting bug that I never ran into before that I ran in the past few months, actually, and I thought it'd make a very good podcast. And to give you an idea what really happens when you run out of this randomly assigned ephemeral source ports in your TCP connections. Because I never run out of those there. Yeah, there are 65,000 ish, but man, you can run onto it. If you wrote inefficient code, which in that particular case I did. Let's jump into it. Welcome to the backend engineering show with your host, Hussein Nasser. And to explain this bug, I wanted to kind of visualize the simple system. You know, I simplified it just for the sake of this video, but essentially it's a it's a web server and there is a message broker, and there's other pieces of the, of the system that is irrelevant here. But these are the two pieces that are critical here. So the web server that accepts connection from clients directly, this is the public, uh, web server that accepts connections. So clients, browsers, calls, anything that connects to the web server and the web server submits jobs, submits messages, all sorts of things to the message queue. And there's there's these will get processed by further downstream services, you know. Or will it will be there for certain metadata information. Okay. So here's what happened. What what happened is after a few commits, something got committed to the repo. And what we noticed is the the system works fine. Requests are being processed very nicely, very fast with no problem at all. You make a request to the web server, you get a response. Very nice. But after a few thousand requests and there is no particular number right after a few thousand requests. The web server will stop responding altogether. It was just a nope. I'm done. What will happen is the client will give up, so you'll give a client timeout. And if you have a proxy which made the problem a little bit worse is the proxy will timeout and we'll get a proxy timeout thinking that, oh there is some things happening on the server that is timing out. So which kind of diluted the problem a little bit. So essentially what's happened is if you talk to the web server directly and make a request after that threshold few thousand requests, your request will just keep processing forever. Sometimes you'll get a response after like three few minutes, but then after that they'll just die. And and you will try it again. It will work. After that it will just go down again. So what was So that's the behavior of the book. I, I usually like not to go directly to the because if I go to the repo, I see the comments, I see, I can I can tell what went wrong. But that's to me that's the last thing I do. Because first I need to investigate the problem. And I have a whole course of how to troubleshoot backend application. Check it out. Go to performance. Hossein nasr.com uh to learn more about it without even if you even if you don't have the source code, you can you can technically learn about a system by just, you know, do a black box testing, as they say from the just looking at what is the behavior of this as a box and how is it communicating to each other. So so what I did is I went to the web server, okay. The web server in this particular case was a different machine than the message broker. So I went to the web server and it just says all right netstat. Give me all your connections. And I noticed a flood of connections which is unusual. You shouldn't have this many connections outgoing from the web server to the message broker. You need one right. And the message broker I used in this particular case we used is uh supports uh multiplexing, i.e. you can send multiple requests concurrently on the same connection. Right. So in this particular case, which means that you can use one connection for pretty much every request, right. You can you can go crazy. You don't need another connection. So you should see one, maybe two in case of an overloaded system. Right. But we've been seeing 10,000. 20,000, right. And that didn't look right. And if the system goes down, it goes to up to 20,000 connections. And so that's the alarm. But but why would that be the problem? That's the second question. All right. You have a lot of connections. So what. So we know the problem. And I looked at that. And I know that even the install that caused it. But now why is the question always this curiosity in mind. It's like a little bit annoying when when people works with me it's like oh no, no, no, we know what the problem is. Just just revert it. No, no, no, I want to understand, you know, it's just that's that's just, uh, that's just my nature. Yeah. It could be annoying sometimes because it slows down the process. Because once, once you understand. But but then. So. All right. So what we are making a lot of connections that should not stop the web server from responding, right? So here's what happened. We essentially, long story short, we ran out of the ephemeral ports in that machine. So what does that mean? So this requires a little bit of understanding of how TCP works. You see, when you can establish a connection between a client and a server, the you are essentially connecting to a well-known IP address and a well-known port. That is often, if you're secure, like an Https 443 and the IP address is also fixed. And after you do a DNS, you get you get a load of IP addresses. If you have, you know, uh, redundancy. But then eventually you only pick one. So then you pick that and you connect. So you know the port. So you know the source, uh, the destination IP. The destination port. But you know the source port. Sorry. The source IP. That's your IP address. And let's ignore Nat for for simplicity here. So the source IP is known but the only one variant is the source port. And this is the kernel's job to assign a random source port so that it completes the four tuples the four entries. I always get confused with the word tuple. Okay. So these four entries. So that the fourth thing port. So it will assign random from I think it's the even the terminal port has a range right. Let me look it up actually so I can give you that uh ephemeral port range. So yeah, I'm seeing here from 32,768 up until 60,999, because we're talking about, uh, IPv4 here. The port is 32 bit. Right. So that's 65,000. So you have a range of 30,000 which kind of lines up with whatever I'm seeing when it gets to the 20,000, 25,000 where we start seeing the problem. So eventually in this particular case the web server IP address is known fixed. The destination IP address is known. That's the message queue right? IP address. The destination port is known. That's the port of the listener on the message queue. Right. And the source port is basically ephemeral that is generated. So you get only what based on these knowing that these two are fixed right. The source IP is fixed, the destination IP is fixed, the destination port is fixed. You get only, you get only 20,000 ish 25,000 ports. That means you get 25,000 Connection from here to here. You might say it's it's low. It's not low number. It's enough for what you need. Because remember this is while fixing the the host to the client. In this case the web server is the client and the destination. The back end is the message queue. Right. And these are fixed. So what is happening is eventually we discovered that every request incorrectly, every request from the client, which also has a connection by the way. But this is this is not a problem. Right. The client to the web server is not a problem. You might say why? Well, the destination is known the web server. The destination IP is known. The destination port is known, the web server port 80 or 443. But the clients change. You don't have one client establishing 100,000 connections. Know you have 100,000 clients establishing a connection each. So that's you can go crazy with that number, right? So the IP address, the IP address change in this particular case, the source port. Of course for each source IP address you get all these. Again the 20,000 or 30,000 whatever source IP address. Right. And these ranges by the way, if you look up at this table I'll put it on the screen. Uh it's filled. It's different for windows. Different for Linux, different for other, you know, other uh kernels. So what was happening? The problem is clients were connecting, clients were sending requests. And then for every request, the web server was supposed to look up a backend connection and establish this connection to the back end, which is the message queue if is not established, if there is an existing connection, write whatever you want to write to the message queue. Write. Sometimes you need the message queue. Sometimes some requests don't need it at all. Right? It's this problem. The bug was in the web server. And specifically this logic that connects to this message queue incorrectly. We were for every request, we're creating a new connection regardless every request. Create new connection. Send a request, create a connection, send for every request. We were creating a new connection. So we were leaving all these connections just idle, right? So you might say, okay, idles where there is a timeout in the TCP, so they eventually die and that's what will happen, right? They will eventually these connections will die. But what makes things worse is there was custom client logic I mean client. Here is the web server that essentially kept intentionally kept the connections alive by using something like WebSockets. It's not really a WebSocket. It's good. Have you heard of ping pong? Uh, protocol and WebSocket. It's a it's a it's an application layer seven uh, keep alive mechanism, not the TCP keep alive. Higher than that. Not the kernel keeping the connection alive. It's the application sending essentially nothing. Right. Just just to tell the client. Hey, I'm still using it. I'm still here. I'm still here. I'm still here. So it's just chatter. Essentially. Just ping pong. Ping pong. Right. So that kept the problems. Well because we're now maintaining all this. So that added a little bit of an overhead to maintain all these connections that we don't really need to. Second we were keeping those connections and they take in resources. And eventually once we reach that threshold ephemeral port, new requests. What will happen is our new request. Remember this. These are got to be careful with that synchronous request. Now the client. I don't care about the client. The client is asynchronous. The web server received this request is synchronous, i.e. it is blocking the request. It's making the request the back end request to the message queue, while the request blocked until it received a response from the message queue. It will unblock the request and it will send the response right. It's stuck at this stage where there's air. All right. I cannot respond to this guy until I get something from my message queue. But guess what? We were stuck in essentially the kernel. We couldn't even create a connection. So the web server got berserk, and eventually other apps started to break because other apps needs to connect to the to that machine for some reason or another. And that also broke something that consumed some of the ephemeral ports. And it becomes very interesting. Right. So yeah, you don't have much, to be honest, to run out of that ephemeral port. So after I, we identified that problem, we saw that, okay, fair enough. Here's the bug we're in. Instead of actually looking out for existing connections, we're always creating a new one. Uh, we fixed that. All is good. Happy, happy, happy. We have back to one beautiful connection. So one resource can give you so much. You can go so far with one connection. So beautiful indeed. Very, very beautiful. So I then started looking up like, is this really a problem people run into? Because it seems like an easy thing to run into and fair enough. Cloudflare has a nice blog about it and running out of ephemeral ports. Things can get really wrong, you know, if that started to happen. And I think the the, the biggest problem that you're going to start having is. If you are, if you're not on the same machine. No. If you are on the same machine, you're going to see it even worse, because what was happening is the the source IP to the let's say you're connected to localhost. Let's ignore IPv6 for for a second. Say only IPv4. Right. So 127001 you're connecting to port 80 locally. Right. And then the source port is also on 12770001. Right. So you're connecting locally. You have a loopback connections for one reason or another. You're using the tcp IP. I always say try to shift away from that. You can't always do that. Like you can use IPV IPC for that. But sometimes you can't do that, especially if you want to use side proxies right where you where you want to. Containers having the same loopback. And they want to connect to each other to enable essentially sidecar proxying to work. Right. You need to this capability TCP IP to do that. So then um, you can essentially also run off your ephemeral loopback ports as long as the fixed the source, the destination port, the destination IP is fixed. You. And of course the source IP is fixed. You have this 20,000 limit 30,000 limit whatever the kernel allows it. Right. But then but then what if I connect to another port completely different 121 like Wright, 22 is or 2121 is FTP 22, right? I want to connect to that. You can technically reuse those ephemeral ports because those are different. Now it's a different tuple, but some kernels don't even allow that because it's sometimes break applications. Because the uniqueness of source ports, they are treated as just unique by itself, not they. Some applications don't take the portable. They take the two tuples, which is of course not a good idea, but it sometimes break applications. So even some kernels just don't allow you to reuse ports, even across destination ports, if that makes sense. Which makes the problem, uh, even more interesting. So I'll reference that blog for you guys to to read. But yeah, that's what I want to talk about today. So yeah, I thought, I thought this was an interesting, uh, bug that I'll share with you guys. See you in the next one. Goodbye.
