WEBVTT

00:00.230 --> 00:03.890
All right, let's, uh, we are reaching connection orientation.

00:03.890 --> 00:04.880
Very critical.

00:05.540 --> 00:10.700
So the concept of connections in TCP is, uh, we've talked about this, right?

00:10.700 --> 00:15.920
WhatsApp, you know, at some point supported 1 million TCP connections.

00:15.920 --> 00:17.240
Then they are back to three.

00:17.240 --> 00:22.100
And I believe that now they're more than that 3 million TCP connection per host.

00:22.100 --> 00:29.510
And they are doing their best to to make sure that they fit a lot of connection in a single host TCP

00:29.510 --> 00:31.490
connection that is in a single host.

00:31.490 --> 00:32.030
Right?

00:32.210 --> 00:36.020
And that's how they are squeezing as much as possible memory wise.

00:36.020 --> 00:36.530
Right?

00:37.880 --> 00:41.330
But the reason they cannot go more than that is because of.

00:41.330 --> 00:43.430
Because each connection takes memory, right?

00:44.060 --> 00:45.590
It's a long lived connection.

00:45.590 --> 00:47.930
States left and right as window sizes.

00:47.930 --> 00:49.340
There is the sequences.

00:49.340 --> 00:50.480
What kind of the sequence?

00:50.480 --> 00:52.070
Their file descriptors.

00:52.220 --> 00:52.550
Right.

00:52.550 --> 00:54.380
And I believe Linux, as they mentioned here.

00:54.380 --> 00:56.060
Yeah, it's actually a dimension here.

00:56.120 --> 01:01.670
It's it's read actually connections are under the undesirable in data center environment because application

01:01.670 --> 01:09.920
can have hundreds or thousands of them resulting in high overhead in space and time.

01:09.920 --> 01:17.480
For example, in Linux, kernel keeps about two kilobytes of state for each TCP socket, excluding packet

01:17.480 --> 01:18.800
packet buffers.

01:19.460 --> 01:21.890
Additional state is required at the application level.

01:21.890 --> 01:22.340
Yeah.

01:22.550 --> 01:25.790
That's that's that's a positive point.

01:25.790 --> 01:27.890
We talked about this many times.

01:28.340 --> 01:33.980
Uh, one of the you know, pet peeves in the TCP is the connection is just it's it's it's expensive

01:33.980 --> 01:37.340
to keep a connection, a stateful connection at the server.

01:37.340 --> 01:39.650
And at the client level as well.

01:39.650 --> 01:40.100
Right.

01:40.490 --> 01:42.710
So yeah, it's expensive to keep a connection.

01:42.710 --> 01:45.350
So they don't like that they want to change that.

01:46.690 --> 01:50.710
Facebook found the memory demand for a separate connection demanding.

01:51.070 --> 01:51.310
Uh.

01:51.310 --> 01:56.020
This allows a single connection for each server to be shared across all applications.

01:56.020 --> 01:59.020
Thread a single so they match.

01:59.020 --> 02:04.060
They made sure that a single connection is shared across all application threads.

02:04.060 --> 02:05.260
Let's continue reading here.

02:06.680 --> 02:06.980
Right.

02:06.980 --> 02:08.030
So that's what they did.

02:08.030 --> 02:08.900
They started okay.

02:08.900 --> 02:13.250
And instead of having a lot of connections, let's have one connection and share it between application

02:13.250 --> 02:13.790
threads.

02:13.790 --> 02:14.060
Right.

02:14.060 --> 02:16.910
So effectively we're multiplexing right.

02:16.910 --> 02:21.170
That's multiplexing or multiplexing with things going into one right.

02:21.860 --> 02:26.600
To reduce proxy overhead Facebook uses UDP because there is an overhead.

02:26.600 --> 02:31.100
Now now that we have with proxies and we have this connection, we have an overhead.

02:31.100 --> 02:36.140
So they use UDP instead of TCP to tolerate the unreliability.

02:36.140 --> 02:36.800
But.

02:38.610 --> 02:41.310
This sacrifice is congestion control for the European.

02:41.310 --> 02:42.690
We know that, right.

02:42.690 --> 02:49.080
But I believe this this is now outdated because I'm pretty sure Facebook completely moved to quick.

02:49.290 --> 02:49.890
Yeah.

02:49.890 --> 02:51.690
Quick is on top of UDP.

02:52.590 --> 02:57.570
But quick supports congestion control at the stream level.

02:58.620 --> 03:02.910
So yeah, maybe this this paper was written a little bit a while back.

03:03.420 --> 03:08.790
The overhead for connection state are also problematic when offloading the transport to the neck, due

03:08.790 --> 03:11.460
to limited resources on the next chip.

03:11.490 --> 03:11.940
Yeah.

03:12.420 --> 03:12.690
What?

03:12.690 --> 03:16.500
You're gonna you're gonna flow the TCP connections down to the neck.

03:16.530 --> 03:17.760
That's impossible.

03:17.760 --> 03:18.120
Right?

03:18.600 --> 03:19.440
What?

03:19.440 --> 03:23.160
How much memory do we can you handle on that controller?

03:23.160 --> 03:23.520
That's.

03:23.520 --> 03:24.810
That's not possible.

03:25.110 --> 03:25.380
Right.

03:25.380 --> 03:29.430
So that I believe they want to move to the neck.

03:29.430 --> 03:37.170
That's from the from the paper here, the Nic, the network interface controller.

03:37.170 --> 03:41.610
They will move everything as much as possible from software to the hardware and make it firmware.

03:41.610 --> 03:45.660
Yeah, patching is going to be a nightmare, but they they're going to take care of it.

03:45.660 --> 03:46.620
It's a data center.

03:46.620 --> 03:48.000
Who cares right.

03:48.990 --> 03:54.210
Still security vulnerabilities, stuff like that I don't know man.

03:54.210 --> 03:59.130
Sounds to me like easier to patch an operating system than an A hardware firmware, especially if you

03:59.130 --> 04:02.250
have thousands and hundreds of thousand servers.

04:02.250 --> 04:03.990
I don't know if you thought about that.

04:05.010 --> 04:06.720
I don't know if it's a good idea.

04:06.720 --> 04:08.400
Yeah, it's fast, but.

04:10.440 --> 04:16.770
Another problem with connection is that they require a setup phase before any data can be transmitted.

04:16.800 --> 04:19.950
Sure, we know that this sends an x ack.

04:20.550 --> 04:23.880
Not only that, they didn't mention even the encryption here like TLS.

04:23.910 --> 04:26.250
You gotta have encryption between your data centers, right?

04:26.760 --> 04:28.290
So you're going to encrypt.

04:28.290 --> 04:31.140
So yep, there is a handshake going on.

04:32.010 --> 04:35.520
I didn't read anything related to encryption when it comes to humor.

04:35.550 --> 04:41.040
Now that I'm actually thinking about it, I didn't read anything related to encryption.

04:41.070 --> 04:42.390
Maybe they don't have that.

04:43.650 --> 04:46.290
Or maybe they do bandwidth sharing.

04:46.290 --> 04:49.470
So there's a concept of fair scheduling, which we talked about in TCP.

04:49.500 --> 04:55.410
Unfortunately scheduling discipline like this, which is this fair thing like a segment as a segment,

04:55.410 --> 04:58.200
hey, you get a segment, I'll get a segment segment, segment.

04:58.200 --> 05:06.900
So even if the segment is one byte and this segment is 1500 bytes, which is full segment, then I'm

05:06.900 --> 05:08.970
going to process them in order.

05:09.420 --> 05:18.420
I don't care if you have, you might have 1001 byte segments and we have uh, three uh, or we have

05:18.420 --> 05:25.050
like a couple a link longer segments, which is like this actual, uh, TCP segment.

05:25.050 --> 05:25.470
Right.

05:25.500 --> 05:28.080
Then the order in order is, is fair.

05:28.080 --> 05:33.000
It's like, hey, I don't care if you're short or long, I'm going to process you one byte, then process

05:33.000 --> 05:39.000
one 1500, then process 1500, then process 1500 up until until I go back to you and then I process

05:39.000 --> 05:39.720
another byte.

05:39.720 --> 05:47.130
So technically the shorter messages or starving because you have ten shorter messages that can be processed

05:47.130 --> 05:50.760
in one go, but you have waited because you are fair.

05:50.760 --> 05:51.630
I don't know what I'm doing.

05:51.630 --> 05:52.380
Air quotes.

05:53.370 --> 05:53.760
But.

05:53.760 --> 05:53.910
Yeah.

05:53.910 --> 05:55.350
So that's, uh, they don't like that.

05:55.350 --> 05:59.670
So they're shifting to this thing that's called shortest remaining processing time.

05:59.670 --> 06:03.000
And and basically what that means is that, uh.

06:03.830 --> 06:11.210
As I am processing something, I'm going to I want to know, like how, how, how long are you how how

06:11.210 --> 06:12.860
short are you in TCP?

06:12.890 --> 06:15.800
We don't know the length right of the actual message.

06:16.010 --> 06:18.290
We know the length of the segment, which is useless.

06:18.290 --> 06:18.680
Right?

06:18.710 --> 06:19.310
Right.

06:19.310 --> 06:21.200
But we don't know the length of the message.

06:21.200 --> 06:21.560
So.

06:21.560 --> 06:23.270
But if we do, we could have done.

06:23.270 --> 06:23.600
Oh.

06:23.600 --> 06:25.910
How much, how much are we remaining for this.

06:25.910 --> 06:28.970
Oh, we need a thousand bytes for this 20, 20.

06:28.970 --> 06:32.090
Oh let me just process the shortest that one is.

06:32.090 --> 06:35.090
And then I'm going to hit the 1001 one hit.

06:35.090 --> 06:38.390
So you your throughput automatically increases here.

06:38.390 --> 06:38.870
Right.

06:39.230 --> 06:43.610
It's just fascinating stuff I like to think about all this stuff.

06:43.610 --> 06:44.270
That's good stuff.

06:44.270 --> 06:45.200
That's good stuff.

06:45.560 --> 06:52.910
Provide a better overall response time because they they dedicate all of the available resources to

06:52.910 --> 06:56.300
a single task at a time, ensuring that it finishes quickly.

06:56.300 --> 06:56.480
Right.

06:56.480 --> 07:00.140
So it's like, hey, single task, let's finish you, let's finish you up.

07:00.140 --> 07:01.430
Let's let's wrap it up.

07:01.430 --> 07:07.100
Anything that can be and that becomes, that comes back to priorities and stuff like that.

07:07.730 --> 07:09.260
Again, I highlighted important stuff.

07:09.260 --> 07:12.800
TCP has no information about message boundaries.

07:12.800 --> 07:17.960
We do not know where the message starts and where the message end.

07:17.960 --> 07:24.410
All this sender driven congestion control TCP drives congestion control from senders.

07:25.170 --> 07:30.360
Which voluntarily slow their rate of bracket transmission when they detect conduction.

07:30.360 --> 07:31.470
We talked about that, right.

07:32.130 --> 07:33.120
That's one problem.

07:33.120 --> 07:34.770
Another problem with the TCP.

07:35.930 --> 07:38.240
Again a problem for data centers.

07:38.240 --> 07:42.050
That is so the continue again discussing a congestion control here.

07:42.530 --> 07:45.500
They rely on congestion signals related to buffer occupancy.

07:45.500 --> 07:52.400
More commonly switches generate ECN notification when queue length reach a certain threshold.

07:52.400 --> 07:53.420
I talk about that.

07:53.750 --> 08:01.310
Uh, the ECN notification where if the routers or the switches again saying just switches here.

08:01.310 --> 08:03.290
To me a switch is a layer two thing.

08:03.290 --> 08:04.850
It has nothing to do with layer three.

08:04.850 --> 08:07.520
It has it doesn't it doesn't touch the IP packets.

08:07.520 --> 08:07.910
Right.

08:07.910 --> 08:11.750
Again maybe in the data center they speak this language to me.

08:11.750 --> 08:12.920
It's confusing.

08:13.010 --> 08:16.160
You have to say layer three switch.

08:16.160 --> 08:17.990
You can't just say switch right.

08:17.990 --> 08:20.930
So that's one another pet peeve with this paper.

08:21.050 --> 08:21.830
It's like yeah.

08:21.860 --> 08:27.230
Is it specifically say layer three switch which are switches that look at layer three, which is the

08:27.230 --> 08:32.630
IP protocol and does stuff to that stuff at that layer, right.

08:32.780 --> 08:36.320
Reaches the IP headers, which most switches just read.

08:36.320 --> 08:38.240
You know, the Mac addresses the frames, right?

08:38.240 --> 08:41.060
It doesn't care about the IP header.

08:41.060 --> 08:47.090
So a layer three switches does a deep packet inspection and determines the IP packet.

08:47.090 --> 08:53.840
And when it does that there is a bit that is can set in the IP called the ECN which is the explicit

08:53.840 --> 08:55.190
congestion notification.

08:55.970 --> 09:03.590
It tells uh as it processes this packet it will say hey I'm I'm about to be congested.

09:03.590 --> 09:06.860
There is a lot of stuff in my buffer and marks it up.

09:06.860 --> 09:16.280
So as it as the IP packet goes all the way to the host, the host replay back that IP packet and sits

09:16.280 --> 09:19.850
it in the set, the ECN header in the IP packet.

09:19.850 --> 09:23.540
And this way the sender will know that oh something happened.

09:23.540 --> 09:24.440
Congestion happened.

09:24.440 --> 09:27.440
But look how long it took right.

09:28.040 --> 09:30.140
For the sender to know there was a conjunction.

09:30.410 --> 09:33.020
It took a whole round trip, basically.

09:34.000 --> 09:35.740
And that's what I mention here.

09:36.580 --> 09:41.290
It takes about one root, one root for a sender to find out about traffic changes.

09:41.320 --> 09:45.730
TCP does not take advantage of priority queues in modern network.

09:45.730 --> 09:48.370
I honestly don't know what a priority queue is.

09:48.700 --> 09:48.910
Now.

09:48.910 --> 09:52.750
Maybe again network engineers let us know in the comment section.

09:53.230 --> 10:00.280
Uh, apparently it's a it's a specific feature in switches that allows certain packets to have a priority

10:00.280 --> 10:01.060
over others.

10:01.420 --> 10:04.570
That's all packets are treated equal for.

10:04.570 --> 10:07.540
Short messages can get queued behind long ones.

10:07.660 --> 10:13.300
And that's a bad thing for data center because a short message queued behind a long one.

10:13.300 --> 10:16.990
You know the long one will take longer time to process.

10:16.990 --> 10:22.870
And when I say process, I don't mean the application actually processing it right in the app.

10:22.870 --> 10:27.280
No, I mean just just to receive that message, to deliver that message.

10:27.460 --> 10:33.550
It takes time, more time to deliver than a single, shorter message in order.

10:33.550 --> 10:34.210
Packet delivery.

10:34.210 --> 10:40.060
Another bad thing about the TCP, as far as this paper mentions in data center networks, the most effective

10:40.060 --> 10:47.320
way to perform load balancing is to perform packet spraying, which is something new to me as well.

10:47.470 --> 10:53.560
Packet spraying, if you don't know, is the idea of having a lot of you have a lot of packets coming

10:53.560 --> 10:59.320
IP packets, that is, and then you just spray it across the different links that you have, like for

10:59.320 --> 11:06.220
load balancing reasons, let's say you have, uh, your switch or your router has multiple links and

11:06.220 --> 11:09.280
all of these links, eventually they are load balanced.

11:09.280 --> 11:09.790
Right.

11:10.150 --> 11:12.400
And it will it will eventually lead there.

11:12.550 --> 11:14.890
It's a whole mesh at the end of the day, right.

11:14.890 --> 11:18.430
If you think about it this way then you can send the packet number one here.

11:18.430 --> 11:19.180
Packet number two here.

11:19.180 --> 11:20.290
Packet number three here.

11:20.290 --> 11:26.440
Instead of thinking packet packet packet you send packet packet number 123 on each link.

11:26.440 --> 11:32.020
So effectively you're spraying the packets just like it's a hose just spraying it I'm doing that with

11:32.020 --> 11:32.650
my hand now.

11:32.650 --> 11:35.290
Just spraying the hose.

11:35.290 --> 11:35.920
All right.

11:35.920 --> 11:38.410
So what's wrong.

11:38.410 --> 11:40.180
Can we do this in TCP.

11:40.210 --> 11:40.870
Apparently not.

11:40.870 --> 11:46.930
So instead TCP network must use flow consistent routing where all the packets from a given connection

11:46.930 --> 11:50.050
take the same trajectory through the network flow.

11:50.050 --> 11:56.320
Consistent routing ensures in-order packet delivery, but it virtually guarantees that there will be

11:56.320 --> 11:58.270
overloaded links in the network.

11:58.270 --> 11:59.440
Yeah, that makes sense, right?

11:59.440 --> 12:04.870
Because you're now you're now everything just goes into the same link.

12:04.870 --> 12:07.270
But it creates a hotspot, right?

12:07.270 --> 12:08.080
For a connection.

12:08.080 --> 12:09.850
It just follows one link.

12:10.120 --> 12:13.570
But I'm really surprised to learn about this.

12:13.570 --> 12:16.840
Do we really in the internet, forget about the data centers?

12:16.840 --> 12:18.790
I don't have an answer for that.

12:19.670 --> 12:23.060
Do routers in the internet actually do that?

12:23.540 --> 12:26.510
Do they look at the source and the destination?

12:27.460 --> 12:30.940
And then say, oh, your TCP and you're going to the source.

12:30.940 --> 12:36.370
So you're always going to take this path so we can ensure in order, uh, guarantee.

12:37.420 --> 12:39.970
And is that why multipath.

12:40.480 --> 12:43.240
TCP was invented.

12:44.240 --> 12:47.840
Right because to to take a different path effectively.

12:48.720 --> 12:51.450
That sounds like a bad idea for doing that.

12:51.840 --> 12:52.260
Really?

12:52.260 --> 12:52.950
On the internet.

12:52.950 --> 12:54.840
That sounds like a bad idea.

12:55.530 --> 12:56.790
I don't know.

12:58.100 --> 12:59.660
I don't know anything anymore.

13:01.040 --> 13:01.460
Really.

13:01.490 --> 13:05.240
We're just doing a sticky session per connection.

13:06.380 --> 13:11.630
Yeah, but apparently from this papers, this is the second time I've seen this turn the flow consistent

13:11.720 --> 13:17.690
first in when I read the multipath TCP paper and then when I read this paper flow consistent.

13:17.690 --> 13:19.460
Sounds like this is what's happening.

13:19.700 --> 13:27.020
It's like I can't find information anymore online about these low level questions.

13:27.020 --> 13:30.710
I need to speak to someone who is an expert, not not me.

13:30.710 --> 13:37.460
You know, obviously I need to to ask someone who actually entrenched with this on a day to day basis

13:37.460 --> 13:41.120
that knows these answers because I don't have answers to this, to be honest.

13:41.300 --> 13:46.010
So here's they say TCP is beyond repair.

13:47.420 --> 13:50.600
TCP is beyond repair.

13:50.690 --> 13:54.350
Again, one of the problems consider congestion control.

13:54.380 --> 13:55.010
Right.

13:55.010 --> 13:56.510
And this is the data center TCP.

13:56.510 --> 14:00.140
Let's talk about a little bit the data center TCP protocol.

14:00.440 --> 14:05.960
You you remember when I talked about the explicit congestion notification right.

14:05.960 --> 14:08.630
This bit that we set in the IP header.

14:08.630 --> 14:11.000
What happens is.

14:12.140 --> 14:14.630
That bit that tells you there is a congestion.

14:14.630 --> 14:15.920
It doesn't tell you anything else.

14:15.920 --> 14:18.080
It doesn't tell you how much congestion.

14:18.080 --> 14:21.320
How much are we about to be congested?

14:21.320 --> 14:23.480
How much bite left doesn't tell you any of that.

14:23.480 --> 14:30.800
Data center TCP sets more metadata to the sender to make better decisions about congestion.

14:30.800 --> 14:32.510
That's that's all what it is.

14:32.510 --> 14:39.380
And if you want to read more about it right here and that's the abstract, uh, the data center TCP

14:39.560 --> 14:42.230
just it's a RFC for those listening.

14:42.230 --> 14:47.390
It's RFC, it's RFC 8257 for those who are interested.

14:47.390 --> 14:53.840
So again, they're mentioning here that all of these schemes, all the protocols that try to enhance

14:53.840 --> 15:01.670
TCP or recreate TCP are based on the fundamental problem that it is buffer based.

15:01.850 --> 15:02.150
Right?

15:02.150 --> 15:07.100
So if one of the routers in the middle, uh, filled up, tough luck.

15:07.100 --> 15:07.400
Right.

15:07.400 --> 15:09.080
That's the only signal.

15:09.080 --> 15:09.710
Right.

15:09.710 --> 15:15.650
Which they want to change that buffer is not just because you're you're filled with stuff.

15:15.650 --> 15:17.900
Doesn't mean it's time to drop it.

15:17.900 --> 15:19.010
They want to change that.

15:19.010 --> 15:20.030
They want to flip that.

15:20.030 --> 15:22.400
They want to avoid the buffer to begin with.

15:22.400 --> 15:23.990
It's it's very interesting.

15:25.550 --> 15:27.320
Let's continue.

15:27.350 --> 15:28.010
We're almost there.

15:28.010 --> 15:28.910
Now we get to Homer.

15:29.780 --> 15:35.540
Homer is a clean slate redesign of network transport or for the data center.

15:35.540 --> 15:38.120
So the first concept messages.

15:38.120 --> 15:40.160
Homer is message based.

15:40.850 --> 15:41.840
It's not biased.

15:41.840 --> 15:44.360
It's not streams, it's message based.

15:44.360 --> 15:49.310
So at the transport layer, at layer four, you have access to a message.

15:49.310 --> 15:50.300
It's a complete message.

15:50.300 --> 15:52.610
When I get you something, it's a complete message.

15:53.000 --> 15:53.390
Yeah.

15:53.810 --> 15:57.800
That's the that's the work we're working around a message.

15:57.800 --> 15:59.270
The context is a message.

15:59.390 --> 16:00.170
Yeah.

16:00.170 --> 16:07.190
We kind of got that with the with the with TCP we got segments but segment didn't really correspond

16:07.220 --> 16:08.000
to messages.

16:08.000 --> 16:09.740
And that's what breaks this.

16:09.770 --> 16:14.900
A client sends a request message to a server and eventually receives a response message.

16:14.900 --> 16:22.460
The primary advantage of a message is they make dispatchable unit explicit at the transport layer.

16:22.580 --> 16:28.280
This dispatchable unit of work, not a bunch of bytes, right.

16:28.280 --> 16:35.090
It's actual message that we can consume immediately and we can work on it immediately.

16:35.240 --> 16:35.480
Right.

16:36.320 --> 16:40.850
Nic based implementation of the protocol could dispatch message directly to a pool of worker thread

16:40.850 --> 16:42.380
via kernel bypass.

16:42.380 --> 16:50.690
So now even if you implemented humor in the neck in the network interface controller, the Nic will

16:50.690 --> 16:56.990
only give the application a message that it immediately can consume, not just a byte.

16:56.990 --> 17:03.140
So the application has to do zero work when it comes to, you know, parsing and doing all this, you

17:03.140 --> 17:08.690
know, uh, work that the Http protocol does, oh, content-length blah blah, blah.

17:08.690 --> 17:10.820
You know, it doesn't have any of that, right?

17:11.120 --> 17:13.580
It's just immediately consume it.

17:14.530 --> 17:15.340
Beautiful.

17:15.340 --> 17:16.900
It's a it's a commended way.

17:16.900 --> 17:23.470
But the moment we work with messages, you guys forget about video streaming.

17:23.470 --> 17:25.690
This is useless for video streaming.

17:25.690 --> 17:27.550
This is useless for gaming.

17:27.550 --> 17:29.860
You know the concept of a gaming maybe.

17:29.860 --> 17:31.870
Yeah, well, it depends on the game, I guess.

17:32.470 --> 17:33.100
Right?

17:33.370 --> 17:41.020
Uh, if a game relies heavily on delivering state from the server, then that would be large messages.

17:41.020 --> 17:41.860
I take that back.

17:41.860 --> 17:46.930
Maybe it's it's it might be a good idea for the game, but if you're receiving like a video streaming

17:46.930 --> 17:49.210
or live streaming that forget about it.

17:49.450 --> 17:54.310
Any of that stuff, you know, audio calls that will that won't work.

17:54.310 --> 17:54.850
Oklahoma.

17:54.880 --> 18:02.710
Because you'll have to wait for the whole message to arrive in order to to deliver it for the application.

18:02.710 --> 18:03.910
That is the trick.

18:03.910 --> 18:09.940
So if you have a large message, the neck is buffering this message, I think it should.

18:09.940 --> 18:10.570
Right.

18:10.570 --> 18:15.160
So that's another limitation is like what if you have like a large message, are you gonna buffer it

18:15.160 --> 18:21.100
in the neck and does the does does the neck actually support buffering this large messages.

18:21.100 --> 18:26.290
So for example, an application cannot receive any part of the message until the entire message has

18:26.290 --> 18:27.640
been received.

18:28.180 --> 18:28.420
Right.

18:28.420 --> 18:30.730
So that's a limitation I guess of this.

18:30.730 --> 18:34.180
But at the end of the day they they are fine with this limitation.

18:34.180 --> 18:37.870
And that's what I like about they actually mentioned that, hey, this is the limitation.

18:38.140 --> 18:38.770
It's not good.

18:38.770 --> 18:40.360
It's not great for anyone.

18:40.600 --> 18:43.480
But hey right no connections.

18:43.480 --> 18:44.860
Humor is connectionless.

18:44.860 --> 18:52.720
So the when I read this while humor is connectionless you guys, it doesn't mean it's stateless.

18:53.600 --> 18:55.670
It is a stateful protocol.

18:55.790 --> 18:59.390
It has a state stored both in the client and the server.

19:00.100 --> 19:01.150
About these things.

19:01.150 --> 19:02.260
That's called the RPC.

19:02.470 --> 19:02.830
Right?

19:02.830 --> 19:05.530
So there is a state but there's no connection.

19:06.340 --> 19:08.290
Right the concept of a connection.

19:08.290 --> 19:14.290
There is no connection set up overhead, and an application can use a single socket to manage any number

19:14.290 --> 19:17.680
of concurrent rpcs with any number of peers.

19:17.680 --> 19:20.050
Again, here they're talking about a scheduling policy.

19:20.050 --> 19:23.770
Let's continue receiver driven congestion control.

19:23.770 --> 19:25.210
So that's interesting here.

19:25.210 --> 19:31.000
So the difference here is the receiver dictates how the sender sends the information in Homo.

19:31.210 --> 19:31.840
Hmm.

19:32.230 --> 19:33.100
Interesting.

19:33.100 --> 19:37.060
The receiver has knowledge of all its incoming messages.

19:37.060 --> 19:37.810
That's true.

19:37.840 --> 19:38.200
Right.

19:38.200 --> 19:42.520
So it is in a better position to manage this congestion.

19:42.520 --> 19:52.270
When a sender transmits a message, it can send a few unscheduled packets unilaterally enough to cover

19:52.270 --> 19:53.290
the round trip time.

19:53.290 --> 20:00.160
But the remaining schedule packets may only be sent in response to grants from the receiver.

20:00.160 --> 20:01.570
That's what we talked in the beginning.

20:01.570 --> 20:01.840
Right.

20:01.840 --> 20:07.390
So there is yeah, we always send something called an unscheduled packet.

20:07.390 --> 20:10.990
So you cannot first of all large or small messages.

20:11.500 --> 20:18.700
They homa doesn't send large messages at once, like TCP is like oh let's just send send send send send.

20:18.700 --> 20:19.240
No.

20:19.240 --> 20:24.580
It divides things into two buckets, if you will.

20:24.580 --> 20:29.710
The unscheduled packets which always get sent, the schedule packets, which is like get schedule until

20:29.710 --> 20:32.440
we get a grant from the receiver to receive them.

20:32.440 --> 20:39.280
And that basically the moment you get a grant that means we will have the congestion is controlled by

20:39.280 --> 20:43.750
the receiver and this will guarantee almost no congestion, right.

20:43.750 --> 20:47.770
Because we know the moment we receive a grant, that means, hey, I'm good to send.

20:48.220 --> 20:50.560
What other problems that does this cause?

20:50.560 --> 20:52.600
To be honest, what kind of problems?

20:52.600 --> 20:54.670
I don't know, maybe.

20:54.850 --> 21:00.070
Maybe the sender will have backed up a lot of scheduled packets backed up.

21:00.430 --> 21:00.670
Right?

21:02.450 --> 21:07.220
And this chattiness again going back from the server to the client.

21:08.540 --> 21:15.140
Once the first packet of the message has been seen, the total length of the message is known.

21:15.170 --> 21:16.130
That's very interesting.

21:16.130 --> 21:24.200
So when you send unscheduled packet for each message we for free get the headers right and the header

21:24.200 --> 21:25.340
includes the length of the message.

21:25.340 --> 21:29.360
So the receiver immediately know that they're going to get a bunch of messages.

21:29.360 --> 21:31.310
Not complete, not necessarily complete.

21:31.310 --> 21:34.280
Some of them might be complete, some of them might not.

21:34.280 --> 21:36.260
And they get to decide.

21:36.260 --> 21:38.960
The receiver gets to decide okay, let's grant this.

21:38.960 --> 21:40.130
Oh these are short.

21:40.130 --> 21:41.300
They are already complete.

21:41.300 --> 21:42.830
Let me let me deliver them.

21:43.040 --> 21:45.050
These are these are not completed yet.

21:45.740 --> 21:47.210
Let's send a grant for this one.

21:47.210 --> 21:50.540
And now we can you get to choose and pick and choose.

21:50.540 --> 21:53.000
So that's interesting design right there.

21:53.570 --> 21:54.950
Does it have limitations.

21:55.810 --> 21:56.890
I don't know, man.

21:56.920 --> 22:00.550
Sounds like keeping stuff in the center buffer.

22:00.820 --> 22:01.060
Um.

22:02.460 --> 22:03.270
Yeah.

22:03.270 --> 22:06.300
I don't know if this will starve longer messages or not.

22:06.510 --> 22:07.380
It might.

22:09.080 --> 22:10.670
Out of order packets.

22:11.180 --> 22:14.720
A key design feature of Homa is that it can tolerate out-of-order packets.

22:14.720 --> 22:20.450
Sure, I don't care if if a message was three received before message one.

22:20.450 --> 22:24.770
The order of which the messages are sent has no.

22:26.540 --> 22:30.320
To be honest, that has nothing to do with the processing of the messages.

22:30.320 --> 22:31.460
It's just messages.

22:31.460 --> 22:33.320
Why are you blocking message?

22:33.560 --> 22:34.820
Uh, three.

22:34.820 --> 22:36.380
Just because message one didn't arrive?

22:36.380 --> 22:36.920
No.

22:36.920 --> 22:38.300
I get to choose.

22:38.450 --> 22:45.140
Like, okay, I might block message three because message one is short and I have to I have to arrive

22:45.140 --> 22:46.040
message one first.

22:46.040 --> 22:48.710
So yeah, you message three is very long.

22:48.710 --> 22:50.210
So yeah, you can wait.

22:50.210 --> 22:50.990
It's okay.

22:50.990 --> 22:55.370
But message one is short and I want to process shorter package.

22:55.460 --> 22:58.430
So all of this can be actually controlled.

22:58.430 --> 23:02.720
So out of order packets are so fine in this case.

23:04.610 --> 23:13.010
But again, as long as we have enough information delivered to us so that we can see these messages

23:13.010 --> 23:13.610
to begin with.

23:13.610 --> 23:14.030
Right.

23:15.140 --> 23:15.710
All right.

23:15.710 --> 23:18.710
So getting there from here.

23:18.710 --> 23:19.400
Almost done.

23:19.400 --> 23:20.060
Conclusion.

23:20.060 --> 23:20.780
Almost done.

23:21.950 --> 23:27.680
So again, uh, Homer, because of all of this, all the scheduled and unscheduled and grants, it has

23:27.680 --> 23:30.290
its own API and it's not compatible with TCP.

23:30.290 --> 23:33.440
So what the what those guys did, they said, wait a minute.

23:33.950 --> 23:35.120
Yeah, sure.

23:35.360 --> 23:42.020
Nobody who who's the last one who built on top of TCP has some application built directly on TCP, but

23:42.020 --> 23:48.830
most people use APIs that sits on top of TCP, such as gRPC, Apache Thrift.

23:48.830 --> 23:49.310
Right.

23:49.310 --> 23:51.740
Those are built on top of Http two.

23:51.770 --> 23:54.230
gRPC is built on Http two, which uses TCP.

23:54.230 --> 23:54.620
Right.

23:54.800 --> 23:59.720
And there's like an ongoing backlog item in gRPC to use Http three.

24:00.910 --> 24:11.290
Uh, now those guys are working with gRPC team, the Google team, to support Homa as a gRPC transport

24:11.290 --> 24:11.860
layer.

24:11.860 --> 24:18.100
In this case, the moment Homa comes to gRPC, immediately all the applications light up.

24:18.610 --> 24:20.230
And that's the beauty of this.

24:20.230 --> 24:26.710
If you are a gRPC user, you're going to get Homa for free if this get implemented.

24:26.920 --> 24:30.580
And by the way, did I mention that they have a Linux implementation already.

24:30.730 --> 24:34.240
So those guys already did the work and they showed the numbers.

24:34.870 --> 24:35.560
So yeah.

24:35.560 --> 24:38.770
So yeah, not all applications are Http.

24:39.760 --> 24:48.910
Do I see http on top of Homa http for maybe because Http has nothing to do with streaming or wait a

24:48.910 --> 24:49.210
minute.

24:49.240 --> 24:53.020
No no no we can't do we can't use messages on top of Http.

24:53.020 --> 24:57.430
That's just not not a good idea because HDP is a streaming.

24:57.430 --> 24:58.870
Also the same concept, right?

24:58.960 --> 25:02.770
Imagine like you don't you don't see the page until everything is loaded.

25:02.770 --> 25:09.790
No, I want to see as things are arrive, especially Http streaming and yeah video we watched YouTube.

25:09.970 --> 25:11.380
No no no no no no no no.

25:11.380 --> 25:16.150
Keep this away keep keep keep using TCP and quick.

25:16.180 --> 25:16.720
Uh yeah.

25:16.750 --> 25:18.460
No no no no Http three.

25:18.490 --> 25:19.150
That's it.

25:19.150 --> 25:20.200
No no no no.

25:20.200 --> 25:26.290
Bad idea, bad idea I don't think it works for the web Homa I don't think it does.

25:26.290 --> 25:27.040
So yeah.

25:27.040 --> 25:28.360
Uh, this is their proposal.

25:28.360 --> 25:30.130
They're going to use, uh, gRPC.

25:32.100 --> 25:34.380
Let's read the conclusion and end this video.

25:34.380 --> 25:35.940
I know you guys are tired.

25:35.970 --> 25:40.650
I am also exhausted, so I think it's one of the longest videos I made ever.

25:41.310 --> 25:42.150
But let's read this.

25:42.150 --> 25:46.380
The conclusion TCP is the wrong protocol for data and computer.

25:46.380 --> 25:50.040
Again, they focus on the data center here, right?

25:50.340 --> 25:53.190
Nothing to do with other stuff, nothing to do with the web.

25:53.900 --> 25:55.460
Data center computing.

25:55.490 --> 25:56.540
Dhcp is wrong.

25:56.690 --> 26:04.910
They want a low level protocol, a transport protocol that fixes these problems, which they articulated

26:04.910 --> 26:05.930
very well in this.

26:06.320 --> 26:11.570
In my opinion in this paper over this way, this paper doesn't have details about humor.

26:11.570 --> 26:18.920
If you want the actual details of the humor, that's there's like a lot of more detailed, uh, paper

26:18.920 --> 26:25.940
that I'm going to reference as well that reads like the actual in the weed if you want to go that far.

26:25.940 --> 26:28.010
Every aspect of TCP design is wrong.

26:28.010 --> 26:30.440
There is no part worth keeping.

26:30.440 --> 26:33.230
Again, wrong in for the data center.

26:33.230 --> 26:35.480
There is no part worth keeping.

26:35.480 --> 26:44.720
If we want to eliminate the data center tax, we want to find a way to move most data traffic to a radically

26:44.720 --> 26:45.710
different protocol.

26:45.710 --> 26:50.660
Homo offers an alternative that appears to solve all of TCP problems.

26:51.860 --> 27:00.500
The best way to bring humor into widespread usage is integrated with RPC frameworks, with RPC frameworks

27:00.500 --> 27:05.600
that underlie most large scale center applications, like, yeah, in microservices.

27:05.600 --> 27:10.370
Basically, most of this stuff, uh, uses gRPC.

27:10.370 --> 27:14.450
Any communication between services are actually used, maybe gRPC.

27:14.450 --> 27:15.140
That's pretty much it.

27:15.140 --> 27:17.570
That's the like the de facto right isn't it?

27:18.530 --> 27:23.060
So yeah, this was the we need a replacement for TCP in the data center.

27:23.300 --> 27:23.510
Right.

27:23.510 --> 27:29.030
Written by, uh, Professor John Ousterhout, Stanford University.

27:29.120 --> 27:35.600
Again, paper is still currently under submission, but didn't didn't prevent us from actually reviewing

27:35.600 --> 27:36.560
it and reading it.

27:36.560 --> 27:37.760
I think it's a good protocol.

27:37.760 --> 27:43.760
I think it's a good idea, uh, for the data center that is, uh, does it does it fix all our TCP problems?

27:43.760 --> 27:46.010
No, I think TCP is still relevant.

27:46.010 --> 27:48.740
I think we still need the TCP protocol.

27:49.310 --> 27:50.150
Yeah.

27:50.150 --> 27:50.690
Uh.

27:51.860 --> 27:53.030
But it's not.

27:53.030 --> 27:53.510
It's not.

27:53.510 --> 27:55.850
It's not applicable for everything.

27:55.850 --> 28:01.460
It's applicable for certain use cases, especially if you have this request response and responses.

28:01.460 --> 28:05.690
And your responses don't have the concept of streaming.

28:05.690 --> 28:06.110
Right.

28:06.110 --> 28:09.920
So in the web, I don't believe this is good for us.

28:09.920 --> 28:13.730
Yeah, Http is a request response system, but.

28:14.850 --> 28:15.000
Uh.

28:15.000 --> 28:15.990
The response?

28:15.990 --> 28:21.630
I cannot think of waiting for the whole response to arrive in order to just deliver it, right.

28:22.080 --> 28:25.590
I want to see the HTML headers.

28:25.590 --> 28:27.900
I want to see the body.

28:27.900 --> 28:33.180
I want to see the skeleton comes in right as content comes in.

28:33.180 --> 28:40.260
Let me let me this idea of streaming the HTML page, whatever you see is what you get is important.

28:40.260 --> 28:42.060
We need to see stuff, right?

28:42.330 --> 28:47.610
Unless you just want to click and then just have a loading screen and then poof, everything appears.

28:47.790 --> 28:50.790
You can do that, but I don't know if that will fly in the web.

28:50.790 --> 28:51.240
Right.

28:51.240 --> 28:53.430
And that just dead on arrival.

28:53.430 --> 28:56.220
When when you like watch a YouTube video or stuff like that.

28:56.220 --> 28:59.520
This is not suitable for streaming videos, right?

29:00.450 --> 29:07.860
Because, uh, the main disadvantage is we have to have a message which we don't have this concept in

29:07.860 --> 29:11.040
TCP, TCP is just whatever is in bytes.

29:11.040 --> 29:12.510
It's bytes streaming data.

29:12.510 --> 29:16.290
It's just like a hose come fills with data.

29:16.290 --> 29:20.700
And that is a disadvantage or advantages for this streaming concept.

29:20.700 --> 29:23.340
So really depends on you what you want.

29:23.520 --> 29:25.110
Uh what's your use cases.

29:25.110 --> 29:27.330
Uh, I enjoyed, uh, going through this.

29:27.330 --> 29:29.670
I think it's a, uh, it's a good paper.

29:29.670 --> 29:32.100
Guys, what do you think about this paper?

29:32.100 --> 29:33.570
What do you think about this whole protocol?

29:33.570 --> 29:34.410
Do you think we need it?

29:34.440 --> 29:35.550
Do you think we don't?

29:35.550 --> 29:36.810
In the data center again?

29:36.810 --> 29:39.030
In the data center outside?

29:39.030 --> 29:41.430
They don't address anything outside at all.

29:41.430 --> 29:42.960
But let me know.

29:42.960 --> 29:43.920
What do you think?

29:43.920 --> 29:45.630
Uh, I'm gonna see you in the next one.

29:45.630 --> 29:46.320
You guys stay awesome.

29:46.320 --> 29:46.920
Goodbye.
